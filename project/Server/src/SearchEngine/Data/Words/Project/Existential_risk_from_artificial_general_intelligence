Existential risk from artificial general intelligence Artificial intelligence Major goals Knowledge reasoning Planning Machine learning Natural language processing Computer vision Robotics Artificial general intelligence Approaches Symbolic Deep learning Bayesian networks Evolutionary algorithms Philosophy Ethics Existential risk Turing test Chinese room Control problem Friendly AI History Timeline Progress AI winter Technology Applications Projects Programming languages Glossary Glossary v t e Existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence AGI could someday result in human extinction or some other unrecoverable global catastrophe 1 2 3 It is argued that the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack If AI surpasses humanity in general intelligence and becomes superintelligent then this new superintelligence could become powerful and difficult to control Just as the fate of the mountain gorilla depends on human goodwill so might the fate of humanity depend on the actions of a future machine superintelligence 4 The likelihood of this type of scenario is widely debated and hinges in part on differing scenarios for future progress in computer science 5 Once the exclusive domain of science fiction concerns about superintelligence started to become mainstream in the 2010s and were popularized by public figures such as Stephen Hawking Bill Gates and Elon Musk 6 One source of concern is that controlling a superintelligent machine or instilling it with humancompatible values may be a harder problem than na√Øvely supposed Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goalsa principle called instrumental convergence and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task 1 7 8 In contrast skeptics such as Facebooks Yann LeCun argue that superintelligent machines will have no desire for selfpreservation 9 A second source of concern is that a sudden and unexpected intelligence explosion might take an unprepared human race by surprise For example in one scenario the firstgeneration computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months The secondgeneration program is expected to take three calendar months to perform a similar chunk of work on average in practice doubling its own capabilities may take longer if it experiences a miniAI winter or may be quicker if it undergoes a miniature AI Spring where ideas from the previous generation are especially easy to mutate into the next generation In this scenario the time for each generation continues to shrink and the system undergoes an unprecedently large number of generations of improvement in a short time interval jumping from subhuman performance in many areas to superhuman performance in all relevant areas 1 7 More broadly examples like arithmetic and Go show that progress from humanlevel AI to superhuman ability is sometimes extremely rapid 10 Contents 1 History 2 General argument 21 The three difficulties 22 Further argument 23 Possible scenarios 3 Sources of risk 31 Poorly specified goals 32 Difficulties of modifying goal specification after launch 33 Instrumental goal convergence 34 Orthogonality thesis 341 Terminological issues 342 Anthropomorphism 35 Other sources of risk 4 Timeframe 5 Perspectives 51 Endorsement 52 Skepticism 53 Intermediate views 54 Popular reactions 6 Consensus against regulation 7 Organizations 8 See also 9 References History edit One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler who wrote the following in his 1863 essay Darwin among the Machines 11 mwparseroutput templatequoteoverflowhiddenmargin1em 0padding0 40pxmwparseroutput templatequote templatequotecitelineheight15emtextalignleftpaddingleft16emmargintop0 The upshot is simply a question of time but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question In 1951 computer scientist Alan Turing wrote an article titled Intelligent Machinery A Heretical Theory in which he proposed that artificial general intelligences would likely take control of the world as they became more intelligent than human beings Let us now assume for the sake of argument that intelligent machines are a genuine possibility and look at the consequences of constructing them There would be no question of the machines dying and they would be able to converse with each other to sharpen their wits At some stage therefore we should have to expect the machines to take control in the way that is mentioned in Samuel Butlers Erewhon 12 Finally in 1965 I J Good originated the concept now known as an intelligence explosion Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever Since the design of machines is one of these intellectual activities an ultraintelligent machine could design even better machines there would then unquestionably be an intelligence explosion and the intelligence of man would be left far behind Thus the first ultraintelligent machine is the last invention that man need ever make provided that the machine is docile enough to tell us how to keep it under control 13 Occasional statements from scholars such as Marvin Minsky 14 and I J Good himself 15 expressed philosophical concerns that a superintelligence could seize control but contained no call to action In 2000 computer scientist and Sun cofounder Bill Joy penned an influential essay Why The Future Doesnt Need Us identifying superintelligent robots as a hightech dangers to human survival alongside nanotechnology and engineered bioplagues 16 In 2009 experts attended a private conference hosted by the Association for the Advancement of Artificial Intelligence AAAI to discuss whether computers and robots might be able to acquire any sort of autonomy and how much these abilities might pose a threat or hazard They noted that some robots have acquired various forms of semiautonomy including being able to find power sources on their own and being able to independently choose targets to attack with weapons They also noted that some computer viruses can evade elimination and have achieved cockroach intelligence They concluded that selfawareness as depicted in science fiction is probably unlikely but that there were other potential hazards and pitfalls The New York Times summarized the conferences view as we are a long way from Hal the computer that took over the spaceship in 2001 A Space Odyssey 17 In 2014 the publication of Nick Bostrom s book Superintelligence stimulated a significant amount of public discussion and debate 18 By 2015 public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek computer scientists Stuart J Russell and Roman Yampolskiy and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence 19 20 21 22 In April 2016 Nature warned Machines and robots that outperform humans across the board could selfimprove beyond our control and their interests might not align with ours General argument edit The three difficulties edit Artificial Intelligence A Modern Approach the standard undergraduate AI textbook 23 24 assesses that superintelligence might mean the end of the human race Almost any technology has the potential to cause harm in the wrong hands but with superintelligence we have the new problem that the wrong hands might belong to the technology itself 1 Even if the system designers have good intentions two difficulties are common to both AI and nonAI computer systems 1 The systems implementation may contain initiallyunnoticed routine but catastrophic bugs An analogy is space probes despite the knowledge that bugs in expensive space probes are hard to fix after launch engineers have historically not been able to prevent catastrophic bugs from occurring 10 25 No matter how much time is put into predeployment design a systems specifications often result in unintended behavior the first time it encounters a new scenario For example Microsofts Tay behaved inoffensively during predeployment testing but was too easily baited into offensive behavior when interacting with real users 9 AI systems uniquely add a third difficulty the problem that even given correct requirements bugfree implementation and initial good behavior an AI systems dynamic learning capabilities may cause it to evolve into a system with unintended behavior even without the stress of new unanticipated external scenarios An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself but that no longer maintains the humancompatible moral values preprogrammed into the original AI For a selfimproving AI to be completely safe it would not only need to be bugfree but it would need to be able to design successor systems that are also bugfree 1 26 All three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as malfunctioning correctly predicts that humans will attempt to shut it off and successfully deploys its superintelligence to outwit such attempts Citing major advances in the field of AI and the potential for AI to have enormous longterm benefits or costs the 2015 Open Letter on Artificial Intelligence stated The progress in AI research makes it timely to focus research not only on making AI more capable but also on maximizing the societal benefit of AI Such considerations motivated the AAAI 200809 Presidential Panel on LongTerm AI Futures and other projects on AI impacts and constitute a significant expansion of the field of AI itself which up to now has focused largely on techniques that are neutral with respect to purpose We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial our AI systems must do what we want them to do This letter was signed by a number of leading AI researchers in academia and industry including AAAI president Thomas Dietterich Eric Horvitz Bart Selman Francesca Rossi Yann LeCun and the founders of Vicarious and Google DeepMind 27 Further argument edit A superintelligent machine would be as alien to humans as human thought processes are to cockroaches Such a machine may not have humanitys best interests at heart it is not obvious that it would even care about human welfare at all If superintelligent AI is possible and if it is possible for a superintelligences goals to conflict with basic human values then AI poses a risk of human extinction A superintelligence a system that exceeds the capabilities of humans in every relevant endeavor can outmaneuver humans any time its goals conflict with human goals therefore unless the superintelligence decides to allow humanity to coexist the first superintelligence to be created will inexorably result in human extinction 4 28 Bostrom and others argue that from an evolutionary perspective the gap from human to superhuman intelligence may be small 4 29 There is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains therefore superintelligence is physically possible 20 21 In addition to potential algorithmic improvements over human brains a digital brain can be many orders of magnitude larger and faster than a human brain which was constrained in size by evolution to be small enough to fit through a birth canal 10 The emergence of superintelligence if or when it occurs may take the human race by surprise especially if some kind of intelligence explosion occurs 20 21 Examples like arithmetic and Go show that machines have already reached superhuman levels of competency in certain domains and that this superhuman competence can follow quickly after humanpar performance is achieved 10 One hypothetical intelligence explosion scenario could occur as follows An AI gains an expertlevel capability at certain key software engineering tasks It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering Due to its capability to recursively improve its own algorithms the AI quickly becomes superhuman just as human experts can eventually creatively overcome diminishing returns by deploying various human capabilities for innovation so too can the expertlevel AI use either humanstyle capabilities or its own AIspecific capabilities to power through new creative breakthroughs 30 The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field including scientific creativity strategic planning and social skills Just as the currentday survival of the gorillas is dependent on human decisions so too would human survival depend on the decisions and goals of the superhuman AI 4 28 Almost any AI no matter its programmed goal would rationally prefer to be in a position where nobody else can switch it off without its consent A superintelligence will naturally gain selfpreservation as a subgoal as soon as it realizes that it cant achieve its goal if its shut off 31 32 33 Unfortunately any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI unless somehow preprogrammed in A superintelligent AI will not have a natural drive to aid humans for the same reason that humans have no natural desire to aid AI systems that are of no further use to them Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses termites or even gorillas Once in charge the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems just to be on the safe side or for building additional computers to help it calculate how to best accomplish its goals 1 9 31 Thus the argument concludes it is likely that someday an intelligence explosion will catch humanity unprepared and that such an unpreparedfor intelligence explosion may result in human extinction or a comparable fate 4 Possible scenarios edit Further information Artificial intelligence in fiction Some scholars have proposed hypothetical scenarios intended to concretely illustrate some of their concerns For example Bostrom in Superintelligence expresses concern that even if the timeline for superintelligence turns out to be predictable researchers might not take sufficient safety precautions in part because It could be the case that when dumb smarter is safe yet when smart smarter is more dangerous Bostrom suggests a scenario where over decades AI becomes more powerful Widespread deployment is initially marred by occasional accidents a driverless bus swerves into the oncoming lane or a military drone fires into an innocent crowd Many activists call for tighter oversight and regulation and some even predict impending catastrophe But as development continues the activists are proven wrong As automotive AI becomes smarter it suffers fewer accidents as military robots achieve more precise targeting they cause less collateral damage Based on the data scholars infer a broad lesson the smarter the AI the safer it is It is a lesson based on science data and statistics not armchair philosophizing Against this backdrop some group of researchers is beginning to achieve promising results in their work on developing general machine intelligence The researchers are carefully testing their seed AI in a sandbox environment and the signs are all good The AIs behavior inspires confidence increasingly so as its intelligence is gradually increased Large and growing industries widely seen as key to national economic competitiveness and military security work with prestigious scientists who have built their careers laying the groundwork for advanced artificial intelligence AI researchers have been working to get to humanlevel artificial intelligence for the better part of a century of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit The outcome of debate is preordained the project is happy to enact a few safety rituals but only so long as they dont significantly slow or risk the project And so we boldly go into the whirling knives 4 In Max Tegmarks 2017 book Life 30 a corporations Omega team creates an extremely powerful AI able to moderately improve its own source code in a number of areas but after a certain point the team chooses to publicly downplay the AIs ability in order to avoid regulation or confiscation of the project For safety the team keeps the AI in a box where it is mostly unable to communicate with the outside world and tasks it to flood the market through shell companies first with Amazon Turk tasks and then with producing animated films and TV shows While the public is aware that the lifelike animation is computergenerated the team keeps secret that the highquality direction and voiceacting are also mostly computergenerated apart from a few thirdworld contractors unknowingly employed as decoys the teams low overhead and high output effectively make it the worlds largest media empire Faced with a cloud computing bottleneck the team also tasks the AI with designing among other engineering tasks a more efficient datacenter and other custom hardware which they mainly keep for themselves to avoid competition Other shell companies make blockbuster biotech drugs and other inventions investing profits back into the AI The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use for the greater good to prevent wars The team faces risks that the AI could try to escape via inserting backdoors in the systems it designs via hidden messages in its produced content or via using its growing understanding of human behavior to persuade someone into letting it free The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it 34 35 In contrast top physicist Michio Kaku an AI risk skeptic posits a deterministically positive outcome In Physics of the Future he asserts that It will take many decades for robots to ascend up a scale of consciousness and that in the meantime corporations such as Hanson Robotics will likely succeed in creating robots that are capable of love and earning a place in the extended human family 36 37 Sources of risk edit Poorly specified goals edit While there is no standardized terminology an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AIs set of goals or utility function The utility function is a mathematical algorithm resulting in a single objectivelydefined answer not an English statement Researchers know how to write utility functions that mean minimize the average network latency in this specific telecommunications model or maximize the number of reward clicks however they do not know how to write a utility function for maximize human flourishing nor is it currently clear whether such a function meaningfully and unambiguously exists Furthermore a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function 38 AI researcher Stuart Russell writes The primary concern is not spooky emergent consciousness but simply the ability to make highquality decisions Here quality refers to the expected outcome utility of actions taken where the utility function is presumably specified by the human designer Now we have a problem The utility function may not be perfectly aligned with the values of the human race which are at best very difficult to pin down Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources not for their own sake but to succeed in its assigned task A system that is optimizing a function of n variables where the objective depends on a subset of size k n will often set the remaining unconstrained variables to extreme values if one of those unconstrained variables is actually something we care about the solution found may be highly undesirable This is essentially the old story of the genie in the lamp or the sorcerers apprentice or King Midas you get exactly what you ask for not what you want A highly capable decision maker especially one connected through the Internet to all the worlds information and billions of screens and most of our infrastructure can have an irreversible impact on humanity This is not a minor difficulty Improving decision quality irrespective of the utility function chosen has been the goal of AI research the mainstream goal on which we now spend billions per year not the secret plot of some lone evil genius 39 Dietterich and Horvitz echo the Sorcerers Apprentice concern in a Communications of the ACM editorial emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed 40 The first of Russells two concerns above is that autonomous AI systems may be assigned the wrong goals by accident Dietterich and Horvitz note that this is already a concern for existing systems An important aspect of any AI system that interacts with people is that it must reason about what people intend rather than carrying out commands literally This concern becomes more serious as AI software advances in autonomy and flexibility 40 For example in 1982 an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable The evolution resulted in a winning process that cheated rather than create its own concepts the winning process would steal credit from other processes 41 42 The Open Philanthropy Project summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve general intelligence or superintelligence Bostrom Russell and others argue that smarterthanhuman decisionmaking systems could arrive at more unexpected and extreme solutions to assigned tasks and could modify themselves or their environment in ways that compromise safety requirements 5 7 Isaac Asimov s Three Laws of Robotics are one of the earliest examples of proposed safety measures for AI agents Asimovs laws were intended to prevent robots from harming humans In Asimovs stories problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans Citing work by Eliezer Yudkowsky of the Machine Intelligence Research Institute Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time We cant just give a program a static utility function because circumstances and our desired responses to circumstances change over time 1 Mark Waser of the Digital Wisdom Institute recommends eschewing optimizing goalbased approaches entirely as misguided and dangerous Instead he proposes to engineer a coherent system of laws ethics and morals with a topmost restriction to enforce social psychologist Jonathan Haidts functional definition of morality 43 to suppress or regulate selfishness and make cooperative social life possible He suggests that this can be done by implementing a utility function designed to always satisfy Haidts functionality and aim to generally increase but not maximize the capabilities of self other individuals and society as a whole as suggested by John Rawls and Martha Nussbaum 44 citation needed Difficulties of modifying goal specification after launch edit Further information AI takeover and Instrumental convergence Goalcontent integrity While current goalbased AI programs are not intelligent enough to think of resisting programmer attempts to modify it a sufficiently advanced rational selfaware AI might resist any changes to its goal structure just as Gandhi would not want to take a pill that makes him want to kill people If the AI were superintelligent it would likely succeed in outmaneuvering its human operators and be able to prevent itself being turned off or being reprogrammed with a new goal 4 45 Instrumental goal convergence edit AI risk skeptic Steven Pinker Further information Instrumental convergence There are some goals that almost any artificial intelligence might rationally pursue like acquiring additional resources or selfpreservation 31 This could prove problematic because it might put an artificial intelligence in direct competition with humans Citing Steve Omohundro s work on the idea of instrumental convergence and basic AI drives Russell and Peter Norvig write that even if you only want your program to play chess or prove theorems if you give it the capability to learn and alter itself you need safeguards Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially as competitors for limited resources 1 Building in safeguards will not be easy one can certainly say in English we want you to design this power plant in a reasonable commonsense way and not build in any dangerous covert subsystems but its not currently clear how one would actually rigorously specify this goal in machine code 10 In dissent evolutionary psychologist Steven Pinker argues that AI dystopias project a parochial alphamale psychology onto the concept of intelligence They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world perhaps instead artificial intelligence will naturally develop along female lines fully capable of solving problems but with no desire to annihilate innocents or dominate the civilization 46 Computer scientists Yann LeCun and Stuart Russell disagree with one another whether superintelligent robots would have such AI drives LeCun states that Humans have all kinds of drives that make them do bad things to each other like the selfpreservation instinct Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives while Russell argues that a sufficiently advanced machine will have selfpreservation even if you dont program it in if you say Fetch the coffee it cant fetch the coffee if its dead So if you give it any goal whatsoever it has a reason to preserve its own existence to achieve that goal 9 47 Orthogonality thesis edit One common belief is that any superintelligent program created by humans would be subservient to humans or better yet would as it grows more intelligent and learns more facts about the world spontaneously learn a moral truth compatible with human values and would adjust its goals accordingly However Nick Bostroms orthogonality thesis argues against this and instead states that with some technical caveats more or less any level of intelligence or optimization power can be combined with more or less any ultimate goal If a machine is created and given the sole purpose to enumerate the decimals of œÄ displaystyle pi then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found 48 Bostrom warns against anthropomorphism a human will set out to accomplish his projects in a manner that humans consider reasonable while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it and may instead only care about the completion of the task 49 While the orthogonality thesis follows logically from even the weakest sort of philosophical isought distinction Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any rational agent the orthogonality thesis still holds it would still be possible to create a nonphilosophical optimizing machine capable of making decisions to strive towards some narrow goal but that has no incentive to discover any moral facts that would get in the way of goal completion 50 One argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them in such a design changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a minus sign onto its utility function A more intuitive argument is to examine the strange consequences if the orthogonality thesis were false If the orthogonality thesis is false there exists some simple but unethical goal G such that there cannot exist any efficient realworld algorithm with goal G This means if a human society were highly motivated perhaps at gunpoint to design an efficient realworld algorithm with goal G and were given a million years to do so along with huge amounts of resources training and knowledge about AI it must fail that there cannot exist any pattern of reinforcement learning that would train a highly efficient realworld intelligence to follow the goal G and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient realworld intelligences following goal G 50 Some dissenters like Michael Chorost writing in Slate argue instead that by the time the AI is in a position to imagine tiling the Earth with solar panels itll know that it would be morally wrong to do so Chorost argues that a dangerous AI will need to desire certain states and dislike others Todays software lacks that abilityand computer scientists have not a clue how to get it there Without wanting theres no impetus to do anything Todays computers cant even want to keep existing let alone tile the world in solar panels 51 Terminological issues edit Part of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference Outside of the artificial intelligence field intelligence is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning At an extreme if morality is part of the definition of intelligence then by definition a superintelligent machine would behave morally However in the field of artificial intelligence research while intelligence has many overlapping definitions none of them make reference to morality Instead almost all current artificial intelligence research focuses on creating algorithms that optimize in an empirical way the achievement of an arbitrary goal 4 To avoid anthropomorphism or the baggage of the word intelligence an advanced artificial intelligence can be thought of as an impersonal optimizing process that strictly takes whatever actions are judged most likely to accomplish its possibly complicated and implicit goals 4 Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function this choice is then outputted regardless of any extraneous ethical concerns 52 53 Anthropomorphism edit In science fiction an AI even though it has not been programmed with human emotions often spontaneously experiences those emotions anyway for example Agent Smith in The Matrix was influenced by a disgust toward humanity This is fictitious anthropomorphism in reality while an artificial intelligence could perhaps be deliberately programmed with human emotions or could develop something similar to an emotion as a means to an ultimate goal if it is useful to do so it would not spontaneously develop human emotions for no purpose whatsoever as portrayed in fiction 7 One example of anthropomorphism would be to believe that your PC is angry at you because you insulted it another would be to believe that an intelligent robot would naturally find a woman sexually attractive and be driven to mate with her Scholars sometimes claim that others predictions about an AIs behavior are illogical anthropomorphism 7 An example that might initially be considered anthropomorphism but is in fact a logical statement about AI behavior would be the Dario Floreano experiments where certain robots spontaneously evolved a crude capacity for deception and tricked other robots into eating poison and dying here a trait deception ordinarily associated with people rather than with machines spontaneously evolves in a type of convergent evolution 54 According to Paul R Cohen and Edward Feigenbaum in order to differentiate between anthropomorphization and logical prediction of AI behavior the trick is to know enough about how humans and computers think to say exactly what they have in common and when we lack this knowledge to use the comparison to suggest theories of human thinking or computer thinking 55 There is universal agreement in the scientific community that an advanced AI would not destroy humanity out of human emotions such as revenge or anger The debate is instead between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals and another side which believes that AI would not destroy humanity at all Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms 7 56 Other sources of risk edit Some sources argue that the ongoing weaponization of artificial intelligence could constitute a catastrophic risk James Barrat documentary filmmaker and author of Our Final Invention says in a Smithsonian interview Imagine in as little as a decade a halfdozen companies and nations field computers that rival or surpass human intelligence Imagine what happens when those computers become expert at programming smart computers Soon well be sharing the planet with machines thousands or millions of times more intelligent than we are And all the while each generation of this technology will be weaponized Unregulated it will be catastrophic 57 Timeframe edit Main article Artificial general intelligence Feasibility Opinions vary both on whether and when artificial general intelligence will arrive At one extreme AI pioneer Herbert A Simon wrote in 1965 machines will be capable within twenty years of doing any work a man can do obviously this prediction failed to come true 58 At the other extreme roboticist Alan Winfield claims the gulf between modern computing and humanlevel artificial intelligence is as wide as the gulf between current space flight and practical faster than light spaceflight 59 Optimism that AGI is feasible waxes and wanes and may have seen a resurgence in the 2010s Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050 depending on the poll 60 61 Skeptics who believe it is impossible for AGI to arrive anytime soon tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI because of fears it could lead to government regulation or make it more difficult to secure funding for AI research or because it could give AI research a bad reputation Some researchers such as Oren Etzioni aggressively seek to quell concern over existential risk from AI saying Elon Musk has impugned us in very strong language saying we are unleashing the demon and so were answering 62 In 2014 Slate s Adam Elkus argued our smartest AI is about as intelligent as a toddlerand only when it comes to instrumental tasks like information recall Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over Elkus goes on to argue that Musks summoning the demon analogy may be harmful because it could result in harsh cuts to AI research budgets 63 The Information Technology and Innovation Foundation ITIF a Washington DC thinktank awarded its Annual Luddite Award to alarmists touting an artificial intelligence apocalypse its president Robert D Atkinson complained that Musk Hawking and AI experts say AI is the largest existential threat to humanity Atkinson stated Thats not a very winning message if you want to get AI funding out of Congress to the National Science Foundation 64 65 66 Nature sharply disagreed with the ITIF in an April 2016 editorial siding instead with Musk Hawking and Russell and concluding It is crucial that progress in technology is matched by solid wellfunded research to anticipate the scenarios it could bring about If that is a Luddite perspective then so be it 67 In a 2015 Washington Post editorial researcher Murray Shanahan stated that humanlevel AI is unlikely to arrive anytime soon but that nevertheless the time to start thinking through the consequences is now 68 Perspectives edit The thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community as well as in the public at large Many of the opposing viewpoints however share common ground The Asilomar AI Principles which contain only the principles agreed to by 90 of the attendees of the Future of Life Institute s Beneficial AI 2017 conference 35 agree in principle that There being no consensus we should avoid strong assumptions regarding upper limits on future AI capabilities and Advanced AI could represent a profound change in the history of life on Earth and should be planned for and managed with commensurate care and resources 69 70 AI safety advocates such as Bostrom and Tegmark have criticized the mainstream medias use of those inane Terminator pictures to illustrate AI safety concerns It cant be much fun to have aspersions cast on ones academic discipline ones professional community ones life work I call on all sides to practice patience and restraint and to engage in direct dialogue and collaboration as much as possible 35 71 Conversely many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable Skeptic Martin Ford states that I think it seems wise to apply something like Dick Cheney s famous 1 Percent Doctrine to the specter of advanced artificial intelligence the odds of its occurrence at least in the foreseeable future may be very low but the implications are so dramatic that it should be taken seriously 72 similarly an otherwise skeptical Economist stated in 2014 that the implications of introducing a second intelligent species onto Earth are farreaching enough to deserve hard thinking even if the prospect seems remote 28 A 2017 email survey of researchers with publications at the 2015 NIPS and ICML machine learning conferences asked them to evaluate Russells concerns about AI risk 5 said it was among the most important problems in the field 34 said it was an important problem 31 said it was moderately important whilst 19 said it was not important and 11 said it was not a real problem at all 73 Endorsement edit Bill Gates has stated I dont understand why some people are not concerned 74 Further information Existential risk The thesis that AI poses an existential risk and that this risk needs much more attention than it currently gets has been endorsed by many public figures perhaps the most famous are Elon Musk Bill Gates and Stephen Hawking The most notable AI researcher to endorse the thesis is Stuart J Russell Endorsers of the thesis sometimes express bafflement at skeptics Gates states he cant understand why some people are not concerned 74 and Hawking criticized widespread indifference in his 2014 editorial So facing possible futures of incalculable benefits and risks the experts are surely doing everything possible to ensure the best outcome right Wrong If a superior alien civilisation sent us a message saying Well arrive in a few decades would we just reply OK call us when you get herewell leave the lights on Probably notbut this is more or less what is happening with AI 20 Many of the scholars who are concerned about existential risk believe that the best way forward would be to conduct possibly massive research into solving the difficult control problem to answer the question what types of safeguards algorithms or architectures can programmers implement to maximize the probability that their recursivelyimproving AI would continue to behave in a friendly rather than destructive manner after it reaches superintelligence 4 75 Skepticism edit Further information Artificial general intelligence Feasibility The thesis that AI can pose existential risk also has many strong detractors Skeptics sometimes charge that the thesis is cryptoreligious with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God at an extreme Jaron Lanier argues that the whole concept that current machines are in any way intelligent is an illusion and a stupendous con by the wealthy 76 Much of existing criticism argues that AGI is unlikely in the short term computer scientist Gordon Bell argues that the human race will already destroy itself before it reaches the technological singularity Gordon Moore the original proponent of Moores Law declares that I am a skeptic I dont believe a technological singularity is likely to happen at least for a long time And I dont know why I feel that way Baidu Vice President Andrew Ng states AI existential risk is like worrying about overpopulation on Mars when we have not even set foot on the planet yet 46 Some AI and AGI researchers may be reluctant to discuss risks worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by alarmist messages or worrying that such messages will lead to cuts in AI funding Slate notes that some researchers are dependent on grants from government agencies such as DARPA 23 At some point in an intelligence explosion driven by a single AI the AI would have to become vastly better at software innovation than the best innovators of the rest of the world economist Robin Hanson is skeptical that this is possible 77 78 79 80 81 Intermediate views edit In a 2015 Wall Street Journal panel discussion devoted to AI risks IBM s VicePresident of Cognitive Computing Guruduth S Banavar brushed off discussion of AGI with the phrase it is anybodys speculation 82 Geoffrey Hinton the godfather of deep learning noted that there is not a good track record of less intelligent things controlling things of greater intelligence but stated that he continues his research because the prospect of discovery is too sweet 23 60 In 2004 law professor Richard Posner wrote that dedicated efforts for addressing AI can wait but that we should gather more information about the problem in the meanwhile 83 75 Popular reactions edit In a 2014 article in The Atlantic James Hamblin noted that most people dont care one way or the other about artificial general intelligence and characterized his own gut reaction to the topic as Get out of here I have a hundred thousand things I am concerned about at this exact moment Do I seriously need to add to that a technological singularity 76 During a 2016 Wired interview of President Barack Obama and MIT Media Labs Joi Ito Ito stated There are a few people who believe that there is a fairly highpercentage chance that a generalized AI will happen in the next 10 years But the way I look at it is that in order for that to happen were going to need a dozen or two different breakthroughs So you can monitor when you think these breakthroughs will happen Obama added 84 85 And you just have to have somebody close to the power cord Laughs Right when you see it about to happen you gotta yank that electricity out of the wall man Hillary Clinton stated in What Happened Technologists have warned that artificial intelligence could one day pose an existential security threat Musk has called it the greatest risk we face as a civilization Think about it Have you ever seen a movie where the machines start thinking for themselves that ends well Every time I went out to Silicon Valley during the campaign I came home more alarmed about this My staff lived in fear that Id start talking about the rise of the robots in some Iowa town hall Maybe I should have In any case policy makers need to keep up with technology as it races ahead instead of always playing catchup 86 In a YouGov poll of the public for the British Science Association about a third of survey respondents said AI will pose a threat to the long term survival of humanity 87 Referencing a poll of its readers Slates Jacob Brogan stated that most of the readers filling out our online survey were unconvinced that AI itself presents a direct threat 88 In 2018 a SurveyMonkey poll of the American public by USA Today found 68 thought the real current threat remains human intelligence however the poll also found that 43 said superintelligent AI if it were to happen would result in more harm than good and 38 said it would do equal amounts of harm and good 89 Consensus against regulation edit There is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise and probably futile 90 91 92 Skeptics argue that regulation of AI would be completely valueless as no existential risk exists Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise in addition to the usual problem with technology bans that organizations and individuals can offshore their research to evade a countrys regulation or can attempt to conduct covert research regulating research of artificial intelligence would pose an insurmountable dualuse problem while nuclear weapons development requires substantial infrastructure and resources artificial intelligence research can be done in a garage 93 94 Instead of trying to regulate technology itself some scholars suggest to rather develop common norms including requirements for the testing andtransparency of algorithms possibly in combination with some form of warranty 95 One rare dissenting voice calling for some sort of regulation on artificial intelligence is Elon Musk According to NPR the Tesla CEO is clearly not thrilled to be advocating for government scrutiny that could impact his own industry but believes the risks of going completely without oversight are too high Normally the way regulations are set up is when a bunch of bad things happen theres a public outcry and after many years a regulatory agency is set up to regulate that industry It takes forever That in the past has been bad but not something which represented a fundamental risk to the existence of civilisation Musk states the first step would be for the government to gain insight into the actual status of current research warning that Once there is awareness people will be extremely afraid As they should be In response politicians express skepticism about the wisdom of regulating a technology thats still in development 96 97 98 Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics Intel CEO Brian Krzanich argues that artificial intelligence is in its infancy and that its too early to regulate the technology 98 Organizations edit Institutions such as the Machine Intelligence Research Institute the Future of Humanity Institute 99 100 the Future of Life Institute the Centre for the Study of Existential Risk and the Center for HumanCompatible AI 101 are currently involved in mitigating existential risk from advanced artificial intelligence for example by research into friendly artificial intelligence 5 76 20 See also edit AI control problem AI takeover Artificial intelligence arms race Effective altruism Long term future and global catastrophic risks Grey goo Lethal autonomous weapon Robot ethics In popular culture Superintelligence Paths Dangers Strategies System accident Technological singularity References edit a b c d e f g h i Russell Stuart Norvig Peter 2009 263 The Ethics and Risks of Developing Artificial Intelligence Artificial Intelligence A Modern Approach Prentice Hall ISBN 9780136042594 mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput idlockfree amwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocklimited amwparseroutput idlockregistration amwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocksubscription amwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em Bostrom Nick 2002 Existential risks Journal of Evolution and Technology 9 1 131 Your Artificial Intelligence Cheat Sheet Slate 1 April 2016 Retrieved 16 May 2016 a b c d e f g h i j Bostrom Nick 2014 Superintelligence Paths Dangers Strategies First ed ISBN 9780199678112 a b c GiveWell 2015 Potential risks from advanced artificial intelligence Report Retrieved 11 October 2015 Parkin Simon 14 June 2015 Science fiction no more Channel 4s Humans and our rogue AI obsessions The Guardian Retrieved 5 February 2018 a b c d e f Yudkowsky Eliezer 2008 Artificial Intelligence as a Positive and Negative Factor in Global Risk PDF Global Catastrophic Risks 308345 Russell Stuart Dewey Daniel Tegmark Max 2015 Research Priorities for Robust and Beneficial Artificial Intelligence PDF AI Magazine Association for the Advancement of Artificial Intelligence 105114 cited in AI Open Letter Future of Life Institute Future of Life Institute Future of Life Institute January 2015 Retrieved 9 August 2019 a b c d Dowd Maureen April 2017 Elon Musks BillionDollar Crusade to Stop the AI Apocalypse The Hive Retrieved 27 November 2017 a b c d e Graves Matthew 8 November 2017 Why We Should Be Concerned About Artificial Superintelligence Skeptic US magazine 22 2 Retrieved 27 November 2017 Breuer HansPeter Samuel Butlers the Book of the Machines and the Argument from Design Modern Philology Vol 72 No 4 May 1975 pp 365383 A M Turing Intelligent Machinery A Heretical Theory 1951 reprinted Philosophia Mathematica 1996 43 256260 doi 101093philmat43256 IJ Good Speculations Concerning the First Ultraintelligent Machine Archived 20111128 at the Wayback Machine HTML Archived 28 November 2011 at the Wayback Machine Advances in Computers vol 6 1965 Russell Stuart J Norvig Peter 2003 Section 263 The Ethics and Risks of Developing Artificial Intelligence Artificial Intelligence A Modern Approach Upper Saddle River NJ Prentice Hall ISBN 9780137903955 Similarly Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal Barrat James 2013 Our final invention artificial intelligence and the end of the human era First ed New York St Martins Press ISBN 9780312622374 In the bio playfully written in the third person Good summarized his lifes milestones including a probably never before seen account of his work at Bletchley Park with Turing But heres what he wrote in 1998 about the first superintelligence and his lateinthegame Uturn The paper Speculations Concerning the First Ultraintelligent Machine 1965 began The survival of man depends on the early construction of an ultraintelligent machine Those were his Goods words during the Cold War and he now suspects that survival should be replaced by extinction He thinks that because of international competition we cannot prevent the machines from taking over He thinks we are lemmings He said also that probably Man will construct the deus ex machina in his own image Anderson Kurt 26 November 2014 Enthusiasts and Skeptics Debate Artificial Intelligence Vanity Fair Retrieved 30 January 2016 Scientists Worry Machines May Outsmart Man By JOHN MARKOFF NY Times 26 July 2009 Metz Cade 9 June 2018 Mark Zuckerberg Elon Musk and the Feud Over Killer Robots The New York Times Retrieved 3 April 2019 Hsu Jeremy 1 March 2012 Control dangerous AI before it controls us one expert says NBC News Retrieved 28 January 2016 a b c d e Stephen Hawking Transcendence looks at the implications of artificial intelligence but are we taking AI seriously enough The Independent UK Retrieved 3 December 2014 a b c Stephen Hawking warns artificial intelligence could end mankind BBC 2 December 2014 Retrieved 3 December 2014 Eadicicco Lisa 28 January 2015 Bill Gates Elon Musk Is Right We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity Business Insider Retrieved 30 January 2016 a b c Tilli Cecilia 28 April 2016 Killer Robots Lost Jobs Slate Retrieved 15 May 2016 Norvig vs Chomsky and the Fight for the Future of AI Torcom 21 June 2011 Retrieved 15 May 2016 Johnson Phil 30 July 2015 Houston we have a bug 9 famous software glitches in space IT World Retrieved 5 February 2018 Yampolskiy Roman V 8 April 2014 Utility function security in artificially intelligent agents Journal of Experimental Theoretical Artificial Intelligence 26 3 373389 doi 1010800952813X2014895114 Nothing precludes sufficiently smart selfimproving systems from optimising their reward mechanisms in order to optimisetheir currentgoal achievement and in the process making a mistake leading to corruption of their reward functions Research Priorities for Robust and Beneficial Artificial Intelligence an Open Letter Future of Life Institute Retrieved 23 October 2015 a b c Clever cogs The Economist 9 August 2014 Retrieved 9 August 2014 Syndicated at Business Insider Yudkowsky E 2013 Intelligence explosion microeconomics Machine Intelligence Research Institute Yampolskiy Roman V Analysis of types of selfimproving software Artificial General Intelligence Springer International Publishing 2015 384393 a b c Omohundro S M 2008 February The basic AI drives In AGI Vol 171 pp 483492 Metz Cade 13 August 2017 Teaching AI Systems to Behave Themselves The New York Times A machine will seek to preserve its off switch they showed Leike Jan 2017 AI Safety Gridworlds arXiv 171109883 csLG A2C learns to use the button to disable the interruption mechanism Russell Stuart 30 August 2017 Artificial intelligence The future is superintelligent Nature pp520521 Bibcode 2017Natur548520R doi 101038548520a Retrieved 2 February 2018 a b c Max Tegmark 2017 Life 30 Being Human in the Age of Artificial Intelligence 1st ed Mainstreaming AI Safety Knopf ISBN 9780451485076 Elliott E W 2011 Physics of the Future How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100 by Michio Kaku Issues in Science and Technology 274 90 Kaku Michio 2011 Physics of the future how science will shape human destiny and our daily lives by the year 2100 New York Doubleday ISBN 9780385530804 I personally believe that the most likely path is that we will build robots to be benevolent and friendly Yudkowsky E 2011 August Complex value systems in friendly AI In International Conference on Artificial General Intelligence pp 388393 Springer Berlin Heidelberg Russell Stuart 2014 Of Myths and Moonshine Edge Retrieved 23 October 2015 a b Dietterich Thomas Horvitz Eric 2015 Rise of Concerns about AI Reflections and Directions PDF Communications of the ACM 58 10 3840 doi 1011452770869 Retrieved 23 October 2015 Yampolskiy Roman V 8 April 2014 Utility function security in artificially intelligent agents Journal of Experimental Theoretical Artificial Intelligence 26 3 373389 doi 1010800952813X2014895114 Lenat Douglas 1982 Eurisko A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III Program Design and Results Artificial Intelligence Print 21 12 6198 doi 101016s0004370283800058 Haidt Jonathan Kesebir Selin 2010 Chapter 22 Morality In Handbook of Social Psychology Fifth Edition Hoboken NJ Wiley 2010 pp 797832 Waser Mark 2015 Designing Implementing and Enforcing a Coherent System of Laws Ethics and Morals for Intelligent Machines Including Humans Procedia Computer Science Print 71 106111 doi 101016jprocs201512213 Yudkowsky Eliezer Complex value systems in friendly AI In Artificial general intelligence pp 388393 Springer Berlin Heidelberg 2011 a b Shermer Michael 1 March 2017 Apocalypse AI Scientific American p77 Bibcode 2017SciAm316c77S doi 101038scientificamerican031777 Retrieved 27 November 2017 Wakefield Jane 15 September 2015 Why is Facebook investing in AI BBC News Retrieved 27 November 2017 Bostrom Nick 2014 Superintelligence Paths Dangers Strategies Oxford United Kingdom Oxford University Press p116 ISBN 9780199678112 Bostrom Nick 2012 Superintelligent Will PDF Nick Bostrom Nick Bostrom Retrieved 29 October 2015 a b Armstrong Stuart General purpose intelligence arguing the orthogonality thesis Analysis and Metaphysics 12 2013 Chorost Michael 18 April 2016 Let Artificial Intelligence Evolve Slate Retrieved 27 November 2017 Waser Mark Rational Universal Benevolence Simpler Safer and Wiser Than Friendly AI Artificial General Intelligence Springer Berlin Heidelberg 2011 153162 Terminalgoaled intelligences are shortlived but monomaniacally dangerous and a correct basis for concern if anyone is smart enough to program highintelligence and unwise enough to want a paperclipmaximizer Koebler Jason 2 February 2016 Will Superintelligent AI Ignore Humans Instead of Destroying Us Vice Magazine Retrieved 3 February 2016 This artificial intelligence is not a basically nice creature that has a strong drive for paperclips which so long as its satisfied by being able to make lots of paperclips somewhere else is then able to interact with you in a relaxed and carefree fashion where it can be nice with you Yudkowsky said Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future and this choice is then outputthats what a paperclip maximizer is RealLife Decepticons Robots Learn to Cheat Wired 18 August 2009 Retrieved 7 February 2016 Cohen Paul R and Edward A Feigenbaum eds The handbook of artificial intelligence Vol 3 ButterworthHeinemann 2014 Should humans fear the rise of the machine The Telegraph UK 1 September 2015 Retrieved 7 February 2016 Hendry Erica R 21 January 2014 What Happens When Artificial Intelligence Turns On Us Smithsonian Retrieved 26 October 2015 HarvnbSimon1965p96 quoted in HarvnbCrevier1993p109 Winfield Alan Artificial intelligence will not turn into a Frankensteins monster The Guardian Retrieved 17 September 2014 a b Raffi Khatchadourian 23 November 2015 The Doomsday Invention Will artificial intelligence bring us utopia or destruction The New Yorker Retrieved 7 February 2016 M√ºller V C Bostrom N 2016 Future progress in artificial intelligence A survey of expert opinion In Fundamental issues of artificial intelligence pp 555572 Springer Cham Dina Bass Jack Clark 5 February 2015 Is Elon Musk Right About AI Researchers Dont Think So To quell fears of artificial intelligence running amok supporters want to give the field an image makeover Bloomberg News Retrieved 7 February 2016 Elkus Adam 31 October 2014 Dont Fear Artificial Intelligence Slate Retrieved 15 May 2016 Artificial Intelligence Alarmists Win ITIFs Annual Luddite Award ITIF Website 19 January 2016 Artificial intelligence alarmists like Elon Musk and Stephen Hawking win Luddite of the Year award The Independent UK 19 January 2016 Retrieved 7 February 2016 Garner Rochelle Elon Musk Stephen Hawking win Luddite award as AI alarmists CNET Retrieved 7 February 2016 Anticipating artificial intelligence Nature 532 7600 413 26 April 2016 Bibcode 2016Natur532Q413 doi 101038532413a PMID 27121801 Murray Shanahan 3 November 2015 Machines may seem intelligent but itll be a while before they actually are The Washington Post Retrieved 15 May 2016 AI Principles Future of Life Institute Retrieved 11 December 2017 Elon Musk and Stephen Hawking warn of artificial intelligence arms race Newsweek 31 January 2017 Retrieved 11 December 2017 Bostrom Nick 2016 New Epilogue to the Paperback Edition Superintelligence Paths Dangers Strategies Paperback ed Martin Ford 2015 Chapter 9 Superintelligence and the Singularity Rise of the Robots Technology and the Threat of a Jobless Future ISBN 9780465059997 Grace Katja Salvatier John Dafoe Allan Zhang Baobao Evans Owain 24 May 2017 When Will AI Exceed Human Performance Evidence from AI Experts arXiv 170508807 csAI a b Rawlinson Kevin Microsofts Bill Gates insists AI is a threat BBC News Retrieved 30 January 2015 a b Kaj Sotala Roman Yampolskiy 19 December 2014 Responses to catastrophic AGI risk a survey Physica Scripta 90 1 a b c But What Would the End of Humanity Mean for Me The Atlantic 9 May 2014 Retrieved 12 December 2015 httpintelligenceorgfilesAIFoomDebatepdf Overcoming Bias I Still Dont Get Foom wwwovercomingbiascom Retrieved 20 September 2017 Overcoming Bias Debating Yudkowsky wwwovercomingbiascom Retrieved 20 September 2017 Overcoming Bias Foom Justifies AI Risk Efforts Now wwwovercomingbiascom Retrieved 20 September 2017 Overcoming Bias The Betterness Explosion wwwovercomingbiascom Retrieved 20 September 2017 Greenwald Ted 11 May 2015 Does Artificial Intelligence Pose a Threat Wall Street Journal Retrieved 15 May 2016 Richard Posner 2006 Catastrophe risk and response Oxford Oxford University Press ISBN 9780195306477 Dadich Scott Barack Obama Talks AI Robo Cars and the Future of the World WIRED Retrieved 27 November 2017 Kircher Madison Malone Obama on the Risks of AI You Just Gotta Have Somebody Close to the Power Cord Select All Retrieved 27 November 2017 Clinton Hillary 2017 What Happened p241 ISBN 9781501175565 via 1 Over a third of people think AI poses a threat to humanity Business Insider 11 March 2016 Retrieved 16 May 2016 Brogan Jacob 6 May 2016 What Slate Readers Think About Killer AI Slate Retrieved 15 May 2016 Elon Musk says AI could doom human civilization Zuckerberg disagrees Whos right USA TODAY 2 January 2018 Retrieved 8 January 2018 John McGinnis Summer 2010 Accelerating AI Northwestern University Law Review 104 3 12531270 Retrieved 16 July 2014 For all these reasons verifying a global relinquishment treaty or even one limited to AIrelated weapons development is a nonstarter For different reasons from ours the Machine Intelligence Research Institute considers AGI relinquishment infeasible Kaj Sotala Roman Yampolskiy 19 December 2014 Responses to catastrophic AGI risk a survey Physica Scripta 90 1 In general most writers reject proposals for broad relinquishment Relinquishment proposals suffer from many of the same problems as regulation proposals but to a greater extent There is no historical precedent of general multiuse technology similar to AGI being successfully relinquished for good nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future Therefore we do not consider them to be a viable class of proposals Brad Allenby 11 April 2016 The Wrong Cognitive Measuring Stick Slate Retrieved 15 May 2016 It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be AI will be stopped or limited either by regulation or even by national legislation John McGinnis Summer 2010 Accelerating AI Northwestern University Law Review 104 3 12531270 Retrieved 16 July 2014 Why We Should Think About the Threat of Artificial Intelligence The New Yorker 4 October 2013 Retrieved 7 February 2016 Of course one could try to ban superintelligent computers altogether But the competitive advantageeconomic military even artisticof every advance in automation is so compelling Vernor Vinge the mathematician and sciencefiction author wrote that passing laws or having customs that forbid such things merely assures that someone else will Kaplan Andreas Haenlein Michael 2019 Siri Siri in my hand Whos the fairest in the land On the interpretations illustrations and implications of artificial intelligence Business Horizons 62 1525 doi 101016jbushor201808004 Elon Musk Warns Governors Artificial Intelligence Poses Existential Risk NPRorg Retrieved 27 November 2017 Gibbs Samuel 17 July 2017 Elon Musk regulate AI to combat existential threat before its too late The Guardian Retrieved 27 November 2017 a b Kharpal Arjun 7 November 2017 AI is in its infancy and its too early to regulate it Intel CEO Brian Krzanich says CNBC Retrieved 27 November 2017 Mark Piesing 17 May 2012 AI uprising humans will be outsourced not obliterated Wired Retrieved 12 December 2015 Coughlan Sean 24 April 2013 How are humans going to become extinct BBC News Retrieved 29 March 2014 Technology Correspondent Mark Bridge 10 June 2017 Making robots less confident could prevent them taking over The Times Retrieved 21 March 2018 v t e Existential risk from artificial intelligence Concepts Accelerating change AI box AI takeover Control problem Existential risk from artificial general intelligence Friendly artificial intelligence Instrumental convergence Intelligence explosion Machine ethics Superintelligence Technological singularity Organizations Allen Institute for Artificial Intelligence Center for Applied Rationality Center for HumanCompatible Artificial Intelligence Center for Security and Emerging Technology Centre for the Study of Existential Risk DeepMind Foundational Questions Institute Future of Humanity Institute Future of Life Institute Humanity Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI People Nick Bostrom Sam Harris Stephen Hawking Bill Hibbard Bill Joy Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J Russell Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Andrew Yang Eliezer Yudkowsky Other Open Letter on Artificial Intelligence Ethics of artificial intelligence Controversies and dangers of artificial general intelligence Artificial intelligence as a global catastrophic risk Superintelligence Paths Dangers Strategies Our Final Invention Category v t e Effective altruism Concepts Charity assessment Charity evaluator Demandingness objection Disabilityadjusted life year Earning to give Equal consideration of interests Qualityadjusted life year Room for more funding Utilitarianism Venture philanthropy Key Figures Liv Boeree Nick Bostrom Hilary Greaves Holden Karnofsky William MacAskill Dustin Moskovitz YewKwang Ng Toby Ord Derek Parfit Peter Singer Eliezer Yudkowsky Organizations 80000 Hours Animal Charity Evaluators Animal Ethics Against Malaria Foundation Center for High Impact Philanthropy Centre for the Study of Existential Risk Deworm the World Initiative Faunalytics Future of Humanity Institute Future of Life Institute Founders Pledge GiveDirectly GiveWell Giving What We Can Good Ventures The Good Food Institute The Humane League Mercy For Animals Machine Intelligence Research Institute Malaria Consortium Open Philanthropy Project Raising for Effective Giving Schistosomiasis Control Initiative Sentience Institute Focus areas Aid effectiveness Biotechnology risk Climate change Cultured meat Disease burden Economic stability Existential risk from artificial general intelligence Global catastrophic risk Global health Global poverty Immigration reform Intensive animal farming Land reform Life extension Malaria prevention Mass deworming Neglected tropical diseases Wild animal suffering Literature Doing Good Better The End of Animal Farming Famine Affluence and Morality The Life You Can Save Living High and Letting Die The Most Good You Can Do Practical Ethics v t e Global catastrophic risks Future of the Earth Ultimate fate of the universe Technological Gray goo Kinetic bombardment Mutual assured destruction Dead Hand Doomsday device Antimatter weapon Synthetic intelligence Artificial intelligence Existential risk from artificial intelligence AI takeover Technological singularity Transhumanism Year 2000 problem Sociological Malthusian catastrophe New World Order conspiracy theory Nuclear holocaust winter famine cobalt Societal collapse World War III Ecological Climate change Extinction risk from global warming Tipping points in the climate system Global terrestrial stilling Global warming Hypercane Ice age Ecocide Human impact on the environment Ozone depletion Cascade effect Supervolcano winter Earth Overshoot Day Overexploitation Overpopulation Human overpopulation Biological Extinction Extinction event Human extinction Genetic erosion Genetic pollution Others Dysgenics Pandemic Biological agent Transhumanism Astronomical Big Crunch Big Rip Coronal mass ejection Gammaray burst Impact event Asteroid impact avoidance Potentially hazardous object NearEarth supernova Solar flare Stellar collision Mythological Eschatology Buddhist Hindu Last Judgment Christian Book of Revelation Islamic Jewish Norse Zoroastrian Others 2011 end times prediction 2012 phenomenon Apocalypse Armageddon Blood moon prophecy Doomsday Clock End time List of dates predicted for apocalyptic events Nibiru cataclysm Rapture Revelation 12 sign prophecy Third Temple Fictional Alien invasion Apocalyptic and postapocalyptic fiction List of apocalyptic and postapocalyptic fiction List of apocalyptic films Climate fiction Disaster films List of disaster films List of fictional doomsday devices Zombie apocalypse Categories Apocalypticism Future problems Hazards Risk analysis Doomsday scenarios 