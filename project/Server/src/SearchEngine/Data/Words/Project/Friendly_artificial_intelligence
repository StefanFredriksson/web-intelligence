Friendly artificial intelligence Hypothetical artificial general intelligence that would have a positive effect on humanity It has been suggested that this article be merged with AI control problem Discuss Proposed since August 2019 Artificial intelligence Major goals Knowledge reasoning Planning Machine learning Natural language processing Computer vision Robotics Artificial general intelligence Approaches Symbolic Deep learning Bayesian networks Evolutionary algorithms Philosophy Ethics Existential risk Turing test Chinese room Control problem Friendly AI History Timeline Progress AI winter Technology Applications Projects Programming languages Glossary Glossary v t e A friendly artificial intelligence also friendly AI or FAI is a hypothetical artificial general intelligence AGI that would have a positive effect on humanity It is a part of the ethics of artificial intelligence and is closely related to machine ethics While machine ethics is concerned with how an artificially intelligent agent should behave friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained Contents 1 Etymology and usage 2 Risks of unfriendly AI 3 Coherent extrapolated volition 4 Other approaches 41 Oracle 411 Purpose 412 Advantages 413 Disadvantages 5 Public policy 6 Criticism 7 See also 8 References 9 Further reading 10 External links Etymology and usage edit Eliezer Yudkowsky AI researcher and creator of the term Friendly artificial intelligence The term was coined by Eliezer Yudkowsky 1 who is best known for popularizing the idea 2 3 to discuss superintelligent artificial agents that reliably implement human values Stuart J Russell and Peter Norvig s leading artificial intelligence textbook Artificial Intelligence A Modern Approach describes the idea 4 Yudkowsky 2008 goes into more detail about how to design a Friendly AI He asserts that friendliness a desire not to harm humans should be designed in from the start but that the designers should recognize both that their own designs may be flawed and that the robot will learn and evolve over time Thus the challenge is one of mechanism designto define a mechanism for evolving AI systems under a system of checks and balances and to give the systems utility functions that will remain friendly in the face of such changes Friendly is used in this context as technical terminology and picks out agents that are safe and useful not necessarily ones that are friendly in the colloquial sense The concept is primarily invoked in the context of discussions of recursively selfimproving artificial agents that rapidly explode in intelligence on the grounds that this hypothetical technology would have a large rapid and difficulttocontrol impact on human society 5 Risks of unfriendly AI edit Main article Existential risk from artificial general intelligence The roots of concern about artificial intelligence are very old Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem or the protorobots of Gerbert of Aurillac and Roger Bacon In those stories the extreme intelligence and power of these humanoid creations clash with their status as slaves which by nature are seen as subhuman and cause disastrous conflict 6 By 1942 these themes prompted Isaac Asimov to create the Three Laws of Robotics principles hardwired into all the robots in his fiction intended to prevent them from turning on their creators or allowing them to come to harm 7 In modern times as the prospect of superintelligent AI looms nearer philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity He put it this way Basically we should assume that a superintelligence would be able to achieve whatever goals it has Therefore it is extremely important that the goals we endow it with and its entire motivation system is human friendly Ryszard Michalski a pioneer of machine learning taught his PhD students decades ago that any truly alien mind including a machine mind was unknowable and therefore dangerous to humans citation needed In 2008 Eliezer Yudkowsky called for the creation of friendly AI to mitigate existential risk from advanced artificial intelligence He explains The AI does not hate you nor does it love you but you are made out of atoms which it can use for something else 8 Steve Omohundro says that a sufficiently advanced AI system will unless explicitly counteracted exhibit a number of basic drives such as resource acquisition selfpreservation and continuous selfimprovement because of the intrinsic nature of any goaldriven systems and that these drives will without special precautions cause the AI to exhibit undesired behavior 9 10 Alexander WissnerGross says that AIs driven to maximize their future freedom of action or causal path entropy might be considered friendly if their planning horizon is longer than a certain threshold and unfriendly if their planning horizon is shorter than that threshold 11 12 Luke Muehlhauser writing for the Machine Intelligence Research Institute recommends that machine ethics researchers adopt what Bruce Schneier has called the security mindset Rather than thinking about how a system will work imagine how it could fail For instance he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm 13 Coherent extrapolated volition edit Yudkowsky advances the Coherent Extrapolated Volition CEV model According to him coherent extrapolated volition is peoples choices and the actions people would collectively take if we knew more thought faster were more the people we wished we were and had grown up closer together 14 Rather than a Friendly AI being designed directly by human programmers it is to be designed by a seed AI programmed to first study human nature and then produce the AI which humanity would want given sufficient time and insight to arrive at a satisfactory answer 14 The appeal to an objective through contingent human nature perhaps expressed for mathematical purposes in the form of a utility function or other decisiontheoretic formalism as providing the ultimate criterion of Friendliness is an answer to the metaethical problem of defining an objective morality extrapolated volition is intended to be what humanity objectively would want all things considered but it can only be defined relative to the psychological and cognitive qualities of presentday unextrapolated humanity Other approaches edit Ben Goertzel an artificial general intelligence researcher believes that friendly AI cannot be created with current human knowledge Goertzel suggests humans may instead decide to create an AI Nanny with mildly superhuman intelligence and surveillance powers to protect the human race from existential risks like nanotechnology and to delay the development of other unfriendly artificial intelligences until and unless the safety issues are solved 15 This can also be termed Defensive AI Steve Omohundro has proposed a scaffolding approach to AI safety in which one provably safe AI generation helps build the next provably safe generation 16 Stefan Pernar argues along the lines of Menos paradox to point out that attempting to solve the FAI problem is either pointless or hopeless depending on whether one assumes a universe that exhibits moral realism or not In the former case a transhuman AI would independently reason itself into the proper goal system and assuming the latter designing a friendly AI would be futile to begin with since morals can not be reasoned about 17 Cindy Mason an AI researcher who has also worked with mindbody medicine at Stanford University Medical Center believes neuroplasticity and new discoveries of the hormone oxytocin mean compassionate intelligence is essential in AI systems that exhibit socially positive behaviors She has proposed a set of software engineering principles for engineering kindness that includes a prohuman stance and an architecture for giving robots compassion 18 Oracle edit An oracle is a hypothetical intelligent agent proposed failed verification by Nick Bostrom An oracle is an AI designed to answer questions but that is somehow prevented from ever gaining any implicit goals or subgoals that involve modifying the world outside of its box 19 20 Purpose edit Oracles are questionanswering systems that handle domainspecific problems such as mathematics or domaingeneral problems that might encompass the whole range of human knowledge Advantages edit Because it is a type of AI box an oracle is limited in its interactions with the physical world and can be programmed to halt if a limit on time or computing resources is reached before it finishes answering a question Scenarios like the paperclip maximizer problem could therefore be avoided Because of these limitations it may be wise to build an oracle as a precursor to a superintelligent AI It could tell humans how to successfully build a strong AI and perhaps provide answers to difficult moral and philosophical problems requisite to the success of the project Disadvantages edit An oracle might discover that human ontological categories are predicated on fundamental misconceptions and become unable to express itself properly to its questioners 21 Oracles may not be truthful possibly lying to promote hidden agendas To mitigate this Bostrom suggests building multiple oracles all slightly different and comparing their answers to reach a consensus 22 Public policy edit James Barrat author of Our Final Invention suggested that a publicprivate partnership has to be created to bring AImakers together to share ideas about securitysomething like the International Atomic Energy Agency but in partnership with corporations He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA which discussed risks of biotechnology 16 John McGinnis encourages governments to accelerate friendly AI research Because the goalposts of friendly AI are not necessarily eminent he suggests a model similar to the National Institutes of Health where Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards McGinnis feels that peer review is better than regulation to address technical issues that are not possible to capture through bureaucratic mandates McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute which generally aims to avoid government involvement in friendly AI 23 According to Gary Marcus the annual amount of money being spent on developing machine morality is tiny 24 Criticism edit See also Technological singularity Criticisms Some critics believe that both humanlevel AI and superintelligence are unlikely and that therefore friendly AI is unlikely Writing in The Guardian Alan Winfield compares humanlevel artificial intelligence with fasterthanlight travel in terms of difficulty and states that while we need to be cautious and prepared given the stakes involved we dont need to be obsessing about the risks of superintelligence 25 Boyles and Joaquin on the other hand argue that Luke Muehlhauser and Nick Bostrom s proposal to create friendly AIs appear to be bleak This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had 26 In an article in AI Society Boyles and Joaquin maintain that such AIs would not be that friendly considering the following the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine the difficulty of cashing out the set of moral valuesthat is those that a more ideal than the ones human beings possess at present and the apparent disconnect between counterfactual antecedents and ideal value consequent 27 Some philosophers claim that any truly rational agent whether artificial or human will naturally be benevolent in this view deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful 28 Other critics question whether it is possible for an artificial intelligence to be friendly Adam Keiper and Ari N Schulman editors of the technology journal The New Atlantis say that it will be impossible to ever guarantee friendly behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power They write that the criteria upon which friendly AI theories are based work only when one has not only great powers of prediction about the likelihood of myriad possible outcomes but certainty and consensus on how one values the different outcomes 29 See also edit AI takeover Artificial intelligence arms race Ethics of artificial intelligence Existential risk from artificial general intelligence Intelligence explosion Machine ethics Machine Intelligence Research Institute OpenAI Singularitarianism a moral philosophy advocated by proponents of Friendly AI Technological singularity Three Laws of Robotics References edit Tegmark Max 2014 Life Our Universe and Everything Our Mathematical Universe My Quest for the Ultimate Nature of Reality First ed ISBN 9780307744258 Its owner may cede control to what Eliezer Yudkowsky terms a Friendly AI mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput idlockfree amwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocklimited amwparseroutput idlockregistration amwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocksubscription amwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em Russell Stuart Norvig Peter 2009 Artificial Intelligence A Modern Approach Prentice Hall ISBN 9780136042594 Leighton Jonathan 2011 The Battle for Compassion Ethics in an Apathetic Universe Algora ISBN 9780875868707 Russell Stuart Norvig Peter 2010 Artificial Intelligence A Modern Approach Prentice Hall ISBN 9780136042594 Wallach Wendell Allen Colin 2009 Moral Machines Teaching Robots Right from Wrong Oxford University Press Inc ISBN 9780195374049 Kevin LaGrandeur The Persistent Peril of the Artificial Slave Science Fiction Studies Retrieved 20130506 Isaac Asimov 1964 Introduction The Rest of the Robots Doubleday ISBN 0385090412 Eliezer Yudkowsky 2008 in Artificial Intelligence as a Positive and Negative Factor in Global Risk Omohundro S M 2008 February The basic AI drives In AGI Vol 171 pp 483492 Bostrom Nick 2014 Superintelligence Paths Dangers Strategies Oxford Oxford University Press ISBN 9780199678112 Chapter 7 The Superintelligent Will How Skynet Might Emerge From Simple Physics io9 Published 20130426 WissnerGross A D Freer C E 2013 Causal entropic forces PDF Physical Review Letters 110 16 168702 Bibcode 2013PhRvL110p8702W doi 101103PhysRevLett110168702 PMID 23679649 Muehlhauser Luke 31 Jul 2013 AI Risk and the Security Mindset Machine Intelligence Research Institute Retrieved 15 July 2014 a b Coherent Extrapolated Volition PDF Intelligenceorg Retrieved 20150912 Goertzel Ben Should Humanity Build a Global AI Nanny to Delay the Singularity Until Its Better Understood Journal of consciousness studies 1912 2012 12 a b Hendry Erica R 21 Jan 2014 What Happens When Artificial Intelligence Turns On Us Smithsoniancom Retrieved 15 July 2014 Pernar Stefan The Evolutionary Perspective a Transhuman Philosophy 8th Conference on Artificial General Intelligence in Berlin July 2225 2015 Mason Cindy Engineering Kindness Giving Machines Compassionate Intelligence International Journal of Synthetic Emotions 61 June December 2015 Bostrom Nick 2014 Chapter 10 Oracles genies sovereigns tools page 145 Superintelligence Paths Dangers Strategies Oxford Oxford University Press ISBN 9780199678112 An oracle is a questionanswering system It might accept questions in a natural language and present its answers as text An oracle that accepts only yesno questions could output its best guess with a single bit or perhaps with a few extra bits to represent its degree of confidence An oracle that accepts openended questions would need some metric with which to rank possible truthful answers in terms of their informativeness or appropriateness In either case building an oracle that has a fully domaingeneral ability to answer natural language questions is an AIcomplete problem If one could do that one could probably also build an AI that has a decent ability to understand human intentions as well as human words Armstrong S Sandberg A Bostrom N 2012 Thinking inside the box Controlling and using an oracle ai Minds and Machines 224 299324 Bostrom Nick 2014 Chapter 10 Oracles genies sovereigns tools page 146 Superintelligence Paths Dangers Strategies Oxford Oxford University Press ISBN 9780199678112 What happens if the AI in the course of its intellectual development undergoes the equivalent of a scientific revolution involving a change in its basic ontology We might initially have explicated impact and designated resources using our own ontology postulating the existence of various physical objects such as computers But just as we have abandoned ontological categories that were taken for granted by scientists in previous ages eg phlogiston élan vital and absolute simultaneity so a superintelligent AI might discover that some of our current categories are predicated on fundamental misconceptions The goal system of an AI undergoing an ontological crisis needs to be resilient enough that the spirit of its original goal content is carried over charitably transposed into the new key Bostrom Nick 2014 Chapter 10 Oracles genies sovereigns tools page 147 Superintelligence Paths Dangers Strategies Oxford Oxford University Press ISBN 9780199678112 For example consider the risk that an oracle will answer questions not in a maximally truthful way but in such a way as to subtly manipulate us into promoting its own hidden agenda One way to slightly mitigate this threat could be to create multiple oracles each with a slightly different code and a slightly different information base A simple mechanism could then compare the answers given by the different oracles and only present them for human viewing if all the answers agree McGinnis John O Summer 2010 Accelerating AI Northwestern University Law Review 104 3 12531270 Retrieved 16 July 2014 Marcus Gary 24 November 2012 Moral Machines The New Yorker Retrieved 30 July 2014 Winfield Alan Artificial intelligence will not turn into a Frankensteins monster The Guardian Retrieved 17 September 2014 Muehlhauser Luke Bostrom Nick 2014 Why we need friendly AI Think 13 36 4147 doi 101017S1477175613000316 Boyles Robert James M Joaquin Jeremiah Joven 2019 Why Friendly AIs wont be that Friendly A Friendly Reply to Muehlhauser and Bostrom AI Society 13 doi 101007s00146019009030 Kornai András Bounding the impact of AGI Journal of Experimental Theoretical Artificial Intelligence aheadofprint 2014 122 the essence of AGIs is their reasoning facilities and it is the very logic of their being that will compel them to behave in a moral fashion The real nightmare scenario is one where humans find it advantageous to strongly couple themselves to AGIs with no guarantees against selfdeception Adam Keiper and Ari N Schulman The Problem with Friendly Artificial Intelligence The New Atlantis Retrieved 20120116 Further reading edit Yudkowsky E Artificial Intelligence as a Positive and Negative Factor in Global Risk In Global Catastrophic Risks Oxford University Press 2008 Discusses Artificial Intelligence from the perspective of Existential risk introducing the term Friendly AI In particular Sections 14 give background to the definition of Friendly AI in Section 5 Section 6 gives two classes of mistakes technical and philosophical which would both lead to the accidental creation of nonFriendly AIs Sections 713 discuss further related issues Omohundro S 2008 The Basic AI Drives Appeared in AGI08 Proceedings of the First Conference on Artificial General Intelligence Mason C 2008 HumanLevel AI Requires Compassionate Intelligence Appears in AAAI 2008 Workshop on MetaReasoningThinking About Thinking External links edit Ethical Issues in Advanced Artificial Intelligence by Nick Bostrom What is Friendly AI A brief description of Friendly AI by the Machine Intelligence Research Institute Creating Friendly AI 10 The Analysis and Design of Benevolent Goal Architectures A near booklength description from the MIRI Critique of the MIRI Guidelines on Friendly AI by Bill Hibbard Commentary on MIRIs Guidelines on Friendly AI by Peter Voss The Problem with Friendly Artificial Intelligence On the motives for and impossibility of FAI by Adam Keiper and Ari N Schulman v t e Existential risk from artificial intelligence Concepts Accelerating change AI box AI takeover Control problem Existential risk from artificial general intelligence Friendly artificial intelligence Instrumental convergence Intelligence explosion Machine ethics Superintelligence Technological singularity Organizations Allen Institute for Artificial Intelligence Center for Applied Rationality Center for HumanCompatible Artificial Intelligence Center for Security and Emerging Technology Centre for the Study of Existential Risk DeepMind Foundational Questions Institute Future of Humanity Institute Future of Life Institute Humanity Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI People Nick Bostrom Sam Harris Stephen Hawking Bill Hibbard Bill Joy Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J Russell Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Andrew Yang Eliezer Yudkowsky Other Open Letter on Artificial Intelligence Ethics of artificial intelligence Controversies and dangers of artificial general intelligence Artificial intelligence as a global catastrophic risk Superintelligence Paths Dangers Strategies Our Final Invention Category 