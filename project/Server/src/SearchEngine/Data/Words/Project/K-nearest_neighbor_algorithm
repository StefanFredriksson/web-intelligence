k nearest neighbors algorithm Not to be confused with kmeans clustering Machine learning and data mining Problems Classification Clustering Regression Anomaly detection AutoML Association rules Reinforcement learning Structured prediction Feature engineering Feature learning Online learning Semisupervised learning Unsupervised learning Learning to rank Grammar induction Supervised learning mwparseroutput noboldfontweightnormal classification regression Decision trees Ensembles Bagging Boosting Random forest k NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine RVM Support vector machine SVM Clustering BIRCH CURE Hierarchical k means Expectationmaximization EM DBSCAN OPTICS Meanshift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA tSNE Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection k NN Local outlier factor Artificial neural network Autoencoder Deep learning DeepDream Multilayer perceptron RNN LSTM GRU Restricted Boltzmann machine GAN SOM Convolutional neural network UNet Reinforcement learning Qlearning SARSA Temporal difference TD Theory Biasvariance dilemma Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Machinelearning venues NeurIPS ICML ML JMLR ArXivcsLG Glossary of artificial intelligence Glossary of artificial intelligence Related articles List of datasets for machinelearning research Outline of machine learning v t e In pattern recognition the k nearest neighbors algorithm k NN is a nonparametric method used for classification and regression 1 In both cases the input consists of the k closest training examples in the feature space The output depends on whether k NN is used for classification or regression In kNN classification the output is a class membership An object is classified by a plurality vote of its neighbors with the object being assigned to the class most common among its k nearest neighbors k is a positive integer typically small If k 1 then the object is simply assigned to the class of that single nearest neighbor In kNN regression the output is the property value for the object This value is the average of the values of k nearest neighbors k NN is a type of instancebased learning or lazy learning where the function is only approximated locally and all computation is deferred until classification Both for classification and regression a useful technique can be to assign weights to the contributions of the neighbors so that the nearer neighbors contribute more to the average than the more distant ones For example a common weighting scheme consists in giving each neighbor a weight of 1 d where d is the distance to the neighbor 2 The neighbors are taken from a set of objects for which the class for k NN classification or the object property value for k NN regression is known This can be thought of as the training set for the algorithm though no explicit training step is required A peculiarity of the k NN algorithm is that it is sensitive to the local structure of the data Contents 1 Statistical setting 2 Algorithm 3 Parameter selection 4 The 1 nearest neighbor classifier 5 The weighted nearest neighbour classifier 6 Properties 7 Error rates 8 Metric learning 9 Feature extraction 10 Dimension reduction 11 Decision boundary 12 Data reduction 121 Selection of classoutliers 122 CNN for data reduction 13 k NN regression 14 k NN outlier 15 Validation of results 16 See also 17 References 18 Further reading Statistical setting edit Suppose we have pairs X 1 Y 1 X 2 Y 2 X n Y n displaystyle X1Y1X2Y2dots XnYn taking values in R d 1 2 displaystyle mathbb R dtimes 12 where Y is the class label of X so that X Y r P r displaystyle XYrsim Pr for r 1 2 displaystyle r12 and probability distributions P r displaystyle Pr Given some norm displaystyle cdot on R d displaystyle mathbb R d and a point x R d displaystyle xin mathbb R d let X 1 Y 1 X n Y n displaystyle X1Y1dots XnYn be a reordering of the training data such that X 1 x X n x displaystyle X1xleq dots leq Xnx Algorithm edit Example of k NN classification The test sample green dot should be classified either to blue squares or to red triangles If k 3 solid line circle it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle If k 5 dashed line circle it is assigned to the blue squares 3 squares vs 2 triangles inside the outer circle The training examples are vectors in a multidimensional feature space each with a class label The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples In the classification phase k is a userdefined constant and an unlabeled vector a query or test point is classified by assigning the label which is most frequent among the k training samples nearest to that query point A commonly used distance metric for continuous variables is Euclidean distance For discrete variables such as for text classification another metric can be used such as the overlap metric or Hamming distance In the context of gene expression microarray data for example k NN has been employed with correlation coefficients such as Pearson and Spearman as a metric 3 Often the classification accuracy of k NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis A drawback of the basic majority voting classification occurs when the class distribution is skewed That is examples of a more frequent class tend to dominate the prediction of the new example because they tend to be common among the k nearest neighbors due to their large number 4 One way to overcome this problem is to weight the classification taking into account the distance from the test point to each of its k nearest neighbors The class or value in regression problems of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point Another way to overcome skew is by abstraction in data representation For example in a selforganizing map SOM each node is a representative a center of a cluster of similar points regardless of their density in the original training data K NN can then be applied to the SOM Parameter selection edit The best choice of k depends upon the data generally larger values of k reduces effect of the noise on the classification 5 but make boundaries between classes less distinct A good k can be selected by various heuristic techniques see hyperparameter optimization The special case where the class is predicted to be the class of the closest training sample ie when k 1 is called the nearest neighbor algorithm The accuracy of the k NN algorithm can be severely degraded by the presence of noisy or irrelevant features or if the feature scales are not consistent with their importance Much research effort has been put into selecting or scaling features to improve classification A particularly popular citation needed approach is the use of evolutionary algorithms to optimize feature scaling 6 Another popular approach is to scale features by the mutual information of the training data with the training classes citation needed In binary two class classification problems it is helpful to choose k to be an odd number as this avoids tied votes One popular way of choosing the empirically optimal k in this setting is via bootstrap method 7 The 1 nearest neighbor classifier edit The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space that is C n 1 n n x Y 1 displaystyle Cn1nnxY1 As the size of training data set approaches infinity the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate the minimum achievable error rate given the distribution of the data The weighted nearest neighbour classifier edit The k nearest neighbour classifier can be viewed as assigning the k nearest neighbours a weight 1 k displaystyle 1k and all others 0 weight This can be generalised to weighted nearest neighbour classifiers That is where the i th nearest neighbour is assigned a weight w n i displaystyle wni with i 1 n w n i 1 displaystyle sum i1nwni1 An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds 8 Let C n w n n displaystyle Cnwnn denote the weighted nearest classifier with weights w n i i 1 n displaystyle wnii1n Subject to regularity conditions further explanation needed on the class distributions the excess risk has the following asymptotic expansion 9 R R C n w n n R R C B a y e s B 1 s n 2 B 2 t n 2 1 o 1 displaystyle mathcal Rmathcal RCnwnnmathcal Rmathcal RCBayesleftB1sn2B2tn2right1o1 for constants B 1 displaystyle B1 and B 2 displaystyle B2 where s n 2 i 1 n w n i 2 displaystyle sn2sum i1nwni2 and t n n 2 d i 1 n w n i i 1 2 d i 1 1 2 d displaystyle tnn2dsum i1nwnii12di112d The optimal weighting scheme w n i i 1 n displaystyle wnii1n that balances the two terms in the display above is given as follows set k B n 4 d 4 displaystyle klfloor Bnfrac 4d4rfloor w n i 1 k 1 d 2 d 2 k 2 d i 1 2 d i 1 1 2 d displaystyle wnifrac 1kleft1frac d2frac d2k2di12di112dright for i 1 2 k displaystyle i12dots k and w n i 0 displaystyle wni0 for i k 1 n displaystyle ik1dots n With optimal weights the dominant term in the asymptotic expansion of the excess risk is O n 4 d 4 displaystyle mathcal Onfrac 4d4 Similar results are true when using a bagged nearest neighbour classifier Properties edit k NN is a special case of a variablebandwidth kernel density balloon estimator with a uniform kernel 10 11 The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples but it is computationally intensive for large training sets Using an approximate nearest neighbor search algorithm makes k NN computationally tractable even for large data sets Many nearest neighbor search algorithms have been proposed over the years these generally seek to reduce the number of distance evaluations actually performed k NN has some strong consistency results As the amount of data approaches infinity the twoclass k NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate the minimum achievable error rate given the distribution of the data 12 Various improvements to the k NN speed are possible by using proximity graphs 13 For multiclass k NN classification Cover and Hart 1967 prove an upper bound error rate of R R k N N R 2 M R M 1 displaystyle R leq Rkmathrm NN leq Rleft2frac MRM1right where R displaystyle R is the Bayes error rate which is the minimal error rate possible R k N N displaystyle RkNN is the k NN error rate and M is the number of classes in the problem For M 2 displaystyle M2 and as the Bayesian error rate R displaystyle R approaches zero this limit reduces to not more than twice the Bayesian error rate Error rates edit There are many results on the error rate of the k nearest neighbour classifiers 14 The k nearest neighbour classifier is strongly that is for any joint distribution on X Y displaystyle XY consistent provided k k n displaystyle kkn diverges and k n n displaystyle knn converges to zero as n displaystyle nto infty Let C n k n n displaystyle Cnknn denote the k nearest neighbour classifier based on a training set of size n Under certain regularity conditions the excess risk yields the following asymptotic expansion 9 R R C n k n n R R C B a y e s B 1 1 k B 2 k n 4 d 1 o 1 displaystyle mathcal Rmathcal RCnknnmathcal Rmathcal RCBayesleftB1frac 1kB2leftfrac knright4dright1o1 for some constants B 1 displaystyle B1 and B 2 displaystyle B2 The choice k B n 4 d 4 displaystyle klfloor Bnfrac 4d4rfloor offers a trade off between the two terms in the above display for which the k displaystyle k nearest neighbour error converges to the Bayes error at the optimal minimax rate O n 4 d 4 displaystyle mathcal Onfrac 4d4 Metric learning edit The Knearest neighbor classification performance can often be significantly improved through supervised metric learning Popular algorithms are neighbourhood components analysis and large margin nearest neighbor Supervised metric learning algorithms use the label information to learn a new metric or pseudometric Feature extraction edit When the input data to an algorithm is too large to be processed and it is suspected to be redundant eg the same measurement in both feet and meters then the input data will be transformed into a reduced representation set of features also named features vector Transforming the input data into the set of features is called feature extraction If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input Feature extraction is performed on raw data prior to applying k NN algorithm on the transformed data in feature space An example of a typical computer vision computation pipeline for face recognition using k NN including feature extraction and dimension reduction preprocessing steps usually implemented with OpenCV Haar face detection Meanshift tracking analysis PCA or Fisher LDA projection into feature space followed by k NN classification Dimension reduction edit For highdimensional data eg with number of dimensions more than 10 dimension reduction is usually performed prior to applying the k NN algorithm in order to avoid the effects of the curse of dimensionality 15 The curse of dimensionality in the k NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector imagine multiple points lying more or less on a circle with the query point at the center the distance from the query to all data points in the search space is almost the same Feature extraction and dimension reduction can be combined in one step using principal component analysis PCA linear discriminant analysis LDA or canonical correlation analysis CCA techniques as a preprocessing step followed by clustering by k NN on feature vectors in reduceddimension space In machine learning this process is also called lowdimensional embedding 16 For veryhighdimensional datasets eg when performing a similarity search on live video streams DNA data or highdimensional time series running a fast approximate k NN search using locality sensitive hashing random projections 17 sketches 18 or other highdimensional similarity search techniques from the VLDB toolbox might be the only feasible option Decision boundary edit Nearest neighbor rules in effect implicitly compute the decision boundary It is also possible to compute the decision boundary explicitly and to do so efficiently so that the computational complexity is a function of the boundary complexity 19 Data reduction edit Data reduction is one of the most important problems for work with huge data sets Usually only some of the data points are needed for accurate classification Those data are called the prototypes and can be found as follows Select the classoutliers that is training data that are classified incorrectly by k NN for a given k Separate the rest of the data into two sets i the prototypes that are used for the classification decisions and ii the absorbed points that can be correctly classified by k NN using prototypes The absorbed points can then be removed from the training set Selection of classoutliers edit A training example surrounded by examples of other classes is called a class outlier Causes of class outliers include random error insufficient training examples of this class an isolated example appears instead of a cluster missing important features the classes are separated in other dimensions which we do not know too many training examples of other classes unbalanced classes that create a hostile background for the given small class Class outliers with k NN produce noise They can be detected and separated for future analysis Given two natural numbers kr0 a training example is called a kr NN classoutlier if its k nearest neighbors include more than r examples of other classes CNN for data reduction edit Condensed nearest neighbor CNN the Hart algorithm is an algorithm designed to reduce the data set for k NN classification 20 It selects the set of prototypes U from the training data such that 1NN with U can classify the examples almost as accurately as 1NN does with the whole data set Calculation of the border ratio Three types of points prototypes classoutliers and absorbed points Given a training set X CNN works iteratively Scan all elements of X looking for an element x whose nearest prototype from U has a different label than x Remove x from X and add it to U Repeat the scan until no more prototypes are added to U Use U instead of X for classification The examples that are not prototypes are called absorbed points It is efficient to scan the training examples in order of decreasing border ratio 21 The border ratio of a training example x is defined as a x xy xy where xy is the distance to the closest example y having a different color than x and xy is the distance from y to its closest example x with the same label as x The border ratio is in the interval 01 because xy never exceeds xy This ordering gives preference to the borders of the classes for inclusion in the set of prototypes U A point of a different label than x is called external to x The calculation of the border ratio is illustrated by the figure on the right The data points are labeled by colors the initial point is x and its label is red External points are blue and green The closest to x external point is y The closest to y red point is x The border ratio a x xy xy is the attribute of the initial point x Below is an illustration of CNN in a series of figures There are three classes red green and blue Fig 1 initially there are 60 points in each class Fig 2 shows the 1NN classification map each pixel is classified by 1NN using all the data Fig 3 shows the 5NN classification map White areas correspond to the unclassified regions where 5NN voting is tied for example if there are two green two red and one blue points among 5 nearest neighbors Fig 4 shows the reduced data set The crosses are the classoutliers selected by the 32NN rule all the three nearest neighbors of these instances belong to other classes the squares are the prototypes and the empty circles are the absorbed points The left bottom corner shows the numbers of the classoutliers prototypes and absorbed points for all three classes The number of prototypes varies from 15 to 20 for different classes in this example Fig 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set The figures were produced using the Mirkes applet 21 CNN model reduction for kNN classifiers Fig 1 The dataset Fig 2 The 1NN classification map Fig 3 The 5NN classification map Fig 4 The CNN reduced dataset Fig 5 The 1NN classification map based on the CNN extracted prototypes k NN regression edit In k NN regression the k NN algorithm citation needed is used for estimating continuous variables One such algorithm uses a weighted average of the k nearest neighbors weighted by the inverse of their distance This algorithm works as follows Compute the Euclidean or Mahalanobis distance from the query example to the labeled examples Order the labeled examples by increasing distance Find a heuristically optimal number k of nearest neighbors based on RMSE This is done using cross validation Calculate an inverse distance weighted average with the k nearest multivariate neighbors k NN outlier edit The distance to the k th nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection The larger the distance to the k NN the lower the local density the more likely the query point is an outlier 22 Although quite simple this outlier model along with another classic data mining method local outlier factor works quite well also in comparison to more recent and more complex approaches according to a large scale experimental analysis 23 Validation of results edit A confusion matrix or matching matrix is often used as a tool to validate the accuracy of k NN classification More robust statistical methods such as likelihoodratio test can also be applied See also edit Mathematics portal Nearest centroid classifier Closest pair of points problem References edit Altman N S 1992 An introduction to kernel and nearestneighbor nonparametric regression PDF The American Statistician 46 3 175185 doi 10108000031305199210475879 hdl 181331637 mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput idlockfree amwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocklimited amwparseroutput idlockregistration amwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocksubscription amwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em This scheme is a generalization of linear interpolation Jaskowiak P A Campello R J G B Comparing Correlation Coefficients as Dissimilarity Measures for Cancer Classification in Gene Expression Data Brazilian Symposium on Bioinformatics BSB 2011 18 CiteSeerX 1011208993 Cite journal requires journal help D Coomans DL Massart 1982 Alternative knearest neighbour rules in supervised pattern recognition Part 1 kNearest neighbour classification by using alternative voting rules Analytica Chimica Acta 136 1527 doi 101016S0003267001953590 Everitt B S Landau S Leese M and Stahl D 2011 Miscellaneous Clustering Methods in Cluster Analysis 5th Edition John Wiley Sons Ltd Chichester UK Nigsch F Bender A van Buuren B Tissen J Nigsch E Mitchell JB 2006 Melting point prediction employing knearest neighbor algorithms and genetic parameter optimization Journal of Chemical Information and Modeling 46 6 24122422 doi 101021ci060149f PMID 17125183 Hall P Park BU Samworth RJ 2008 Choice of neighbor order in nearestneighbor classification Annals of Statistics 36 5 21352152 arXiv 08105276 doi 10121407AOS537 Stone C J 1977 Consistent nonparametric regression Annals of Statistics 5 4 595620 doi 101214aos1176343886 a b Samworth R J 2012 Optimal weighted nearest neighbour classifiers Annals of Statistics 40 5 27332763 arXiv 11015783 doi 10121412AOS1049 D G Terrell D W Scott 1992 Variable kernel density estimation Annals of Statistics 20 3 12361265 doi 101214aos1176348768 Mills Peter Efficient statistical classification of satellite measurements International Journal of Remote Sensing Cover TM Hart PE 1967 Nearest neighbor pattern classification PDF IEEE Transactions on Information Theory 13 1 2127 CiteSeerX 1011682616 doi 101109TIT19671053964 Toussaint GT April 2005 Geometric proximity graphs for improving nearest neighbor methods in instancebased learning and data mining International Journal of Computational Geometry and Applications 15 2 101150 doi 101142S0218195905001622 Devroye L Gyorfi L Lugosi G 1996 A probabilistic theory of pattern recognition Springer ISBN 9780387946184 CS1 maint multiple names authors list link Beyer Kevin et al When is nearest neighbor meaningful Database TheoryICDT99 217235year 1999 Shaw Blake and Tony Jebara Structure preserving embedding Proceedings of the 26th Annual International Conference on Machine Learning ACM2009 Bingham Ella and Heikki Mannila Random projection in dimensionality reduction applications to image and text data Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining ACM year 2001 Shasha D High Performance Discovery in Time SeriesBerlin Springer 2004 ISBN 0387008578 Bremner D Demaine E Erickson J Iacono J Langerman S Morin P Toussaint G 2005 Outputsensitive algorithms for computing nearestneighbor decision boundaries Discrete and Computational Geometry 33 4 593604 doi 101007s0045400411520 P E Hart The Condensed Nearest Neighbor Rule IEEE Transactions on Information Theory 18 1968 515516 doi 101109TIT19681054155 a b E M Mirkes KNN and Potential Energy applet University of Leicester 2011 Ramaswamy S Rastogi R Shim K 2000 Efficient algorithms for mining outliers from large data sets Proceedings of the 2000 ACM SIGMOD international conference on Management of data SIGMOD 00 p427 doi 101145342009335437 ISBN 1581132174 Campos Guilherme O Zimek Arthur Sander Jörg Campello Ricardo J G B Micenková Barbora Schubert Erich Assent Ira Houle Michael E 2016 On the evaluation of unsupervised outlier detection measures datasets and an empirical study Data Mining and Knowledge Discovery 30 4 891927 doi 101007s1061801504448 ISSN 13845810 Further reading edit Belur V Dasarathy ed 1991 Nearest Neighbor NN Norms NN Pattern Classification Techniques ISBN 9780818689307 Shakhnarovish Darrell and Indyk eds 2005 NearestNeighbor Methods in Learning and Vision MIT Press ISBN 9780262195478 CS1 maint uses editors parameter link 