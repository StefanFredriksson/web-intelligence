Central processing unit Computer processor redirects here For other uses see Processor Computing CPU redirects here For other uses see CPU disambiguation Central component of any computer system which executes inputoutput arithmetical and logical operations mwparseroutput tmulti thumbinnerdisplayflexflexdirectioncolumnmwparseroutput tmulti trowdisplayflexflexdirectionrowclearleftflexwrapwrapwidth100boxsizingborderboxmwparseroutput tmulti tsinglemargin1pxfloatleftmwparseroutput tmulti theaderclearbothfontweightboldtextaligncenteralignselfcenterbackgroundcolortransparentwidth100mwparseroutput tmulti thumbcaptiontextalignleftbackgroundcolortransparentmwparseroutput tmulti thumbcaptioncentertextaligncenterbackgroundcolortransparentmwparseroutput tmulti textalignlefttextalignleftmwparseroutput tmulti textalignrighttextalignrightmwparseroutput tmulti textaligncentertextaligncentermedia all and maxwidth720pxmwparseroutput tmulti thumbinnerwidth100importantboxsizingborderboxmaxwidthnoneimportantalignitemscentermwparseroutput tmulti trowjustifycontentcentermwparseroutput tmulti tsinglefloatnoneimportantmaxwidth100importantboxsizingborderboxtextaligncentermwparseroutput tmulti thumbcaptiontextaligncenter An Intel 80486DX2 CPU as seen from above Bottom side of an Intel 80486DX2 showing its pins A central processing unit CPU also called a central processor or main processor is the electronic circuitry within a computer that executes instructions that make up a computer program The CPU performs basic arithmetic logic controlling and inputoutput IO operations specified by the instructions The computer industry has used the term central processing unit at least since the early 1960s 1 Traditionally the term CPU refers to a processor more specifically to its processing unit and control unit CU distinguishing these core elements of a computer from external components such as main memory and IO circuitry 2 The form design and implementation of CPUs have changed over the course of their history but their fundamental operation remains almost unchanged Principal components of a CPU include the arithmetic logic unit ALU that performs arithmetic and logic operations processor registers that supply operands to the ALU and store the results of ALU operations and a control unit that orchestrates the fetching from memory and execution of instructions by directing the coordinated operations of the ALU registers and other components Most modern CPUs are microprocessors where the CPU is contained on a single metaloxidesemiconductor MOS integrated circuit IC chip An IC that contains a CPU may also contain memory peripheral interfaces and other components of a computer such integrated devices are variously called microcontrollers or systems on a chip SoC Some computers employ a multicore processor which is a single chip containing two or more CPUs called cores in that context one can speak of such single chips as sockets 3 Array processors or vector processors have multiple processors that operate in parallel with no unit considered central There also exists the concept of virtual CPUs which are an abstraction of dynamical aggregated computational resources 4 Contents 1 History 11 Transistor CPUs 12 Smallscale integration CPUs 13 Largescale integration CPUs 14 Microprocessors 2 Operation 21 Fetch 22 Decode 23 Execute 3 Structure and implementation 31 Control unit 32 Arithmetic logic unit 33 Address generation unit 34 Memory management unit MMU 35 Cache 36 Clock rate 37 Integer range 38 Parallelism 381 Instructionlevel parallelism 382 Tasklevel parallelism 383 Data parallelism 4 Virtual CPUs 5 Performance 6 See also 7 Notes 8 References 9 External links History edit Main article History of generalpurpose CPUs EDVAC one of the first storedprogram computers Early computers such as the ENIAC had to be physically rewired to perform different tasks which caused these machines to be called fixedprogram computers 5 Since the term CPU is generally defined as a device for software computer program execution the earliest devices that could rightly be called CPUs came with the advent of the storedprogram computer The idea of a storedprogram computer had been already present in the design of J Presper Eckert and John William Mauchly s ENIAC but was initially omitted so that it could be finished sooner 6 On June30 1945 before ENIAC was made mathematician John von Neumann distributed the paper entitled First Draft of a Report on the EDVAC It was the outline of a storedprogram computer that would eventually be completed in August 1949 7 EDVAC was designed to perform a certain number of instructions or operations of various types Significantly the programs written for EDVAC were to be stored in highspeed computer memory rather than specified by the physical wiring of the computer 8 This overcame a severe limitation of ENIAC which was the considerable time and effort required to reconfigure the computer to perform a new task 9 With von Neumanns design the program that EDVAC ran could be changed simply by changing the contents of the memory EDVAC however was not the first storedprogram computer the Manchester Baby a smallscale experimental storedprogram computer ran its first program on 21 June 1948 10 and the Manchester Mark 1 ran its first program during the night of 1617 June 1949 11 Early CPUs were custom designs used as part of a larger and sometimes distinctive computer 12 However this method of designing custom CPUs for a particular application has largely given way to the development of multipurpose processors produced in large quantities This standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit IC The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers 13 Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines Modern microprocessors appear in electronic devices ranging from automobiles 14 to cellphones 15 and sometimes even in toys 16 17 While von Neumann is most often credited with the design of the storedprogram computer because of his design of EDVAC and the design became known as the von Neumann architecture others before him such as Konrad Zuse had suggested and implemented similar ideas 18 The socalled Harvard architecture of the Harvard Mark I which was completed before EDVAC 19 20 also used a storedprogram design using punched paper tape rather than electronic memory 21 The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data while the former uses the same memory space for both 22 Most modern CPUs are primarily von Neumann in design but CPUs with the Harvard architecture are seen as well especially in embedded applications for instance the Atmel AVR microcontrollers are Harvard architecture processors 23 Relays and vacuum tubes thermionic tubes were commonly used as switching elements 24 25 a useful computer requires thousands or tens of thousands of switching devices The overall speed of a system is dependent on the speed of the switches Tube computers like EDVAC tended to average eight hours between failures whereas relay computers like the slower but earlier Harvard Mark I failed very rarely 1 In the end tubebased CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs Clock signal frequencies ranging from 100 kHz to 4MHz were very common at this time limited largely by the speed of the switching devices they were built with 26 Transistor CPUs edit IBM PowerPC 604e processor Main article Transistor computer The design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices The first such improvement came with the advent of the transistor Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky unreliable and fragile switching elements like vacuum tubes and relays 27 With this improvement more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete individual components In 1964 IBM introduced its IBM System360 computer architecture that was used in a series of computers capable of running the same programs with different speed and performance 28 This was significant at a time when most electronic computers were incompatible with one another even those made by the same manufacturer To facilitate this improvement IBM used the concept of a microprogram often called microcode which still sees widespread usage in modern CPUs 29 The System360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries 30 31 In 1965 Digital Equipment Corporation DEC introduced another influential computer aimed at the scientific and research markets the PDP8 32 Fujitsu board with SPARC64 VIIIfx processors Transistorbased computers had several distinct advantages over their predecessors Aside from facilitating increased reliability and lower power consumption transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay 33 The increased reliability and dramatically increased speed of the switching elements which were almost exclusively transistors by this time CPU clock rates in the tens of megahertz were easily obtained during this period 34 Additionally while discrete transistor and IC CPUs were in heavy usage new highperformance designs like SIMD Single Instruction Multiple Data vector processors began to appear 35 These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd 35 Smallscale integration CPUs edit CPU core memory and external bus interface of a DEC PDP8 I made of mediumscale integrated circuits During this period a method of manufacturing many interconnected transistors in a compact space was developed The integrated circuit IC allowed a large number of transistors to be manufactured on a single semiconductor based die or chip At first only very basic nonspecialized digital circuits such as NOR gates were miniaturized into ICs 36 CPUs based on these building block ICs are generally referred to as smallscale integration SSI devices SSI ICs such as the ones used in the Apollo Guidance Computer usually contained up to a few dozen transistors To build an entire CPU out of SSI ICs required thousands of individual chips but still consumed much less space and power than earlier discrete transistor designs 37 IBMs System370 followon to the System360 used SSI ICs rather than Solid Logic Technology discretetransistor modules 38 39 DECs PDP8 I and KI10 PDP10 also switched from the individual transistors used by the PDP8 and PDP10 to SSI ICs 40 and their extremely popular PDP11 line was originally built with SSI ICs but was eventually implemented with LSI components once these became practical Largescale integration CPUs edit The MOSFET metaloxidesemiconductor fieldeffect transistor also known as the MOS transistor was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959 and demonstrated in 1960 41 This led to the development of the MOS metaloxidesemiconductor integrated circuit proposed by Atalla in 1960 42 and Kahng in 1961 and then fabricated by Fred Heiman and Steven Hofstein at RCA in 1962 41 With its high scalability 43 and much lower power consumption and higher density than bipolar junction transistors 44 the MOSFET made it possible to build highdensity integrated circuits 45 46 Lee Boysel published influential articles including a 1967 manifesto which described how to build the equivalent of a 32bit mainframe computer from a relatively small number of largescale integration circuits LSI 47 48 The only way to build LSI chips which are chips with a hundred or more gates was to build them using a MOS semiconductor manufacturing process either PMOS logic NMOS logic or CMOS logic However some companies continued to build processors out of bipolar transistortransistor logic TTL chips because bipolar junction transistors were faster than MOS chips up until the 1970s a few companies such as Datapoint continued to build processors out of TTL chips until the early 1980s 48 In the 1960s MOS ICs were slower and initially considered useful only in applications that required low power 49 50 Following the development of silicongate MOS technology by Federico Faggin at Fairchild Semiconductor in 1968 MOS ICs largely replaced bipolar TTL as the standard chip technology in the early 1970s 51 As the microelectronic technology advanced an increasing number of transistors were placed on ICs decreasing the number of individual ICs needed for a complete CPU MSI and LSI ICs increased transistor counts to hundreds and then thousands By 1968 the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types with each IC containing roughly 1000 MOSFETs 52 In stark contrast with its SSI and MSI predecessors the first LSI implementation of the PDP11 contained a CPU composed of only four LSI integrated circuits 53 Microprocessors edit Main article Microprocessor Die of an Intel 80486DX2 microprocessor actual size 12 675mm in its packaging Intel Core i5 CPU on a Vaio E series laptop motherboard on the right beneath the heat pipe Advances in MOS IC technology led to the invention of the microprocessor in the early 1970s 54 Since the introduction of the first commercially available microprocessor the Intel 4004 in 1971 and the first widely used microprocessor the Intel 8080 in 1974 this class of CPUs has almost completely overtaken all other central processing unit implementation methods Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures and eventually produced instruction set compatible microprocessors that were backwardcompatible with their older hardware and software Combined with the advent and eventual success of the ubiquitous personal computer the term CPU is now applied almost exclusively a to microprocessors Several CPUs denoted cores can be combined in a single processing chip 55 Previous generations of CPUs were implemented as discrete components and numerous small integrated circuits ICs on one or more circuit boards 56 Microprocessors on the other hand are CPUs manufactured on a very small number of ICs usually just one 57 The overall smaller CPU size as a result of being implemented on a single die means faster switching time because of physical factors like decreased gate parasitic capacitance 58 59 This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz Additionally the ability to construct exceedingly small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold This widely observed trend is described by Moores law which had proven to be a fairly accurate predictor of the growth of CPU and other IC complexity until 2016 60 61 While the complexity size construction and general form of CPUs have changed enormously since 1950 62 the basic design and function has not changed much at all Almost all common CPUs today can be very accurately described as von Neumann storedprogram machines 63 b As Moores law no longer holds concerns have arisen about the limits of integrated circuit transistor technology Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant 65 66 These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model Operation edit The fundamental operation of most CPUs regardless of the physical form they take is to execute a sequence of stored instructions that is called a program The instructions to be executed are kept in some kind of computer memory Nearly all CPUs follow the fetch decode and execute steps in their operation which are collectively known as the instruction cycle After the execution of an instruction the entire process repeats with the next instruction cycle normally fetching the nextinsequence instruction because of the incremented value in the program counter If a jump instruction was executed the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally In more complex CPUs multiple instructions can be fetched decoded and executed simultaneously This section describes what is generally referred to as the classic RISC pipeline which is quite common among the simple CPUs used in many electronic devices often called microcontroller It largely ignores the important role of CPU cache and therefore the access stage of the pipeline Some instructions manipulate the program counter rather than producing result data directly such instructions are generally called jumps and facilitate program behavior like loops conditional program execution through the use of a conditional jump and existence of functions c In some processors some other instructions change the state of bits in a flags register These flags can be used to influence how a program behaves since they often indicate the outcome of various operations For example in such processors a compare instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal one of these flags could then be used by a later jump instruction to determine program flow Fetch edit The first step fetch involves retrieving an instruction which is represented by a number or sequence of numbers from program memory The instructions location address in program memory is determined by a program counter PC which stores a number that identifies the address of the next instruction to be fetched After an instruction is fetched the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence d Often the instruction to be fetched must be retrieved from relatively slow memory causing the CPU to stall while waiting for the instruction to be returned This issue is largely addressed in modern processors by caches and pipeline architectures see below Decode edit The instruction that the CPU fetches from memory determines what the CPU will do In the decode step performed by the circuitry known as the instruction decoder the instruction is converted into signals that control other parts of the CPU The way in which the instruction is interpreted is defined by the CPUs instruction set architecture ISA e Often one group of bits that is a field within the instruction called the opcode indicates which operation is to be performed while the remaining fields usually provide supplemental information required for the operation such as the operands Those operands may be specified as a constant value called an immediate value or as the location of a value that may be a processor register or a memory address as determined by some addressing mode In some CPU designs the instruction decoder is implemented as a hardwired unchangeable circuit In others a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses In some cases the memory that stores the microprogram is rewritable making it possible to change the way in which the CPU decodes instructions Execute edit After the fetch and decode steps the execute step is performed Depending on the CPU architecture this may consist of a single action or a sequence of actions During each action various parts of the CPU are electrically connected so they can perform all or part of the desired operation and then the action is completed typically in response to a clock pulse Very often the results are written to an internal CPU register for quick access by subsequent instructions In other cases results may be written to slower but less expensive and higher capacity main memory For example if an addition instruction is to be executed the arithmetic logic unit ALU inputs are connected to a pair of operand sources numbers to be summed the ALU is configured to perform an addition operation so that the sum of its operand inputs will appear at its output and the ALU output is connected to storage eg a register or memory that will receive the sum When the clock pulse occurs the sum will be transferred to storage and if the resulting sum is too large ie it is larger than the ALUs output word size an arithmetic overflow flag will be set Structure and implementation edit See also Processor design Block diagram of a basic uniprocessorCPU computer Black lines indicate data flow whereas red lines indicate control flow arrows indicate flow directions Hardwired into a CPUs circuitry is a set of basic operations it can perform called an instruction set Such operations may involve for example adding or subtracting two numbers comparing two numbers or jumping to a different part of a program Each basic operation is represented by a particular combination of bits known as the machine language opcode while executing instructions in a machine language program the CPU decides which operation to perform by decoding the opcode A complete machine language instruction consists of an opcode and in many cases additional bits that specify arguments for the operation for example the numbers to be summed in the case of an addition operation Going up the complexity scale a machine language program is a collection of machine language instructions that the CPU executes The actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPUs processor known as the arithmetic logic unit or ALU In general a CPU executes an instruction by fetching it from memory using its ALU to perform an operation and then storing the result to memory Beside the instructions for integer mathematics and logic operations various other machine instructions exist such as those for loading data from memory and storing it back branching operations and mathematical operations on floatingpoint numbers performed by the CPUs floatingpoint unit FPU 67 Control unit edit Main article Control unit The control unit CU is a component of the CPU that directs the operation of the processor It tells the computers memory arithmetic and logic unit and input and output devices how to respond to the instructions that have been sent to the processor It directs the operation of the other units by providing timing and control signals Most computer resources are managed by the CU It directs the flow of data between the CPU and the other devices John von Neumann included the control unit as part of the von Neumann architecture In modern computer designs the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction citation needed Arithmetic logic unit edit Main article Arithmetic logic unit Symbolic representation of an ALU and its input and output signals The arithmetic logic unit ALU is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations The inputs to the ALU are the data words to be operated on called operands status information from previous operations and a code from the control unit indicating which operation to perform Depending on the instruction being executed the operands may come from internal CPU registers or external memory or they may be constants generated by the ALU itself When all input signals have settled and propagated through the ALU circuitry the result of the performed operation appears at the ALUs outputs The result consists of both a data word which may be stored in a register or memory and status information that is typically stored in a special internal CPU register reserved for this purpose Address generation unit edit Main article Address generation unit Address generation unit AGU sometimes also called address computation unit ACU 68 is an execution unit inside the CPU that calculates addresses used by the CPU to access main memory By having address calculations handled by separate circuitry that operates in parallel with the rest of the CPU the number of CPU cycles required for executing various machine instructions can be reduced bringing performance improvements While performing various operations CPUs need to calculate memory addresses required for fetching data from the memory for example inmemory positions of array elements must be calculated before the CPU can fetch the data from actual memory locations Those addressgeneration calculations involve different integer arithmetic operations such as addition subtraction modulo operations or bit shifts Often calculating a memory address involves more than one generalpurpose machine instruction which do not necessarily decode and execute quickly By incorporating an AGU into a CPU design together with introducing specialized instructions that use the AGU various addressgeneration calculations can be offloaded from the rest of the CPU and can often be executed quickly in a single CPU cycle Capabilities of an AGU depend on a particular CPU and its architecture Thus some AGUs implement and expose more addresscalculation operations while some also include more advanced specialized instructions that can operate on multiple operands at a time Furthermore some CPU architectures include multiple AGUs so more than one addresscalculation operation can be executed simultaneously bringing further performance improvements by capitalizing on the superscalar nature of advanced CPU designs For example Intel incorporates multiple AGUs into its Sandy Bridge and Haswell microarchitectures which increase bandwidth of the CPU memory subsystem by allowing multiple memoryaccess instructions to be executed in parallel Memory management unit MMU edit Main article Memory management unit Most highend microprocessors in desktop laptop server computers have a memory management unit translating logical addresses into physical RAM addresses providing memory protection and paging abilities useful for virtual memory Simpler processors especially microcontrollers usually dont include an MMU Cache edit A CPU cache 69 is a hardware cache used by the central processing unit CPU of a computer to reduce the average cost time or energy to access data from the main memory A cache is a smaller faster memory closer to a processor core which stores copies of the data from frequently used main memory locations Most CPUs have different independent caches including instruction and data caches where the data cache is usually organized as a hierarchy of more cache levels L1 L2 L3 L4 etc All modern fast CPUs with few specialized exceptions 70 have multiple levels of CPU caches The first CPUs that used a cache had only one level of cache unlike later level 1 caches it was not split into L1d for data and L1i for instructions Almost all current CPUs with caches have a split L1 cache They also have L2 caches and for larger processors L3 caches as well The L2 cache is usually not split and acts as a common repository for the already split L1 cache Every core of a multicore processor has a dedicated L2 cache and is usually not shared between the cores The L3 cache and higherlevel caches are shared between the cores and are not split An L4 cache is currently uncommon and is generally on dynamic randomaccess memory DRAM rather than on static randomaccess memory SRAM on a separate die or chip That was also the case historically with L1 while bigger chips have allowed integration of it and generally all cache levels with the possible exception of the last level Each extra level of cache tends to be bigger and be optimized differently Other types of caches exist that are not counted towards the cache size of the most important caches mentioned above such as the translation lookaside buffer TLB that is part of the memory management unit MMU that most CPUs have Caches are generally sized in powers of two 4 8 16 etc KiB or MiB for larger nonL1 sizes although the IBM z13 has a 96 KiB L1 instruction cache 71 Clock rate edit Main article Clock rate Most CPUs are synchronous circuits which means they employ a clock signal to pace their sequential operations The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave The frequency of the clock pulses determines the rate at which a CPU executes instructions and consequently the faster the clock the more instructions the CPU will execute each second To ensure proper operation of the CPU the clock period is longer than the maximum time needed for all signals to propagate move through the CPU In setting the clock period to a value well above the worstcase propagation delay it is possible to design the entire CPU and the way it moves data around the edges of the rising and falling clock signal This has the advantage of simplifying the CPU significantly both from a design perspective and a componentcount perspective However it also carries the disadvantage that the entire CPU must wait on its slowest elements even though some portions of it are much faster This limitation has largely been compensated for by various methods of increasing CPU parallelism see below However architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs For example a clock signal is subject to the delays of any other electrical signal Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase synchronized throughout the entire unit This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction Another major issue as clock rates increase dramatically is the amount of heat that is dissipated by the CPU The constantly changing clock causes many components to switch regardless of whether they are being used at that time In general a component that is switching uses more energy than an element in a static state Therefore as clock rate increases so does energy consumption causing the CPU to require more heat dissipation in the form of CPU cooling solutions One method of dealing with the switching of unneeded components is called clock gating which involves turning off the clock signal to unneeded components effectively disabling them However this is often regarded as difficult to implement and therefore does not see common usage outside of very lowpower designs One notable recent CPU design that uses extensive clock gating is the IBM PowerPC based Xenon used in the Xbox 360 that way power requirements of the Xbox 360 are greatly reduced 72 Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether While removing the global clock signal makes the design process considerably more complex in many ways asynchronous or clockless designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs While somewhat uncommon entire asynchronous CPUs have been built without using a global clock signal Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS Rather than totally removing the clock signal some CPU designs allow certain portions of the device to be asynchronous such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts it is evident that they do at least excel in simpler math operations This combined with their excellent power consumption and heat dissipation properties makes them very suitable for embedded computers 73 Integer range edit Every CPU represents numerical values in a specific way For example some early digital computers represented numbers as familiar decimal base 10 numeral system values and others have employed more unusual representations such as ternary base three Nearly all modern CPUs represent numbers in binary form with each digit being represented by some twovalued physical quantity such as a high or low voltage f A sixbit word containing the binary encoded representation of decimal value 40 Most modern CPUs employ word sizes that are a power of two for example 8 16 32 or 64 bits Related to numeric representation is the size and precision of integer numbers that a CPU can represent In the case of a binary CPU this is measured by the number of bits significant digits of a binary encoded integer that the CPU can process in one operation which is commonly called word size bit width data path width integer precision or integer size A CPUs integer size determines the range of integer values it can directly operate on g For example an 8bit CPU can directly manipulate integers represented by eight bits which have a range of 256 2 8 discrete integer values Integer range can also affect the number of memory locations the CPU can directly address an address is an integer value representing a specific memory location For example if a binary CPU uses 32 bits to represent a memory address then it can directly address 2 32 memory locations To circumvent this limitation and for various other reasons some CPUs use mechanisms such as bank switching that allow additional memory to be addressed CPUs with larger word sizes require more circuitry and consequently are physically larger cost more and consume more power and therefore generate more heat As a result smaller 4 or 8bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes such as 16 32 64 even 128bit are available When higher performance is required however the benefits of a larger word size larger data ranges and address spaces may outweigh the disadvantages A CPU can have internal data paths shorter than the word size to reduce size and cost For example even though the IBM System360 instruction set was a 32bit instruction set the System360 Model 30 and Model 40 had 8bit data paths in the arithmetic logical unit so that a 32bit add required four cycles one for each 8 bits of the operands and even though the Motorola 68000 series instruction set was a 32bit instruction set the Motorola 68000 and Motorola 68010 had 16bit data paths in the arithmetic logical unit so that a 32bit add required two cycles To gain some of the advantages afforded by both lower and higher bit lengths many instruction sets have different bit widths for integer and floatingpoint data allowing CPUs implementing that instruction set to have different bit widths for different portions of the device For example the IBM System360 instruction set was primarily 32 bit but supported 64bit floating point values to facilitate greater accuracy and range in floating point numbers 29 The System360 Model 65 had an 8bit adder for decimal and fixedpoint binary arithmetic and a 60bit adder for floatingpoint arithmetic 74 Many later CPU designs use similar mixed bit width especially when the processor is meant for generalpurpose usage where a reasonable balance of integer and floating point capability is required Parallelism edit Main article Parallel computing Model of a subscalar CPU in which it takes fifteen clock cycles to complete three instructions The description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take This type of CPU usually referred to as subscalar operates on and executes one instruction on one or two pieces of data at a time that is less than one instruction per clock cycle IPC 1 This process gives rise to an inherent inefficiency in subscalar CPUs Since only one instruction is executed at a time the entire CPU must wait for that instruction to complete before proceeding to the next instruction As a result the subscalar CPU gets hung up on instructions which take more than one clock cycle to complete execution Even adding a second execution unit see below does not improve performance much rather than one pathway being hung up now two pathways are hung up and the number of unused transistors is increased This design wherein the CPUs execution resources can operate on only one instruction at a time can only possibly reach scalar performance one instruction per clock cycle IPC 1 However the performance is nearly always subscalar less than one instruction per clock cycle IPC 1 Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel When referring to parallelism in CPUs two terms are generally used to classify these design techniques instructionlevel parallelism ILP which seeks to increase the rate at which instructions are executed within a CPU that is to increase the use of ondie execution resources tasklevel parallelism TLP which purposes to increase the number of threads or processes that a CPU can execute simultaneously Each methodology differs both in the ways in which they are implemented as well as the relative effectiveness they afford in increasing the CPUs performance for an application h Instructionlevel parallelism edit Main articles Instruction pipelining and Superscalar processor Basic fivestage pipeline In the best case scenario this pipeline can sustain a completion rate of one instruction per clock cycle One of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing This is the simplest form of a technique known as instruction pipelining and is used in almost all modern generalpurpose CPUs Pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages This separation can be compared to an assembly line in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired Pipelining does however introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation a condition often termed data dependency conflict To cope with this additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs Naturally accomplishing this requires additional circuitry so pipelined processors are more complex than subscalar ones though not very significantly so A pipelined processor can become very nearly scalar inhibited only by pipeline stalls an instruction spending more than one clock cycle in a stage A simple superscalar pipeline By fetching and dispatching two instructions at a time a maximum of two instructions per clock cycle can be completed Further improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of CPU components even further Designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units such as loadstore units arithmeticlogic units floatingpoint units and address generation units 75 In a superscalar pipeline multiple instructions are read and passed to a dispatcher which decides whether or not the instructions can be executed in parallel simultaneously If so they are dispatched to available execution units resulting in the ability for several instructions to be executed simultaneously In general the more instructions a superscalar CPU is able to dispatch simultaneously to waiting execution units the more instructions will be completed in a given cycle Most of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher The dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel as well as dispatch them in such a way as to keep as many execution units busy as possible This requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of CPU cache It also makes hazard avoiding techniques like branch prediction speculative execution register renaming outoforder execution and transactional memory crucial to maintaining high levels of performance By attempting to predict which branch or path a conditional instruction will take the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes Outoforder execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies Also in case of single instruction stream multiple data stream a case when a lot of data from the same type has to be processed modern processors can disable parts of the pipeline so that when a single instruction is executed many times the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions especially in highly monotonous program engines such as video creation software and photo processing In the case where a portion of the CPU is superscalar and part is not the part which is not suffers a performance penalty due to scheduling stalls The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each but its FPU could not accept one instruction per clock cycle Thus the P5 was integer superscalar but not floating point superscalar Intels successor to the P5 architecture P6 added superscalar capabilities to its floating point features and therefore afforded a significant increase in floating point instruction performance Both simple pipelining and superscalar design increase a CPUs ILP by allowing a single processor to complete execution of instructions at rates surpassing one instruction per clock cycle i Most modern CPU designs are at least somewhat superscalar and nearly all general purpose CPUs designed in the last decade are superscalar In later years some of the emphasis in designing highILP computers has been moved out of the CPUs hardware and into its software interface or ISA The strategy of the very long instruction word VLIW causes some ILP to become implied directly by the software reducing the amount of work the CPU must perform to boost ILP and thereby reducing the designs complexity Tasklevel parallelism edit Main articles Multithreading and Multicore processor Another strategy of achieving performance is to execute multiple threads or processes in parallel This area of research is known as parallel computing 76 In Flynns taxonomy this strategy is known as multiple instruction stream multiple data stream MIMD 77 One technology used for this purpose was multiprocessing MP 78 The initial flavor of this technology is known as symmetric multiprocessing SMP where a small number of CPUs share a coherent view of their memory system In this scheme each CPU has additional hardware to maintain a constantly uptodate view of memory By avoiding stale views of memory the CPUs can cooperate on the same program and programs can migrate from one CPU to another To increase the number of cooperating CPUs beyond a handful schemes such as nonuniform memory access NUMA and directorybased coherence protocols were introduced in the 1990s SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors Initially multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors When the processors and their interconnect are all implemented on a single chip the technology is known as chiplevel multiprocessing CMP and the single chip as a multicore processor It was later recognized that finergrain parallelism existed with a single program A single program might have several threads or functions that could be executed separately or in parallel Some of the earliest examples of this technology implemented inputoutput processing such as direct memory access as a separate thread from the computation thread A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel This technology is known as multithreading MT This approach is considered more costeffective than multiprocessing as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP In MT the execution units and the memory system including the caches are shared among multiple threads The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT One type of MT that was implemented is known as temporal multithreading where one thread is executed until it is stalled waiting for data to return from external memory In this scheme the CPU would then quickly context switch to another thread which is ready to run the switch often done in one CPU clock cycle such as the UltraSPARC T1 Another type of MT is simultaneous multithreading where instructions from multiple threads are executed in parallel within one CPU clock cycle For several decades from the 1970s to early 2000s the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining caches superscalar execution outoforder execution etc This trend culminated in large powerhungry CPUs such as the Intel Pentium 4 By the early 2000s CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques CPU designers then borrowed ideas from commercial computing markets such as transaction processing where the aggregate performance of multiple programs also known as throughput computing was more important than the performance of a single thread or process This reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably Intels newer designs resembling its less superscalar P6 architecture Late designs in several processor families exhibit CMP including the x8664 Opteron and Athlon 64 X2 the SPARC UltraSPARC T1 IBM POWER4 and POWER5 as well as several video game console CPUs like the Xbox 360 s triplecore PowerPC design and the PlayStation 3 s 7core Cell microprocessor Data parallelism edit Main articles Vector processor and SIMD A less common but increasingly important paradigm of processors and indeed computing in general deals with data parallelism The processors discussed earlier are all referred to as some type of scalar device j As the name implies vector processors deal with multiple pieces of data in the context of one instruction This contrasts with scalar processors which deal with one piece of data for every instruction Using Flynns taxonomy these two schemes of dealing with data are generally referred to as single instruction stream multiple data stream SIMD and single instruction stream single data stream SISD respectively The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation for example a sum or a dot product to be performed on a large set of data Some classic examples of these types of tasks include multimedia applications images video and sound as well as many types of scientific and engineering tasks Whereas a scalar processor must complete the entire process of fetching decoding and executing each instruction and value in a set of data a vector processor can perform a single operation on a comparatively large set of data with one instruction This is only possible when the application tends to require many steps which apply one operation to a large set of data Most early vector processors such as the Cray1 were associated almost exclusively with scientific research and cryptography applications However as multimedia has largely shifted to digital media the need for some form of SIMD in generalpurpose processors has become significant Shortly after inclusion of floatingpoint units started to become commonplace in generalpurpose processors specifications for and implementations of SIMD execution units also began to appear for generalpurpose processors when Some of these early SIMD specifications like HPs Multimedia Acceleration eXtensions MAX and Intels MMX were integeronly This proved to be a significant impediment for some software developers since many of the applications that benefit from SIMD primarily deal with floatingpoint numbers Progressively developers refined and remade these early designs into some of the common modern SIMD specifications which are usually associated with one ISA Some notable modern examples include Intels SSE and the PowerPCrelated AltiVec also known as VMX k Virtual CPUs edit This section needs expansion You can help by adding to it September 2016 Cloud computing can involve subdividing CPU operation into virtual central processing units 79 vCPU s 80 A host is the virtual equivalent of a physical machine on which a virtual system is operating 81 When there are several physical machines operating in tandem and managed as a whole the grouped computing and memory resources form a cluster In some systems it is possible to dynamically add and remove from a cluster Resources available at a host and cluster level can be partitioned out into resources pools with fine granularity Performance edit Further information Computer performance and Benchmark computing The performance or speed of a processor depends on among many other factors the clock rate generally given in multiples of hertz and the instructions per clock IPC which together are the factors for the instructions per second IPS that the CPU can perform 82 Many reported IPS values have represented peak execution rates on artificial instruction sequences with few branches whereas realistic workloads consist of a mix of instructions and applications some of which take longer to execute than others The performance of the memory hierarchy also greatly affects processor performance an issue barely considered in MIPS calculations Because of these problems various standardized tests often called benchmarks for this purposesuch as SPECint have been developed to attempt to measure the real effective performance in commonly used applications Processing performance of computers is increased by using multicore processors which essentially is plugging two or more individual processors called cores in this sense into one integrated circuit 83 Ideally a dual core processor would be nearly twice as powerful as a single core processor In practice the performance gain is far smaller only about 50 due to imperfect software algorithms and implementation 84 Increasing the number of cores in a processor ie dualcore quadcore etc increases the workload that can be handled This means that the processor can now handle numerous asynchronous events interrupts etc which can take a toll on the CPU when overwhelmed These cores can be thought of as different floors in a processing plant with each floor handling a different task Sometimes these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information Due to specific capabilities of modern CPUs such as simultaneous multithreading and uncore which involve sharing of actual CPU resources while aiming at increased utilization monitoring performance levels and hardware use gradually became a more complex task 85 As a response some CPUs implement additional hardware logic that monitors actual use of various parts of a CPU and provides various counters accessible to software an example is Intels Performance Counter Monitor technology 3 See also edit Technology portal Addressing mode AMD Accelerated Processing Unit CISC Computer bus Computer engineering CPU core voltage CPU socket Digital signal processor GPU Hyperthreading List of instruction sets Microprocessor Multicore processor Protection ring RISC Stream processing True Performance Index TPU Wait state Notes edit Integrated circuits are now used to implement all CPUs except for a few machines designed to withstand large electromagnetic pulses say from a nuclear weapon The socalled von Neumann memo expounded the idea of stored programs 64 which for example may be stored on punched cards paper tape or magnetic tape Some early computers like the Harvard Mark I did not support any kind of jump instruction effectively limiting the complexity of the programs they could run It is largely for this reason that these computers are often not considered to contain a proper CPU despite their close similarity to storedprogram computers Since the program counter counts memory addresses and not instructions it is incremented by the number of memory units that the instruction word contains In the case of simple fixedlength instruction word ISAs this is always the same number For example a fixedlength 32bit instruction word ISA that uses 8bit memory words would always increment the PC by four except in the case of jumps ISAs that use variablelength instruction words increment the PC by the number of memory words corresponding to the last instructions length Because the instruction set architecture of a CPU is fundamental to its interface and usage it is often used as a classification of the type of CPU For example a PowerPC CPU uses some variant of the PowerPC ISA A system can execute a different ISA by running an emulator The physical concept of voltage is an analog one by nature practically having an infinite range of possible values For the purpose of physical representation of binary numbers two specific ranges of voltages are defined one for logic 0 and another for logic 1 These ranges are dictated by design considerations such as noise margins and characteristics of the devices used to create the CPU While a CPUs integer size sets a limit on integer ranges this can and often is overcome using a combination of software and hardware techniques By using additional memory software can represent integers many magnitudes larger than the CPU can Sometimes the CPUs instruction set will even facilitate operations on integers larger than it can natively represent by providing instructions to make large integer arithmetic relatively quick This method of dealing with large integers is slower than utilizing a CPU with higher integer size but is a reasonable tradeoff in cases where natively supporting the full integer range needed would be costprohibitive See Arbitraryprecision arithmetic for more details on purely softwaresupported arbitrarysized integers Neither ILP nor TLP is inherently superior over the other they are simply different means by which to increase CPU parallelism As such they both have advantages and disadvantages which are often determined by the type of software that the processor is intended to run HighTLP CPUs are often used in applications that lend themselves well to being split up into numerous smaller applications socalled embarrassingly parallel problems Frequently a computational problem that can be solved quickly with high TLP design strategies like symmetric multiprocessing takes significantly more time on high ILP devices like superscalar CPUs and vice versa Bestcase scenario or peak IPC rates in very superscalar architectures are difficult to maintain since it is impossible to keep the instruction pipeline filled all the time Therefore in highly superscalar CPUs average sustained IPC is often discussed rather than peak IPC Earlier the term scalar was used to compare the IPC count afforded by various ILP methods Here the term is used in the strictly mathematical sense to contrast with vectors See scalar mathematics and Vector geometric Although SSESSE2SSE3 have superseded MMX in Intels generalpurpose processors later IA32 designs still support MMX This is usually accomplished by providing most of the MMX functionality with the same hardware that supports the much more expansive SSE instruction sets References edit a b Weik Martin H 1961 A Third Survey of Domestic Electronic Digital Computing Systems Ballistic Research Laboratory Cite journal requires journal help mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em Kuck David 1978 Computers and Computations Vol 1 John Wiley Sons Inc p12 ISBN 9780471027164 a b Thomas Willhalm Roman Dementiev Patrick Fay December 18 2014 Intel Performance Counter Monitor A better way to measure CPU utilization softwareintelcom Retrieved February 17 2015 Liebowitz Kusek Spies Matt Christopher Rynardt 2014 VMware vSphere Performance Designing CPU Memory Storage and Networking for PerformanceIntensive Workloads Wiley p68 ISBN 9781118008195 CS1 maint multiple names authors list link Regan Gerard 2008 A Brief History of Computing p66 ISBN 9781848000834 Retrieved 26 November 2014 Bit By Bit Haverford College Archived from the original on October 13 2012 Retrieved August 1 2015 First Draft of a Report on the EDVAC PDF Moore School of Electrical Engineering University of Pennsylvania 1945 Cite journal requires journal help Stanford University The Modern History of Computing The Stanford Encyclopedia of Philosophy Retrieved September 25 2015 ENIACs Birthday The MIT Press February 9 2016 Retrieved October 17 2018 Enticknap Nicholas Summer 1998 Computings Golden Jubilee Resurrection The Computer Conservation Society 20 ISSN 09587403 retrieved 26 June 2019 The Manchester Mark 1 The University of Manchester Retrieved September 25 2015 The First Generation Computer History Museum Retrieved September 29 2015 The History of the Integrated Circuit Nobelprizeorg Retrieved September 29 2015 Turley Jim Motoring with microprocessors Embedded Retrieved November 15 2015 Mobile Processor Guide Summer 2013 Android Authority 20130625 Retrieved November 15 2015 Section 250 Microprocessors and Toys An Introduction to Computing Systems The University of Michigan Retrieved October 9 2018 ARM946 Processor ARM Archived from the original on 17 November 2015 Konrad Zuse Computer History Museum Retrieved September 29 2015 Timeline of Computer History Computers Computer History Museum Retrieved November 21 2015 White Stephen A Brief History of Computing First Generation Computers Retrieved November 21 2015 Harvard University Mark Paper Tape Punch Unit Computer History Museum Retrieved November 21 2015 What is the difference between a von Neumann architecture and a Harvard architecture ARM Retrieved November 22 2015 Advanced Architecture Optimizes the Atmel AVR CPU Atmel Retrieved November 22 2015 Switches transistors and relays BBC Archived from the original on 5 December 2016 Introducing the Vacuum Transistor A Device Made of Nothing IEEE Spectrum 20140623 Retrieved 27 January 2019 What Is Computer Performance The National Academies Press 2011 doi 101722612980 ISBN 9780309159517 Retrieved May 16 2016 1953 Transistorized Computers Emerge Computer History Museum Retrieved June 3 2016 IBM System360 Dates and Characteristics IBM 20030123 a b Amdahl G M Blaauw G A Brooks F P Jr April 1964 Architecture of the IBM System360 IBM Journal of Research and Development IBM 8 2 87101 doi 101147rd820087 ISSN 00188646 Brodkin John 50 years ago IBM created mainframe that helped send men to the Moon Ars Technica Retrieved 9 April 2016 Clarke Gavin Why wont you DIE IBMs S360 and its legacy at 50 The Register Retrieved 9 April 2016 Online PDP8 Home Page Run a PDP8 PDP8 Retrieved September 25 2015 Transistors Relays and Controlling HighCurrent Loads New York University ITP Physical Computing Retrieved 9 April 2016 Lilly Paul 20090414 A Brief History of CPUs 31 Awesome Years of x86 PC Gamer Retrieved June 15 2016 a b Patterson David A Hennessy John L Larus James R 1999 Computer Organization and Design the HardwareSoftware Interface 2 ed 3rd print ed San Francisco Kaufmann p 751 ISBN 9781558604285 1962 Aerospace systems are first the applications for ICs in computers Computer History Museum Retrieved October 9 2018 The integrated circuits in the Apollo manned lunar landing program National Aeronautics and Space Administration Retrieved October 9 2018 System370 Announcement IBM Archives 20030123 Retrieved October 25 2017 System370 Model 155 Continued IBM Archives 20030123 Retrieved October 25 2017 Models and Options The Digital Equipment Corporation PDP8 Retrieved June 15 2018 a b httpswwwcomputerhistoryorgsiliconenginemetaloxidesemiconductormostransistordemonstrated Moskowitz Sanford L 2016 Advanced Materials Innovation Managing Global Technology in the 21st century John Wiley Sons pp165167 ISBN 9780470508923 Motoyoshi M 2009 ThroughSilicon Via TSV PDF Proceedings of the IEEE 97 1 4348 doi 101109JPROC20082007462 ISSN 00189219 Transistors Keep Moores Law Alive EETimes 12 December 2018 Who Invented the Transistor Computer History Museum 4 December 2013 Hittinger William C 1973 MetalOxideSemiconductor Technology Scientific American 229 2 4859 Bibcode 1973SciAm229b48H doi 101038scientificamerican087348 ISSN 00368733 JSTOR 24923169 Ross Knox Bassett 2007 To the Digital Age Research Labs Startup Companies and the Rise of MOS Technology The Johns Hopkins University Press pp127128 256 and 314 ISBN 9780801868092 a b Ken Shirriff The Texas Instruments TMX 1795 the first forgotten microprocessor Speed Power in Logic Families T J Stonham Digital Logic Techniques Principles and Practice 1996p 174 1968 Silicon Gate Technology Developed for ICs Computer History Museum R K Booher MOS GP Computer afips pp877 1968 Proceedings of the Fall Joint Computer Conference 1968 doi 101109AFIPS1968126 LSI11 Module Descriptions PDF LSI11 PDP1103 users manual 2nd ed Maynard Massachusetts Digital Equipment Corporation November 1975 pp43 1971 Microprocessor Integrates CPU Function onto a Single Chip Computer History Museum Margaret Rouse March 27 2007 Definition multicore processor TechTarget Retrieved March 6 2013 Richard Birkby A Brief History of the Microprocessor computermuseumli Archived from the original on September 23 2015 Retrieved October 13 2015 Osborne Adam 1980 An Introduction to Microcomputers Volume 1 Basic Concepts 2nd ed Berkeley California OsborneMcGraw Hill ISBN 9780931988349 Zhislina Victoria 20140219 Why has CPU frequency ceased to grow Intel Retrieved October 14 2015 MOS Transistor Electrical Engineering Computer Science PDF University of California Retrieved October 14 2015 Simonite Tom Moores Law Is Dead Now What MIT Technology Review Retrieved 20180824 Excerpts from A Conversation with Gordon Moore Moores Law PDF Intel 2005 Archived from the original PDF on 20121029 Retrieved 20120725 Cite journal requires journal help A detailed history of the processor Tech Junkie 15 December 2016 Eigenmann Rudolf Lilja David 1998 Von Neumann Computers PDF Retrieved June 15 2018 Aspray William September 1990 The stored program concept IEEE Spectrum Vol27 no9 doi 101109658457 Saraswat Krishna Trends in Integrated Circuits Technology PDF Retrieved June 15 2018 Electromigration Middle East Technical University Retrieved June 15 2018 Ian Wienand September 3 2013 Computer Science from the Bottom Up Chapter 3 Computer Architecture PDF bottomupcscom Retrieved January 7 2015 Cornelis Van Berkel Patrick Meuwissen January 12 2006 Address generation unit for a processor US 2006010255 A1 patent application googlecom Retrieved December 8 2014 verification needed Gabriel Torres September 12 2007 How The Cache Memory Works verification needed A few specialized CPUs accelerators or microcontrollers do not have a cache To be fast if neededwanted they still have an onchip scratchpad memory that has a similar function while software managed In eg microcontrollers it can be better for hard realtime use to have that or at least no cache as with one level of memory latencies of loads are predictable verification needed IBM z13 and IBM z13s Technical Introduction PDF IBM March 2016 p20 verification needed Brown Jeffery 2005 Applicationcustomized CPU design IBM developerWorks Retrieved 20051217 Garside J D Furber S B Chung SH 1999 AMULET3 Revealed University of Manchester Computer Science Department Archived from the original on December 10 2005 Cite journal requires journal help IBM System360 Model 65 Functional Characteristics PDF IBM September 1968 pp89 A2268843 Huynh Jack 2003 The AMD Athlon XP Processor with 512KB L2 Cache PDF University of Illinois UrbanaChampaign pp611 Archived from the original PDF on 20071128 Retrieved 20071006 Gottlieb Allan Almasi George S 1989 Highly parallel computing Redwood City Calif BenjaminCummings ISBN 9780805301779 Flynn M J September 1972 Some Computer Organizations and Their Effectiveness IEEE Trans Comput C21 9 948960 doi 101109TC19725009071 Lu NP Chung CP 1998 Parallelism exploitation in superscalar multiprocessing IEE Proceedings Computers and Digital Techniques Institution of Electrical Engineers 145 4 255 doi 101049ipcdt19981955 Anjum Bushra Perros Harry G 2015 1 Partitioning the EndtoEnd QoS Budget to Domains Bandwidth Allocation for Video Under Quality of Service Constraints Focus Series John Wiley Sons p3 ISBN 9781848217461 Retrieved 20160921 in cloud computing where multiple software components run in a virtual environment on the same blade one component per virtual machine VM Each VM is allocated a virtual central processing unit which is a fraction of the blades CPU Fifield Tom Fleming Diane Gentle Anne Hochstein Lorin Proulx Jonathan Toews Everett Topjian Joe 2014 Glossary OpenStack Operations Guide Beijing OReilly Media Inc p286 ISBN 9781491906309 Retrieved 20160920 Virtual Central Processing Unit vCPU Subdivides physical CPUs Instances can then use those divisions VMware Infrastructure Architecture Overview White Paper PDF VMware VMware 2006 CPU Frequency CPU World Glossary CPU World 25 March 2008 Retrieved 1 January 2010 What is a multicore processor Data Center Definitions SearchDataCentercom Retrieved 8 August 2016 Quad Core Vs Dual Core Tegtmeier Martin CPU utilization of multithreaded architectures explained Oracle Retrieved September 29 2015 External links edit Listen to this article 2 parts info Part 1 Part 2 This audio file was created from a revision of the article Central processing unit dated 20060613 and does not reflect subsequent edits to the article Audio help More spoken articles Wikimedia Commons has media related to Central processing units Wikiversity has learning resources about Introduction to ComputersProcessor How Microprocessors Work at HowStuffWorks 25 Microchips that shook the world an article by the Institute of Electrical and Electronics Engineers v t e Processor technologies Models Turing machine Universal PostTuring Quantum Belt machine Stack machine Finitestate machine with datapath Hierarchical Queue automaton Register machines Counter Pointer Randomaccess Randomaccess stored program Architecture Microarchitecture Von Neumann Harvard modified Dataflow Transporttriggered Cellular Endianness Memory access NUMA HUMA Loadstore Registermemory Cache hierarchy Memory hierarchy Virtual memory Secondary storage Heterogeneous Fabric Multiprocessing Cognitive Neuromorphic Instruction set architectures Types CISC RISC Applicationspecific EDGE TRIPS VLIW EPIC MISC OISC NISC ZISC comparison addressing modes x86 ARM MIPS Power ISA SPARC Itanium Unicore MicroBlaze RISCV others Execution Instruction pipelining Pipeline stall Operand forwarding Classic RISC pipeline Hazards Data dependency Structural Control False sharing Outoforder Tomasulo algorithm Reservation station Reorder buffer Register renaming Speculative Branch prediction Memory dependence prediction Parallelism Level Bit Bitserial Word Instruction Pipelining Scalar Superscalar Task Thread Process Data Vector Memory Distributed Multithreading Temporal Simultaneous Hyperthreading Speculative Preemptive Cooperative Flynns taxonomy SISD SIMD SWAR SIMT MISD MIMD SPMD Processor performance Transistor count Instructions per cycle IPC Cycles per instruction CPI Instructions per second IPS Floatingpoint operations per second FLOPS Transactions per second TPS Synaptic updates per second SUPS Performance per watt PPW Cache performance metrics Computer performance by orders of magnitude Types Central processing unit CPU Graphics processing unit GPU GPGPU Vector Barrel Stream Coprocessor ASIC FPGA CPLD Multichip module MCM System in package SiP By application Microprocessor Microcontroller Mobile Notebook Ultralowvoltage ASIP Systems on chip System on a chip SoC Multiprocessor MPSoC Programmable PSoC Network on a chip NoC Hardware accelerators AI accelerator Vision processing unit VPU Physics processing unit PPU Digital signal processor DSP Tensor processing unit TPU Secure cryptoprocessor Network processor Baseband processor Word size 1bit 2bit 4bit 8bit 16bit 32bit 48bit 64bit 128bit 256bit 512bit others variable Core count Singlecore Multicore Manycore Heterogeneous architecture Components Core Cache CPU cache replacement policies coherence Bus Clock rate Clock signal FIFO Functional units Arithmetic logic unit ALU Address generation unit AGU Floatingpoint unit FPU Memory management unit MMU Loadstore unit Translation lookaside buffer TLB Integrated memory controller IMC Logic Combinational Sequential Glue Logic gate Quantum Array Registers Processor register Status register Stack register Register file Memory buffer Program counter Control unit Instruction unit Data buffer Write buffer Microcode ROM Counter Datapath Multiplexer Demultiplexer Adder Multiplier CPU Binary decoder Address decoder Sum addressed decoder Barrel shifter Circuitry Integrated circuit 3D Mixedsignal Power management Boolean Digital Analog Quantum Switch Power management PMU APM ACPI Dynamic frequency scaling Dynamic voltage scaling Clock gating Performance per watt PPW Race to sleep Related History of generalpurpose CPUs Microprocessor chronology Processor design Digital electronics Hardware security module Semiconductor device fabrication Ticktock model v t e Basic computer components Input devices Keyboard Image scanner Microphone Pointing device Graphics tablet Game controller Light pen Mouse Optical Pointing stick Touchpad Touchscreen Trackball Refreshable braille display Sound card Sound chip Webcam Softcam Video card GPU Output devices Monitor Screen Refreshable braille display Printer Plotter Speakers Sound card Video card Removable data storage Disk pack Floppy disk Optical disc CD DVD Bluray Flash memory Memory card USB flash drive Computer case Central processing unit Microprocessor Motherboard Memory RAM BIOS Data storage HDD SSD SSHD Power supply SMPS MOSFET Power MOSFET VRM Network interface controller Fax modem Expansion card Ports Ethernet FireWire IEEE 1394 Parallel port Serial port PS2 port USB Thunderbolt DisplayPort HDMI DVI VGA eSATA Audio jack