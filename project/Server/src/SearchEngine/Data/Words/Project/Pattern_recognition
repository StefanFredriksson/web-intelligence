Pattern recognition This article is about pattern recognition as a branch of engineering For the cognitive process see Pattern recognition psychology For other uses see Pattern recognition disambiguation This article has multiple issues Please help improve it or discuss these issues on the talk page Learn how and when to remove these template messages This article needs additional citations for verification Please help improve this article by adding citations to reliable sources Unsourced material may be challenged and removed Find sources Pattern recognition news newspapers books scholar JSTOR May 2019 Learn how and when to remove this template message This articles factual accuracy is disputed Relevant discussion may be found on the talk page Please help to ensure that disputed statements are reliably sourced May 2019 Learn how and when to remove this template message Learn how and when to remove this template message Machine learning and data mining Problems Classification Clustering Regression Anomaly detection AutoML Association rules Reinforcement learning Structured prediction Feature engineering Feature learning Online learning Semisupervised learning Unsupervised learning Learning to rank Grammar induction Supervised learning mwparseroutput noboldfontweightnormal classification regression Decision trees Ensembles Bagging Boosting Random forest k NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine RVM Support vector machine SVM Clustering BIRCH CURE Hierarchical k means Expectationmaximization EM DBSCAN OPTICS Meanshift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA tSNE Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection k NN Local outlier factor Artificial neural network Autoencoder Deep learning DeepDream Multilayer perceptron RNN LSTM GRU Restricted Boltzmann machine GAN SOM Convolutional neural network UNet Reinforcement learning Qlearning SARSA Temporal difference TD Theory Biasvariance dilemma Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Machinelearning venues NeurIPS ICML ML JMLR ArXivcsLG Glossary of artificial intelligence Glossary of artificial intelligence Related articles List of datasets for machinelearning research Outline of machine learning v t e Pattern recognition is the automated recognition of patterns and regularities in data Pattern recognition is closely related to artificial intelligence and machine learning 1 together with applications such as data mining and knowledge discovery in databases KDD and is often used interchangeably with these terms However these are distinguished machine learning is one approach to pattern recognition while other approaches include handcrafted not learned rules or heuristics and pattern recognition is one approach to artificial intelligence while other approaches include symbolic artificial intelligence 2 A modern definition of pattern recognition is mwparseroutput templatequoteoverflowhiddenmargin1em 0padding0 40pxmwparseroutput templatequote templatequotecitelineheight15emtextalignleftpaddingleft16emmargintop0 The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories 3 This article focuses on machine learning approaches to pattern recognition Pattern recognition systems are in many cases trained from labeled training data supervised learning but when no labeled data are available other algorithms can be used to discover previously unknown patterns unsupervised learning Machine learning is strongly related to pattern recognition and originates from artificial intelligence KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration It originated in engineering and the term is popular in the context of computer vision a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition In pattern recognition there may be a higher interest to formalize explain and visualize the pattern while machine learning traditionally focuses on maximizing the recognition rates Yet all of these domains have evolved substantially from their roots in artificial intelligence engineering and statistics and theyve become increasingly similar by integrating developments and ideas from each other In machine learning pattern recognition is the assignment of a label to a given input value In statistics discriminant analysis was introduced for this same purpose in 1936 An example of pattern recognition is classification which attempts to assign each input value to one of a given set of classes for example determine whether a given email is spam or nonspam However pattern recognition is a more general problem that encompasses other types of output as well Other examples are regression which assigns a realvalued output to each input 4 sequence labeling which assigns a class to each member of a sequence of values 5 for example part of speech tagging which assigns a part of speech to each word in an input sentence and parsing which assigns a parse tree to an input sentence describing the syntactic structure of the sentence 6 Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform most likely matching of the inputs taking into account their statistical variation This is opposed to pattern matching algorithms which look for exact matches in the input with preexisting patterns A common example of a patternmatching algorithm is regular expression matching which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors In contrast to pattern recognition pattern matching is not generally a type of machine learning although patternmatching algorithms especially with fairly general carefully tailored patterns can sometimes succeed in providing similarquality output of the sort provided by patternrecognition algorithms Contents 1 Overview 11 Probabilistic classifiers 12 Number of important feature variables 2 Problem statement supervised version 21 Frequentist or Bayesian approach to pattern recognition 3 Uses 4 Algorithms 41 Classification algorithms supervised algorithms predicting categorical labels 42 Clustering algorithms unsupervised algorithms predicting categorical labels 43 Ensemble learning algorithms supervised metaalgorithms for combining multiple learning algorithms together 44 General algorithms for predicting arbitrarilystructured sets of labels 45 Multilinear subspace learning algorithms predicting labels of multidimensional data using tensor representations 46 Realvalued sequence labeling algorithms predicting sequences of realvalued labels 47 Regression algorithms predicting realvalued labels 48 Sequence labeling algorithms predicting sequences of categorical labels 5 See also 6 References 7 Further reading 8 External links Overview edit Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value Supervised learning assumes that a set of training data the training set has been provided consisting of a set of instances that have been properly labeled by hand with the correct output A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives Perform as well as possible on the training data and generalize as well as possible to new data usually this means being as simple as possible for some technical definition of simple in accordance with Occams Razor discussed below Unsupervised learning on the other hand assumes training data that has not been handlabeled and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances 7 A combination of the two that has recently been explored is semisupervised learning which uses a combination of labeled and unlabeled data typically a small set of labeled data combined with a large amount of unlabeled data Note that in cases of unsupervised learning there may be no training data at all to speak of in other wordsand the data to be labeled is the training data Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output For example the unsupervised equivalent of classification is normally known as clustering based on the common perception of the task as involving no training data to speak of and of grouping the input data into clusters based on some inherent similarity measure eg the distance between instances considered as vectors in a multidimensional vector space rather than assigning each input instance into one of a set of predefined classes Note also that in some fields the terminology is different For example in community ecology the term classification is used to refer to what is commonly known as clustering The piece of input data for which an output value is generated is formally termed an instance The instance is formally described by a vector of features which together constitute a description of all known characteristics of the instance These feature vectors can be seen as defining points in an appropriate multidimensional space and methods for manipulating vectors in vector spaces can be correspondingly applied to them such as computing the dot product or the angle between two vectors Typically features are either categorical also known as nominal ie consisting of one of a set of unordered items such as a gender of male or female or a blood type of A B AB or O ordinal consisting of one of a set of ordered items eg large medium or small integervalued eg a count of the number of occurrences of a particular word in an email or realvalued eg a measurement of blood pressure Often categorical and ordinal data are grouped together likewise for integervalued and realvalued data Furthermore many algorithms work only in terms of categorical data and require that realvalued or integervalued data be discretized into groups eg less than 5 between 5 and 10 or greater than 10 Probabilistic classifiers edit Main article Probabilistic classifier Many common pattern recognition algorithms are probabilistic in nature in that they use statistical inference to find the best label for a given instance Unlike other algorithms which simply output a best label often probabilistic algorithms also output a probability of the instance being described by the given label In addition many probabilistic algorithms output a list of the N best labels with associated probabilities for some value of N instead of simply a single best label When the number of possible labels is fairly small eg in the case of classification N may be set so that the probability of all possible labels is output Probabilistic algorithms have many advantages over nonprobabilistic algorithms They output a confidence value associated with their choice Note that some other algorithms may also output confidence values but in general only for probabilistic algorithms is this value mathematically grounded in probability theory Nonprobabilistic confidence values can in general not be given any specific meaning and only used to compare against other confidence values output by the same algorithm Correspondingly they can abstain when the confidence of choosing any particular output is too low Because of the probabilities output probabilistic patternrecognition algorithms can be more effectively incorporated into larger machinelearning tasks in a way that partially or completely avoids the problem of error propagation Number of important feature variables edit Feature selection algorithms attempt to directly prune out redundant or irrelevant features A general introduction to feature selection which summarizes approaches and challenges has been given 8 The complexity of featureselection is because of its nonmonotonous character an optimization problem where given a total of n displaystyle n features the powerset consisting of all 2 n 1 displaystyle 2n1 subsets of features need to be explored The BranchandBound algorithm 9 does reduce this complexity but is intractable for medium to large values of the number of available features n displaystyle n For a largescale comparison of featureselection algorithms see 10 Techniques to transform the raw feature vectors feature extraction are sometimes used prior to application of the patternmatching algorithm For example feature extraction algorithms attempt to reduce a largedimensionality feature vector into a smallerdimensionality vector that is easier to work with and encodes less redundancy using mathematical techniques such as principal components analysis PCA The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable while the features left after feature selection are simply a subset of the original features Problem statement supervised version edit Formally the problem of supervised pattern recognition can be stated as follows Given an unknown function g X Y displaystyle gmathcal Xrightarrow mathcal Y the ground truth that maps input instances x X displaystyle boldsymbol xin mathcal X to output labels y Y displaystyle yin mathcal Y along with training data D x 1 y 1 x n y n displaystyle mathbf D boldsymbol x1y1dots boldsymbol xnyn assumed to represent accurate examples of the mapping produce a function h X Y displaystyle hmathcal Xrightarrow mathcal Y that approximates as closely as possible the correct mapping g displaystyle g For example if the problem is filtering spam then x i displaystyle boldsymbol xi is some representation of an email and y displaystyle y is either spam or nonspam In order for this to be a welldefined problem approximates as closely as possible needs to be defined rigorously In decision theory this is defined by specifying a loss function or cost function that assigns a specific value to loss resulting from producing an incorrect label The goal then is to minimize the expected loss with the expectation taken over the probability distribution of X displaystyle mathcal X In practice neither the distribution of X displaystyle mathcal X nor the ground truth function g X Y displaystyle gmathcal Xrightarrow mathcal Y are known exactly but can be computed only empirically by collecting a large number of samples of X displaystyle mathcal X and handlabeling them using the correct value of Y displaystyle mathcal Y a timeconsuming process which is typically the limiting factor in the amount of data of this sort that can be collected The particular loss function depends on the type of label being predicted For example in the case of classification the simple zeroone loss function is often sufficient This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data ie counting up the fraction of instances that the learned function h X Y displaystyle hmathcal Xrightarrow mathcal Y labels wrongly which is equivalent to maximizing the number of correctly classified instances The goal of the learning procedure is then to minimize the error rate maximize the correctness on a typical test set For a probabilistic pattern recognizer the problem is instead to estimate the probability of each possible output label given a particular input instance ie to estimate a function of the form p l a b e l x θ f x θ displaystyle prm labelboldsymbol xboldsymbol theta fleftboldsymbol xboldsymbol theta right where the feature vector input is x displaystyle boldsymbol x and the function f is typically parameterized by some parameters θ displaystyle boldsymbol theta 11 In a discriminative approach to the problem f is estimated directly In a generative approach however the inverse probability p x l a b e l displaystyle pboldsymbol xrm label is instead estimated and combined with the prior probability p l a b e l θ displaystyle prm labelboldsymbol theta using Bayes rule as follows p l a b e l x θ p x l a b e l θ p l a b e l θ L all labels p x L p L θ displaystyle prm labelboldsymbol xboldsymbol theta frac pboldsymbol xrm labelboldsymbol theta prm labelboldsymbol theta sum Lin textall labelspboldsymbol xLpLboldsymbol theta When the labels are continuously distributed eg in regression analysis the denominator involves integration rather than summation p l a b e l x θ p x l a b e l θ p l a b e l θ L all labels p x L p L θ d L displaystyle prm labelboldsymbol xboldsymbol theta frac pboldsymbol xrm labelboldsymbol theta prm labelboldsymbol theta int Lin textall labelspboldsymbol xLpLboldsymbol theta operatorname d L The value of θ displaystyle boldsymbol theta is typically learned using maximum a posteriori MAP estimation This finds the best value that simultaneously meets two conflicting objects To perform as well as possible on the training data smallest errorrate and to find the simplest possible model Essentially this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models In a Bayesian context the regularization procedure can be viewed as placing a prior probability p θ displaystyle pboldsymbol theta on different values of θ displaystyle boldsymbol theta Mathematically θ arg max θ p θ D displaystyle boldsymbol theta arg max boldsymbol theta pboldsymbol theta mathbf D where θ displaystyle boldsymbol theta is the value used for θ displaystyle boldsymbol theta in the subsequent evaluation procedure and p θ D displaystyle pboldsymbol theta mathbf D the posterior probability of θ displaystyle boldsymbol theta is given by p θ D i 1 n p y i x i θ p θ displaystyle pboldsymbol theta mathbf D leftprod i1npyiboldsymbol xiboldsymbol theta rightpboldsymbol theta In the Bayesian approach to this problem instead of choosing a single parameter vector θ displaystyle boldsymbol theta the probability of a given label for a new instance x displaystyle boldsymbol x is computed by integrating over all possible values of θ displaystyle boldsymbol theta weighted according to the posterior probability p l a b e l x p l a b e l x θ p θ D d θ displaystyle prm labelboldsymbol xint prm labelboldsymbol xboldsymbol theta pboldsymbol theta mathbf D operatorname d boldsymbol theta Frequentist or Bayesian approach to pattern recognition edit The first pattern classifier the linear discriminant presented by Fisher was developed in the frequentist tradition The frequentist approach entails that the model parameters are considered unknown but objective The parameters are then computed estimated from the collected data For the linear discriminant these parameters are precisely the mean vectors and the covariance matrix Also the probability of each class p l a b e l θ displaystyle prm labelboldsymbol theta is estimated from the collected dataset Note that the usage of Bayes rule in a pattern classifier does not make the classification approach Bayesian Bayesian statistics has its origin in Greek philosophy where a distinction was already made between the a priori and the a posteriori knowledge Later Kant defined his distinction between what is a priori known before observation and the empirical knowledge gained from observations In a Bayesian pattern classifier the class probabilities p l a b e l θ displaystyle prm labelboldsymbol theta can be chosen by the user which are then a priori Moreover experience quantified as a priori parameter values can be weighted with empirical observations using eg the Beta conjugate prior and Dirichletdistributions The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities and objective observations Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach Uses edit The face was automatically detected by special software Within medical science pattern recognition is the basis for computeraided diagnosis CAD systems CAD describes a procedure that supports the doctors interpretations and findingsOther typical applications of pattern recognition techniques are automatic speech recognition classification of text into several categories eg spamnonspam email messages the automatic recognition of handwriting on postal envelopes automatic recognition of images of human faces or handwriting image extraction from medical forms 12 The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems 13 14 Optical character recognition is a classic example of the application of a pattern classifier see OCRexample The method of signing ones name was captured with stylus and overlay starting in 1990 citation needed The strokes speed relative min relative max acceleration and pressure is used to uniquely identify and confirm identity Banks were first offered this technology but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers citation needed Artificial neural networks neural net classifiers and deep learning have many realworld applications in image processing a few examples identification and authentication eg license plate recognition 15 fingerprint analysis face detection verification 16 and voicebased authentication 17 medical diagnosis eg screening for cervical cancer Papnet 18 breast tumors or heart sounds defence various navigation and guidance systems target recognition systems shape recognition technology etc mobility advanced driver assistance systems autonomous vehicle technology etc 19 20 21 22 23 For a discussion of the aforementioned applications of neural networks in image processing see eg 24 In psychology pattern recognition making sense of and identifying objects is closely related to perception which explains how the sensory inputs humans receive are made meaningful Pattern recognition can be thought of in two different ways the first being template matching and the second being feature detection A template is a pattern used to produce items of the same proportions The templatematching hypothesis suggests that incoming stimuli are compared with templates in the long term memory If there is a match the stimulus is identifiedFeature detection models such as the Pandemonium system for classifying letters Selfridge 1959 suggest that the stimuli are broken down into their component parts for identification For example a capital E has three horizontal lines and one vertical line 25 Algorithms edit Algorithms for pattern recognition depend on the type of label output on whether learning is supervised or unsupervised and on whether the algorithm is statistical or nonstatistical in nature Statistical algorithms can further be categorized as generative or discriminative This article contains embedded lists that may be poorly defined unverified or indiscriminate Please help to clean it up to meet Wikipedias quality standards Where appropriate incorporate items into the main body of the article May 2014 Classification algorithms supervised algorithms predicting categorical labels edit Main article Statistical classification Parametric 26 Linear discriminant analysis Quadratic discriminant analysis Maximum entropy classifier aka logistic regression multinomial logistic regression Note that logistic regression is an algorithm for classification despite its name The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class Nonparametric 27 Decision trees decision lists Kernel estimation and Knearestneighbor algorithms Naive Bayes classifier Neural networks multilayer perceptrons Perceptrons Support vector machines Gene expression programming Clustering algorithms unsupervised algorithms predicting categorical labels edit Main article Cluster analysis Categorical mixture models Hierarchical clustering agglomerative or divisive Kmeans clustering Correlation clustering Kernel principal component analysis Kernel PCA Ensemble learning algorithms supervised metaalgorithms for combining multiple learning algorithms together edit Main article Ensemble learning Boosting metaalgorithm Bootstrap aggregating bagging Ensemble averaging Mixture of experts hierarchical mixture of experts General algorithms for predicting arbitrarilystructured sets of labels edit Bayesian networks Markov random fields Multilinear subspace learning algorithms predicting labels of multidimensional data using tensor representations edit Unsupervised Multilinear principal component analysis MPCA Realvalued sequence labeling algorithms predicting sequences of realvalued labels edit Main article sequence labeling Supervised Kalman filters Particle filters Regression algorithms predicting realvalued labels edit Main article Regression analysis Supervised Gaussian process regression kriging Linear regression and extensions Neural networks and Deep learning methods Unsupervised Independent component analysis ICA Principal components analysis PCA Sequence labeling algorithms predicting sequences of categorical labels edit Supervised Conditional random fields CRFs Hidden Markov models HMMs Maximum entropy Markov models MEMMs Recurrent neural networks RNNs Unsupervised Hidden Markov models HMMs Dynamic time warping DTW See also edit Adaptive resonance theory Black box Cache language model Compound term processing Computeraided diagnosis Data mining Deep Learning Information theory List of numerical analysis software List of numerical libraries Machine learning Multilinear subspace learning Neocognitron Perception Perceptual learning Predictive analytics Prior knowledge for pattern recognition Sequence mining Template matching Contextual image classification List of datasets for machine learning research References edit This article is based on material taken from the Free Online Dictionary of Computing prior to 1 November 2008 and incorporated under the relicensing terms of the GFDL version 13 or later Bishop Christopher M 2006 Pattern Recognition and Machine Learning PDF Springer pvii Pattern recognition has its origins in engineering whereas machine learning grew out of computer science However these activities can be viewed as two facets of the same field and together they have undergone substantial development over the past ten years mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput idlockfree amwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocklimited amwparseroutput idlockregistration amwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocksubscription amwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em Tveter Donald 1998 The Pattern Recognition Basis of Artificial Intelligence Bishop 2006 p 1 Howard WR 20070220 Pattern Recognition and Machine Learning20072Christopher M Bishop Pattern Recognition and Machine Learning Heidelberg Germany Springer 2006 ixx 740 pp ISBN 0387310738 7495 Hardcover Kybernetes 36 2 275 doi 10110803684920710743466 ISSN 0368492X Sequence Labeling PDF utahedu Ian Chiswell 2007 Mathematical logic p 34 Oxford University Press ISBN 9780199215621 OCLC 799802313 Carvalko JR Preston K 1972 On Determining Optimum Simple Golay Marking Transforms for Binary Image Processing IEEE Transactions on Computers 21 12 143033 doi 101109TC1972223519 CS1 maint multiple names authors list link Isabelle Guyon Clopinet André Elisseeff 2003 An Introduction to Variable and Feature Selection The Journal of Machine Learning Research Vol 3 11571182 Link Iman Foroutan Jack Sklansky 1987 Feature Selection for Automatic Classification of NonGaussian Data IEEE Transactions on Systems Man and Cybernetics 17 2 187198 doi 101109TSMC19874309029 Mineichi Kudo Jack Sklansky 2000 Comparison of algorithms that select features for pattern classifiers Pattern Recognition 33 1 2541 CiteSeerX 1011551718 doi 101016S0031320399000412 For linear discriminant analysis the parameter vector θ displaystyle boldsymbol theta consists of the two mean vectors μ 1 displaystyle boldsymbol mu 1 and μ 2 displaystyle boldsymbol mu 2 and the common covariance matrix Σ displaystyle boldsymbol Sigma Milewski Robert Govindaraju Venu 31 March 2008 Binarization and cleanup of handwritten text from carbon copy medical form images Pattern Recognition 41 4 13081315 doi 101016jpatcog200708018 Richard O Duda Peter E Hart David G Stork 2001 Pattern classification 2nd ed Wiley New York ISBN 9780471056690 CS1 maint multiple names authors list link R Brunelli Template Matching Techniques in Computer Vision Theory and Practice Wiley ISBN 9780470517062 2009 THE AUTOMATIC NUMBER PLATE RECOGNITION TUTORIAL httpanprtutorialcom Neural Networks for Face Recognition Companion to Chapter 4 of the textbook Machine Learning Poddar Arnab Sahidullah Md Saha Goutam March 2018 Speaker Verification with Short Utterances A Review of Challenges Trends and Opportunities IET Biometrics 7 2 91101 doi 101049ietbmt20170065 PAPNET For Cervical Screening Archived 20120708 at Archivetoday Archived copy Archived from the original on 20120708 Retrieved 20120506 CS1 maint archived copy as title link Development of an Autonomous Vehicle ControlStrategy Using a Single Camera and Deep Neural Networks 2018010035 Technical Paper SAE Mobilus saemobilussaeorg Retrieved 20190906 Gerdes J Christian Kegelman John C Kapania Nitin R Brown Matthew Spielberg Nathan A 20190327 Neural network vehicle models for highperformance automated driving Science Robotics 4 28 eaaw1975 doi 101126sciroboticsaaw1975 ISSN 24709476 Pickering Chris 20170815 How AI is paving the way for fully autonomous cars The Engineer Retrieved 20190906 Ray Baishakhi Jana Suman Pei Kexin Tian Yuchi 20170828 DeepTest Automated Testing of DeepNeuralNetworkdriven Autonomous Cars Cite journal requires journal help Sinha P K Hadjiiski L M Mutib K 19930401 Neural Networks in Autonomous Vehicle Control IFAC Proceedings Volumes 1st IFAC International Workshop on Intelligent Autonomous Vehicles Hampshire UK 1821 April 26 1 335340 doi 101016S1474667017493220 ISSN 14746670 EgmontPetersen M de Ridder D Handels H 2002 Image processing with neural networks a review Pattern Recognition 35 10 22792301 CiteSeerX 1011215444 doi 101016S0031320301001789 CS1 maint multiple names authors list link Alevel Psychology Attention Revision Pattern recognition Scool the revision website Scoolcouk Retrieved 20120917 Assuming known distributional shape of feature distributions per class such as the Gaussian shape No distributional assumption regarding shape of feature distributions per class Further reading edit Fukunaga Keinosuke 1990 Introduction to Statistical Pattern Recognition 2nd ed Boston Academic Press ISBN 9780122698514 Hornegger Joachim Paulus Dietrich W R 1999 Applied Pattern Recognition A Practical Introduction to Image and Speech Processing in C 2nd ed San Francisco Morgan Kaufmann Publishers ISBN 9783528155582 Schuermann Juergen 1996 Pattern Classification A Unified View of Statistical and Neural Approaches New York Wiley ISBN 9780471135340 Godfried T Toussaint ed 1988 Computational Morphology Amsterdam NorthHolland Publishing Company Kulikowski Casimir A Weiss Sholom M 1991 Computer Systems That Learn Classification and Prediction Methods from Statistics Neural Nets Machine Learning and Expert Systems Machine Learning San Francisco Morgan Kaufmann Publishers ISBN 9781558600652 Duda Richard O Hart Peter E Stork David G 2000 Pattern Classification 2nd ed WileyInterscience ISBN 9780471056690 Jain AnilK Duin RobertPW Mao Jianchang 2000 Statistical pattern recognition a review IEEE Transactions on Pattern Analysis and Machine Intelligence 22 1 437 CiteSeerX 10111238151 doi 10110934824819 An introductory tutorial to classifiers introducing the basic terms with numeric example External links edit The International Association for Pattern Recognition List of Pattern Recognition web sites Journal of Pattern Recognition Research Pattern Recognition Info Pattern Recognition Journal of the Pattern Recognition Society International Journal of Pattern Recognition and Artificial Intelligence International Journal of Applied Pattern Recognition Open Pattern Recognition Project intended to be an open source platform for sharing algorithms of pattern recognition Improved Fast Pattern Matching Improved Fast Pattern Matching Authority control GND 40409363 NDL 00569072 