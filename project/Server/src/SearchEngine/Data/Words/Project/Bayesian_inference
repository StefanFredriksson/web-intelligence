Bayesian inference Method of statistical inference Part of a series on Statistics Bayesian statistics Theory Admissible decision rule Bayesian efficiency Bayesian probability Probability interpretations Bayes theorem Bayes factor Bayesian inference Bayesian network Prior Posterior Likelihood Conjugate prior Posterior predictive Hyperparameter Hyperprior Principle of indifference Principle of maximum entropy Empirical Bayes method Cromwells rule Bernsteinvon Mises theorem Schwarz criterion Credible interval Maximum a posteriori estimation Radical probabilism Techniques Bayesian linear regression Bayesian estimator Approximate Bayesian computation Markov chain Monte Carlo Mathematics portal v t e Bayesian inference is a method of statistical inference in which Bayes theorem is used to update the probability for a hypothesis as more evidence or information becomes available Bayesian inference is an important technique in statistics and especially in mathematical statistics Bayesian updating is particularly important in the dynamic analysis of a sequence of data Bayesian inference has found application in a wide range of activities including science engineering philosophy medicine sport and law In the philosophy of decision theory Bayesian inference is closely related to subjective probability often called Bayesian probability Contents 1 Introduction to Bayes rule 11 Formal explanation 12 Alternatives to Bayesian updating 2 Formal description of Bayesian inference 21 Definitions 22 Bayesian inference 23 Bayesian prediction 3 Inference over exclusive and exhaustive possibilities 31 General formulation 32 Multiple observations 33 Parametric formulation 4 Mathematical properties 41 Interpretation of factor 42 Cromwells rule 43 Asymptotic behaviour of posterior 44 Conjugate priors 45 Estimates of parameters and predictions 5 Examples 51 Probability of a hypothesis 52 Making a prediction 6 In frequentist statistics and decision theory 61 Model selection 7 Probabilistic programming 8 Applications 81 Computer applications 82 Bioinformatic and healthcare applications 83 In the courtroom 84 Bayesian epistemology 85 Other 9 Bayes and Bayesian inference 10 History 11 See also 12 References 121 Citations 122 Sources 13 Further reading 131 Elementary 132 Intermediate or advanced 14 External links Introduction to Bayes rule edit A geometric visualisation of Bayes theorem In the table the values 2 3 6 and 9 give the relative weights of each corresponding condition and case The figures denote the cells of the table involved in each metric the probability being the fraction of each figure that is shaded This shows that PAB PB PBA PA ie PAB PBA PA PB Similar reasoning can be used to show that PĀB PBĀ PĀ PB etc Main article Bayes theorem See also Bayesian probability Formal explanation edit Bayesian inference derives the posterior probability as a consequence of two antecedents a prior probability and a likelihood function derived from a statistical model for the observed data Bayesian inference computes the posterior probability according to Bayes theorem P H E P E H P H P E displaystyle PHmid Efrac PEmid Hcdot PHPE where H displaystyle textstyle H stands for any hypothesis whose probability may be affected by data called evidence below Often there are competing hypotheses and the task is to determine which is the most probable P H displaystyle textstyle PH the prior probability is the estimate of the probability of the hypothesis H displaystyle textstyle H before the data E displaystyle textstyle E the current evidence is observed E displaystyle textstyle E the evidence corresponds to new data that were not used in computing the prior probability P H E displaystyle textstyle PHmid E the posterior probability is the probability of H displaystyle textstyle H given E displaystyle textstyle E ie after E displaystyle textstyle E is observed This is what we want to know the probability of a hypothesis given the observed evidence P E H displaystyle textstyle PEmid H is the probability of observing E displaystyle textstyle E given H displaystyle textstyle H and is called the likelihood As a function of E displaystyle textstyle E with H displaystyle textstyle H fixed it indicates the compatibility of the evidence with the given hypothesis The likelihood function is a function of the evidence E displaystyle textstyle E while the posterior probability is a function of the hypothesis H displaystyle textstyle H P E displaystyle textstyle PE is sometimes termed the marginal likelihood or model evidence This factor is the same for all possible hypotheses being considered as is evident from the fact that the hypothesis H displaystyle textstyle H does not appear anywhere in the symbol unlike for all the other factors so this factor does not enter into determining the relative probabilities of different hypotheses For different values of H displaystyle textstyle H only the factors P H displaystyle textstyle PH and P E H displaystyle textstyle PEmid H both in the numerator affect the value of P H E displaystyle textstyle PHmid E the posterior probability of a hypothesis is proportional to its prior probability its inherent likeliness and the newly acquired likelihood its compatibility with the new observed evidence Bayes rule can also be written as follows P H E P E H P E P H displaystyle PHmid Efrac PEmid HPEcdot PH where the factor P E H P E displaystyle textstyle frac PEmid HPE can be interpreted as the impact of E displaystyle E on the probability of H displaystyle H Alternatives to Bayesian updating edit Bayesian updating is widely used and computationally convenient However it is not the only updating rule that might be considered rational Ian Hacking noted that traditional Dutch book arguments did not specify Bayesian updating they left open the possibility that nonBayesian updating rules could avoid Dutch books Hacking wrote 1 2 And neither the Dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption Not one entails Bayesianism So the personalist requires the dynamic assumption to be Bayesian It is true that in consistency a personalist could abandon the Bayesian model of learning from experience Salt could lose its savour Indeed there are nonBayesian updating rules that also avoid Dutch books as discussed in the literature on probability kinematics following the publication of Richard C Jeffrey s rule which applies Bayes rule to the case where the evidence itself is assigned a probability 3 The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial complicated and unsatisfactory 4 Formal description of Bayesian inference edit Definitions edit x displaystyle x a data point in general This may in fact be a vector of values θ displaystyle theta the parameter of the data points distribution ie x p x θ displaystyle xsim pxmid theta This may in fact be a vector of parameters α displaystyle alpha the hyperparameter of the parameter distribution ie θ p θ α displaystyle theta sim ptheta mid alpha This may in fact be a vector of hyperparameters X displaystyle mathbf X is the sample a set of n displaystyle n observed data points ie x 1 x n displaystyle x1ldots xn x displaystyle tilde x a new data point whose distribution is to be predicted Bayesian inference edit The prior distribution is the distribution of the parameters before any data is observed ie p θ α displaystyle ptheta mid alpha The prior distribution might not be easily determined in such a case one possibility may be to use the Jeffreys prior to obtain a prior distribution before updating it with newer observations The sampling distribution is the distribution of the observed data conditional on its parameters ie p X θ displaystyle pmathbf X mid theta This is also termed the likelihood especially when viewed as a function of the parameters sometimes written L θ X p X θ displaystyle operatorname L theta mid mathbf X pmathbf X mid theta The marginal likelihood sometimes also termed the evidence is the distribution of the observed data marginalized over the parameters ie p X α p X θ p θ α d θ displaystyle pmathbf X mid alpha int pmathbf X mid theta ptheta mid alpha operatorname d theta The posterior distribution is the distribution of the parameters after taking into account the observed data This is determined by Bayes rule which forms the heart of Bayesian inference p θ X α p θ X α p X α p X θ α p θ α p X α p α p X θ α p θ α p X α p X θ α p θ α displaystyle ptheta mid mathbf X alpha frac ptheta mathbf X alpha pmathbf X alpha frac pmathbf X mid theta alpha ptheta alpha pmathbf X mid alpha palpha frac pmathbf X mid theta alpha ptheta mid alpha pmathbf X mid alpha propto pmathbf X mid theta alpha ptheta mid alpha This is expressed in words as posterior is proportional to likelihood times prior or sometimes as posterior likelihood times prior over evidence Bayesian prediction edit The posterior predictive distribution is the distribution of a new data point marginalized over the posterior p x X α p x θ p θ X α d θ displaystyle ptilde xmid mathbf X alpha int ptilde xmid theta ptheta mid mathbf X alpha operatorname d theta The prior predictive distribution is the distribution of a new data point marginalized over the prior p x α p x θ p θ α d θ displaystyle ptilde xmid alpha int ptilde xmid theta ptheta mid alpha operatorname d theta Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference ie to predict the distribution of a new unobserved data point That is instead of a fixed point as a prediction a distribution over possible points is returned Only this way is the entire posterior distribution of the parameters used By comparison prediction in frequentist statistics often involves finding an optimum point estimate of the parameterseg by maximum likelihood or maximum a posteriori estimation MAPand then plugging this estimate into the formula for the distribution of a data point This has the disadvantage that it does not account for any uncertainty in the value of the parameter and hence will underestimate the variance of the predictive distribution In some instances frequentist statistics can work around this problem For example confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance are constructed using a Students tdistribution This correctly estimates the variance due to the fact that 1the average of normally distributed random variables is also normally distributed 2the predictive distribution of a normally distributed data point with unknown mean and variance using conjugate or uninformative priors has a students tdistribution In Bayesian statistics however the posterior predictive distribution can always be determined exactlyor at least to an arbitrary level of precision when numerical methods are used Both types of predictive distributions have the form of a compound probability distribution as does the marginal likelihood In fact if the prior distribution is a conjugate prior and hence the prior and posterior distributions come from the same family it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters applying the Bayesian update rules given in the conjugate prior article while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution Inference over exclusive and exhaustive possibilities edit If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions Bayesian inference may be thought of as acting on this belief distribution as a whole General formulation edit Diagram illustrating event space Ω displaystyle Omega in general formulation of Bayesian inference Although this diagram shows discrete models and events the continuous case may be visualized similarly using probability densities Suppose a process is generating independent and identically distributed events E n n 1 2 3 displaystyle Enn123ldots but the probability distribution is unknown Let the event space Ω displaystyle Omega represent the current state of belief for this process Each model is represented by event M m displaystyle Mm The conditional probabilities P E n M m displaystyle PEnmid Mm are specified to define the models P M m displaystyle PMm is the degree of belief in M m displaystyle Mm Before the first inference step P M m displaystyle PMm is a set of initial prior probabilities These must sum to 1 but are otherwise arbitrary Suppose that the process is observed to generate E E n displaystyle textstyle Ein En For each M M m displaystyle Min Mm the prior P M displaystyle PM is updated to the posterior P M E displaystyle PMmid E From Bayes theorem 5 P M E P E M m P E M m P M m P M displaystyle PMmid Efrac PEmid Msum mPEmid MmPMmcdot PM Upon observation of further evidence this procedure may be repeated Multiple observations edit For a sequence of independent and identically distributed observations E e 1 e n displaystyle mathbf E e1dots en it can be shown by induction that repeated application of the above is equivalent to P M E P E M m P E M m P M m P M displaystyle PMmid mathbf E frac Pmathbf E mid Msum mPmathbf E mid MmPMmcdot PM Where P E M k P e k M displaystyle Pmathbf E mid Mprod kPekmid M Parametric formulation edit By parameterizing the space of models the belief in all models may be updated in a single step The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space The distributions in this section are expressed as continuous represented by probability densities as this is the usual situation The technique is however equally applicable to discrete distributions Let the vector θ displaystyle mathbf theta span the parameter space Let the initial prior distribution over θ displaystyle mathbf theta be p θ α displaystyle pmathbf theta mid mathbf alpha where α displaystyle mathbf alpha is a set of parameters to the prior itself or hyperparameters Let E e 1 e n displaystyle mathbf E e1dots en be a sequence of independent and identically distributed event observations where all e i displaystyle ei are distributed as p e θ displaystyle pemid mathbf theta for some θ displaystyle mathbf theta Bayes theorem is applied to find the posterior distribution over θ displaystyle mathbf theta p θ E α p E θ α p E α p θ α p E θ α p E θ α p θ α d θ p θ α displaystyle beginalignedpmathbf theta mid mathbf E mathbf alpha frac pmathbf E mid mathbf theta mathbf alpha pmathbf E mid mathbf alpha cdot pmathbf theta mid mathbf alpha frac pmathbf E mid mathbf theta mathbf alpha int pmathbf E mathbf theta mathbf alpha pmathbf theta mid mathbf alpha dmathbf theta cdot pmathbf theta mid mathbf alpha endaligned Where p E θ α k p e k θ displaystyle pmathbf E mid mathbf theta mathbf alpha prod kpekmid mathbf theta Mathematical properties edit This section includes a list of references but its sources remain unclear because it has insufficient inline citations Please help to improve this section by introducing more precise citations February 2012 Learn how and when to remove this template message Interpretation of factor edit P E M P E 1 P E M P E displaystyle textstyle frac PEmid MPE1Rightarrow textstyle PEmid MPE That is if the model were true the evidence would be more likely than is predicted by the current state of belief The reverse applies for a decrease in belief If the belief does not change P E M P E 1 P E M P E displaystyle textstyle frac PEmid MPE1Rightarrow textstyle PEmid MPE That is the evidence is independent of the model If the model were true the evidence would be exactly as likely as predicted by the current state of belief Cromwells rule edit Main article Cromwells rule If P M 0 displaystyle PM0 then P M E 0 displaystyle PMmid E0 If P M 1 displaystyle PM1 then P M E 1 displaystyle PME1 This can be interpreted to mean that hard convictions are insensitive to counterevidence The former follows directly from Bayes theorem The latter can be derived by applying the first rule to the event not M displaystyle M in place of M displaystyle M yielding if 1 P M 0 displaystyle 1PM0 then 1 P M E 0 displaystyle 1PMmid E0 from which the result immediately follows Asymptotic behaviour of posterior edit Consider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials For sufficiently nice prior probabilities the Bernsteinvon Mises theorem gives that in the limit of infinite trials the posterior converges to a Gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by Joseph L Doob in 1948 namely if the random variable in consideration has a finite probability space The more general results were obtained later by the statistician David A Freedman who published in two seminal research papers in 1963 6 and 1965 7 when and under what circumstances the asymptotic behaviour of posterior is guaranteed His 1963 paper treats like Doob 1949 the finite case and comes to a satisfactory conclusion However if the random variable has an infinite but countable probability space ie corresponding to a die with infinite many faces the 1965 paper demonstrates that for a dense subset of priors the Bernsteinvon Mises theorem is not applicable In this case there is almost surely no asymptotic convergence Later in the 1980s and 1990s Freedman and Persi Diaconis continued to work on the case of infinite countable probability spaces 8 To summarise there may be insufficient trials to suppress the effects of the initial choice and especially for large but finite systems the convergence might be very slow Conjugate priors edit Main article Conjugate prior In parameterized form the prior distribution is often assumed to come from a family of distributions called conjugate priors The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family and the calculation may be expressed in closed form Estimates of parameters and predictions edit It is often desired to use a posterior distribution to estimate a parameter or variable Several methods of Bayesian estimation select measurements of central tendency from the posterior distribution For onedimensional problems a unique median exists for practical continuous problems The posterior median is attractive as a robust estimator 9 If there exists a finite mean for the posterior distribution then the posterior mean is a method of estimation 10 θ E θ θ p θ X α d θ displaystyle tilde theta operatorname E theta int theta ptheta mid mathbf X alpha dtheta Taking a value with the greatest probability defines maximum aposteriori MAP estimates 11 θ MAP arg max θ p θ X α displaystyle theta textMAPsubset arg max theta ptheta mid mathbf X alpha There are examples where no maximum is attained in which case the set of MAP estimates is empty There are other methods of estimation that minimize the posterior risk expectedposterior loss with respect to a loss function and these are of interest to statistical decision theory using the sampling distribution frequentist statistics 12 The posterior predictive distribution of a new observation x displaystyle tilde x that is independent of previous observations is determined by 13 p x X α p x θ X α d θ p x θ p θ X α d θ displaystyle ptilde xmathbf X alpha int ptilde xtheta mid mathbf X alpha dtheta int ptilde xmid theta ptheta mid mathbf X alpha dtheta Examples edit Probability of a hypothesis edit Suppose there are two full bowls of cookies Bowl 1 has 10 chocolate chip and 30 plain cookies while bowl 2 has 20 of each Our friend Fred picks a bowl at random and then picks a cookie at random We may assume there is no reason to believe Fred treats one bowl differently from another likewise for the cookies The cookie turns out to be a plain one How probable is it that Fred picked it out of bowl 1 Intuitively it seems clear that the answer should be more than a half since there are more plain cookies in bowl 1 The precise answer is given by Bayes theorem Let H 1 displaystyle H1 correspond to bowl 1 and H 2 displaystyle H2 to bowl 2It is given that the bowls are identical from Freds point of view thus P H 1 P H 2 displaystyle PH1PH2 and the two must add up to 1 so both are equal to 05The event E displaystyle E is the observation of a plain cookie From the contents of the bowls we know that P E H 1 30 40 075 displaystyle PEmid H13040075 and P E H 2 20 40 05 displaystyle PEmid H2204005 Bayes formula then yields P H 1 E P E H 1 P H 1 P E H 1 P H 1 P E H 2 P H 2 075 05 075 05 05 05 06 displaystyle beginalignedPH1mid Efrac PEmid H1PH1PEmid H1PH1PEmid H2PH2 frac 075times 05075times 0505times 05 06endaligned Before we observed the cookie the probability we assigned for Fred having chosen bowl 1 was the prior probability P H 1 displaystyle PH1 which was 05 After observing the cookie we must revise the probability to P H 1 E displaystyle PH1mid E which is 06 Making a prediction edit Example results for archaeology example This simulation was generated using c152 An archaeologist is working at a site thought to be from the medieval period between the 11th century to the 16th century However it is uncertain exactly when in this period the site was inhabited Fragments of pottery are found some of which are glazed and some of which are decorated It is expected that if the site were inhabited during the early medieval period then 1 of the pottery would be glazed and 50 of its area decorated whereas if it had been inhabited in the late medieval period then 81 would be glazed and 5 of its area decorated How confident can the archaeologist be in the date of inhabitation as fragments are unearthed The degree of belief in the continuous variable C displaystyle C century is to be calculated with the discrete set of events G D G D G D G D displaystyle GDGbar Dbar GDbar Gbar D as evidence Assuming linear variation of glaze and decoration with time and that these variables are independent P E G D C c 001 081 001 16 11 c 11 05 05 005 16 11 c 11 displaystyle PEGDmid Cc001frac 0810011611c1105frac 050051611c11 P E G D C c 001 081 001 16 11 c 11 05 05 005 16 11 c 11 displaystyle PEGbar Dmid Cc001frac 0810011611c1105frac 050051611c11 P E G D C c 1 001 081 001 16 11 c 11 05 05 005 16 11 c 11 displaystyle PEbar GDmid Cc1001frac 0810011611c1105frac 050051611c11 P E G D C c 1 001 081 001 16 11 c 11 05 05 005 16 11 c 11 displaystyle PEbar Gbar Dmid Cc1001frac 0810011611c1105frac 050051611c11 Assume a uniform prior of f C c 02 displaystyle textstyle fCc02 and that trials are independent and identically distributed When a new fragment of type e displaystyle e is discovered Bayes theorem is applied to update the degree of belief for each c displaystyle c f C c E e P E e C c P E e f C c P E e C c 11 16 P E e C c f C c d c f C c displaystyle fCcmid Eefrac PEemid CcPEefCcfrac PEemid Ccint 1116PEemid CcfCcdcfCc A computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph In the simulation the site was inhabited around 1420 or c 152 displaystyle c152 By calculating the area under the relevant portion of the graph for 50 trials the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries about 1 chance that it was inhabited during the 13th century 63 chance during the 14th century and 36 during the 15th century The Bernsteinvon Mises theorem asserts here the asymptotic convergence to the true distribution because the probability space corresponding to the discrete set of events G D G D G D G D displaystyle GDGbar Dbar GDbar Gbar D is finite see above section on asymptotic behaviour of the posterior In frequentist statistics and decision theory edit A decisiontheoretic justification of the use of Bayesian inference was given by Abraham Wald who proved that every unique Bayesian procedure is admissible Conversely every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures 14 Wald characterized admissible procedures as Bayesian procedures and limits of Bayesian procedures making the Bayesian formalism a central technique in such areas of frequentist inference as parameter estimation hypothesis testing and computing confidence intervals 15 16 17 For example Under some conditions all admissible procedures are either Bayes procedures or limits of Bayes procedures in various senses These remarkable results at least in their original form are due essentially to Wald They are useful because the property of being Bayes is easier to analyze than admissibility 14 In decision theory a quite general method for proving admissibility consists in exhibiting a procedure as a unique Bayes solution 18 In the first chapters of this work prior distributions with finite support and the corresponding Bayes procedures were used to establish some of the main theorems relating to the comparison of experiments Bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics including its asymptotic theory There are many problems where a glance at posterior distributions for suitable priors yields immediately interesting information Also this technique can hardly be avoided in sequential analysis 19 A useful fact is that any Bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible 20 An important area of investigation in the development of admissibility ideas has been that of conventional samplingtheory procedures and many interesting results have been obtained 21 Model selection edit Main article Bayesian model selection This section is empty You can help by adding to it July 2018 Probabilistic programming edit Main article Probabilistic programmingn While conceptually simple Bayesian methods can be mathematically and numerically challenging Probabilistic programming languages PPLs implement functions to easily build Bayesian models together with efficient automatic inference methods This helps separate the model building from the inference allowing practitioners to focus on their specific problems and leaving PPLs to handle the computational details for them 22 23 24 Applications edit Computer applications edit Bayesian inference has applications in artificial intelligence and expert systems Bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s There is also an evergrowing connection between Bayesian methods and simulationbased Monte Carlo techniques since complex models cannot be processed in closed form by a Bayesian analysis while a graphical model structure may allow for efficient simulation algorithms like the Gibbs sampling and other MetropolisHastings algorithm schemes 25 Recently when Bayesian inference has gained popularity among the phylogenetics community for these reasons a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously As applied to statistical classification Bayesian inference has been used in recent years to develop algorithms for identifying email spam Applications which make use of Bayesian inference for spam filtering include CRM114 DSPAM Bogofilter SpamAssassin SpamBayes Mozilla XEAMS and others Spam classification is treated in more detail in the article on the naive Bayes classifier Solomonoffs Inductive inference is the theory of prediction based on observations for example predicting the next symbol based upon a given series of symbols The only assumption is that the environment follows some unknown but computable probability distribution It is a formal inductive framework that combines two wellstudied principles of inductive inference Bayesian statistics and Occams Razor 26 unreliable source Solomonoffs universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs for a universal computer that compute something starting with p Given some p and any computable but unknown probability distribution from which x is sampled the universal prior and Bayes theorem can be used to predict the yet unseen parts of x in optimal fashion 27 28 Bioinformatic and healthcare applications edit Bayesian inference has been applied in different Bioinformatics applications including differential gene expression analysis 29 Bayesian inference is also used in a general cancer risk model called CIRI Continuous Individualized Risk Index where serial measurements are incorporated to update a Bayesian model which is primarily built from prior knowledge 30 31 In the courtroom edit Main article Jurimetrics Bayesian analysis of evidence Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant and to see whether in totality it meets their personal threshold for beyond a reasonable doubt 32 33 34 Bayes theorem is applied successively to all evidence presented with the posterior from one stage becoming the prior for the next The benefit of a Bayesian approach is that it gives the juror an unbiased rational mechanism for combining evidence It may be appropriate to explain Bayes theorem to jurors in odds form as betting odds are more widely understood than probabilities Alternatively a logarithmic approach replacing multiplication with addition might be easier for a jury to handle Adding up evidence If the existence of the crime is not in doubt only the identity of the culprit it has been suggested that the prior should be uniform over the qualifying population 35 For example if 1000 people could have committed the crime the prior probability of guilt would be 11000 The use of Bayes theorem by jurors is controversial In the United Kingdom a defence expert witness explained Bayes theorem to the jury in R v Adams The jury convicted but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes theorem The Court of Appeal upheld the conviction but it also gave the opinion that To introduce Bayes Theorem or any similar method into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity deflecting them from their proper task GardnerMedwin 36 argues that the criterion on which a verdict in a criminal trial should be based is not the probability of guilt but rather the probability of the evidence given that the defendant is innocent akin to a frequentist pvalue He argues that if the posterior probability of guilt is to be computed by Bayes theorem the prior probability of guilt must be known This will depend on the incidence of the crime which is an unusual piece of evidence to consider in a criminal trial Consider the following three propositions A The known facts and testimony could have arisen if the defendant is guilty B The known facts and testimony could have arisen if the defendant is innocent C The defendant is guilty GardnerMedwin argues that the jury should believe both A and notB in order to convict A and notB implies the truth of C but the reverse is not true It is possible that B and C are both true but in this case he argues that a jury should acquit even though they know that they will be letting some guilty people go free See also Lindleys paradox Bayesian epistemology edit Bayesian epistemology is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic Karl Popper and David Miller have rejected the idea of Bayesian rationalism ie using Bayes rule to make epistemological inferences 37 It is prone to the same vicious circle as any other justificationist epistemology because it presupposes what it attempts to justify According to this view a rational interpretation of Bayesian inference would see it merely as a probabilistic version of falsification rejecting the belief commonly held by Bayesians that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt or even with likelihood greater than 0 Other edit The scientific method is sometimes interpreted as an application of Bayesian inference In this view Bayes rule guides or should guide the updating of probabilities about hypotheses conditional on new observations or experiments 38 The Bayesian inference has also been applied to treat stochastic scheduling problems with incomplete information by Cai et al 2009 39 Bayesian search theory is used to search for lost objects Bayesian inference in phylogeny Bayesian tool for methylation analysis Bayesian approaches to brain function investigate the brain as a Bayesian mechanism Bayesian inference in ecological studies 40 41 Bayesian inference is used to estimate parameters in stochastic chemical kinetic models 42 Bayes and Bayesian inference edit The problem considered by Bayes in Proposition9 of his essay An Essay towards solving a Problem in the Doctrine of Chances is the posterior distribution for the parameter a the success rate of the binomial distribution citation needed History edit Main article History of statistics Bayesian statistics The term Bayesian refers to Thomas Bayes 17021761 who proved that probabilistic limits could be placed on an unknown event However it was PierreSimon Laplace 17491827 who introduced as Principle VI what is now called Bayes theorem and used it to address problems in celestial mechanics medical statistics reliability and jurisprudence 43 Early Bayesian inference which used uniform priors following Laplaces principle of insufficient reason was called inverse probability because it infers backwards from observations to parameters or from effects to causes 44 After the 1920s inverse probability was largely supplanted by a collection of methods that came to be called frequentist statistics 44 In the 20th century the ideas of Laplace were further developed in two different directions giving rise to objective and subjective currents in Bayesian practice In the objective or noninformative current the statistical analysis depends on only the model assumed the data analyzed 45 and the method assigning the prior which differs from one objective Bayesian practitioner to another In the subjective or informative current the specification of the prior depends on the belief that is propositions on which the analysis is prepared to act which can summarize information from experts previous studies etc In the 1980s there was a dramatic growth in research and applications of Bayesian methods mostly attributed to the discovery of Markov chain Monte Carlo methods which removed many of the computational problems and an increasing interest in nonstandard complex applications 46 Despite growth of Bayesian research most undergraduate teaching is still based on frequentist statistics 47 Nonetheless Bayesian methods are widely accepted and used such as for example in the field of machine learning 48 See also edit Bayes theorem Bayesian Analysis the journal of the ISBA Bayesian hierarchical modeling Bayesian probability Bayesian regression Bayesian structural time series BSTS Inductive probability International Society for Bayesian Analysis ISBA Jeffreys prior Monty Hall problem Information field theory References edit Citations edit Hacking Ian December 1967 Slightly More Realistic Personal Probability Philosophy of Science 34 4 316 doi 101086288169 mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput idlockfree amwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocklimited amwparseroutput idlockregistration amwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocksubscription amwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em Hacking 1988 p 124 full citation needed Bayes Theorem Stanford Encyclopedia of Philosophy Platostanfordedu Retrieved 20140105 van Fraassen B 1989 Laws and Symmetry Oxford University Press ISBN 0198248601 Gelman Andrew Carlin John B Stern Hal S Dunson David BVehtari Aki Rubin Donald B 2013 Bayesian Data Analysis Third Edition Chapman and HallCRC ISBN 9781439840955 Freedman DA 1963 On the asymptotic behavior of Bayes estimates in the discrete case The Annals of Mathematical Statistics 34 4 13861403 doi 101214aoms1177703871 JSTOR 2238346 Freedman DA 1965 On the asymptotic behavior of Bayes estimates in the discrete case II The Annals of Mathematical Statistics 36 2 454456 doi 101214aoms1177700155 JSTOR 2238150 Robins James Wasserman Larry 2000 Conditioning likelihood and coherence A review of some foundational concepts JASA 95 452 13401346 doi 10108001621459200010474344 Sen Pranab K Keating J P Mason R L 1993 Pitmans measure of closeness A comparison of statistical estimators Philadelphia SIAM Choudhuri Nidhan Ghosal Subhashis Roy Anindya 20050101 Bayesian Methods for Function Estimation Handbook of Statistics Bayesian Thinking 25 pp373414 CiteSeerX 10113243052 doi 101016s0169716105250137 ISBN 9780444515391 Maximum A Posteriori MAP Estimation wwwprobabilitycoursecom Retrieved 20170602 Yu Angela Introduction to Bayesian Decision Theory PDF cogsciucsdedu Archived from the original PDF on 20130228 Hitchcock David Posterior Predictive Distribution Stat Slide PDF statscedu a b Bickel Doksum 2001 p 32 Kiefer J Schwartz R 1965 Admissible Bayes Character of T 2 R 2 and Other Fully Invariant Tests for Multivariate Normal Problems Annals of Mathematical Statistics 36 3 747770 doi 101214aoms1177700051 Schwartz R 1969 Invariant Proper Bayes Tests for Exponential Families Annals of Mathematical Statistics 40 270283 doi 101214aoms1177697822 Hwang J T Casella George 1982 Minimax Confidence Sets for the Mean of a Multivariate Normal Distribution PDF Annals of Statistics 10 3 868881 doi 101214aos1176345877 Lehmann Erich 1986 Testing Statistical Hypotheses Second ed see p 309 of Chapter 67 Admissibilty and pp 1718 of Chapter 18 Complete Classes Le Cam Lucien 1986 Asymptotic Methods in Statistical Decision Theory SpringerVerlag ISBN 9780387963075 From Chapter 12 Posterior Distributions and Bayes Solutions p 324 Cox D R Hinkley DV 1974 Theoretical Statistics Chapman and Hall p432 ISBN 9780041215373 Cox D R Hinkley DV 1974 Theoretical Statistics Chapman and Hall p433 ISBN 9780041215373 Bessiere P Mazer E Ahuactzin J M Mekhnacha K 2013 Bayesian Programming 1 edition Chapman and HallCRC Daniel Roy 2015 Probabilistic Programming httpprobabilisticprogrammingorgwikiHome Ghahramani Z 2015 Probabilistic machine learning and artificial intelligence Nature 5217553452459 doi101038nature14541 Jim Albert 2009 Bayesian Computation with R Second edition New York Dordrecht etc Springer ISBN 9780387922973 Rathmanner Samuel Hutter Marcus Ormerod Thomas C 2011 A Philosophical Treatise of Universal Induction Entropy 13 6 10761136 arXiv 11055721 doi 103390e13061076 Hutter Marcus He YangHui Ormerod Thomas C 2007 On Universal Prediction and Bayesian Confirmation Theoretical Computer Science 384 2007 3348 arXiv 07091516 Bibcode 2007arXiv07091516H doi 101016jtcs200705016 Gács Peter Vitányi Paul MB 2 December 2010 Raymond J Solomonoff 19262009 CiteSeerX CiteSeerX 10111868268 Cite journal requires journal help Robinson Mark D McCarthy Davis J Smyth Gordon K edgeR a Bioconductor package for differential expression analysis of digital gene expression data Bioinformatics CIRI ciristanfordedu Retrieved 20190811 Kurtz David M Esfahani Mohammad S Scherer Florian Soo Joanne Jin Michael C Liu Chih Long Newman Aaron M Dührsen Ulrich Hüttmann Andreas 20190725 Dynamic Risk Profiling Using Serial Tumor Biomarkers for Personalized Outcome Prediction Cell 178 3 699713e19 doi 101016jcell201906011 ISSN 10974172 PMID 31280963 Dawid AP and MorteraJ 1996 Coherent Analysis of Forensic Identification Evidence Journal of the Royal Statistical Society SeriesB 58 425443 Foreman LA Smith AFM and Evett IW 1997 Bayesian analysis of deoxyribonucleic acid profiling data in forensic identification applications with discussion Journal of the Royal Statistical Society SeriesA 160 429469 Robertson B and Vignaux GA 1995 Interpreting Evidence Evaluating Forensic Science in the Courtroom John Wiley and Sons Chichester ISBN 9780471960263 Dawid A P 2001 Bayes Theorem and Weighing Evidence by Juries Archived 20150701 at the Wayback Machine GardnerMedwin A 2005 What Probability Should the Jury Address Significance 2 1 March 2005 Miller David 1994 Critical Rationalism Chicago Open Court ISBN 9780812691979 Howson Urbach 2005 Jaynes 2003 Cai XQ Wu XY Zhou X 2009 Stochastic scheduling subject to breakdownrepeat breakdowns with incomplete information Operations Research 57 5 12361249 doi 101287opre10800660 Ogle Kiona Tucker Colin Cable Jessica M 20140101 Beyond simple linear mixing models processbased isotope partitioning of ecological processes Ecological Applications 24 1 181195 doi 10189010510761241181 ISSN 19395582 Evaristo Jaivime McDonnell Jeffrey J Scholl Martha A Bruijnzeel L Adrian Chun Kwok P 20160101 Insights into plant water uptake from xylemwater isotope measurements in two tropical catchments with contrasting moisture conditions Hydrological Processes 30 18 32103227 Bibcode 2016HyPr303210E doi 101002hyp10841 ISSN 10991085 Gupta Ankur Rawlings James B April 2014 Comparison of Parameter Estimation Methods in Stochastic Chemical Kinetic Models Examples in Systems Biology AIChE Journal 60 4 12531268 doi 101002aic14409 ISSN 00011541 PMC 4946376 PMID 27429455 Stigler Stephen M 1986 Chapter 3 The History of Statistics Harvard University Press a b Fienberg Stephen E 2006 When did Bayesian Inference Become Bayesian PDF Bayesian Analysis 1 1 140 p 5 Bibcode 2007BayAn2665S doi 10121406ba101 Archived from the original PDF on 20140910 Bernardo JoséMiguel 2005 Reference analysis Handbook of statistics 25 pp1790 Wolpert RL 2004 A Conversation with James O Berger Statistical Science 19 1 205218 CiteSeerX 1011716112 doi 101214088342304000000053 MR 2082155 Bernardo José M 2006 A Bayesian mathematical statistics primer PDF Icots7 Bishop C M 2007 Pattern Recognition and Machine Learning New York Springer ISBN 9780387310732 Sources edit mwparseroutput refbeginfontsize90marginbottom05emmwparseroutput refbeginhangingindentsgtulliststyletypenonemarginleft0mwparseroutput refbeginhangingindentsgtulgtlimwparseroutput refbeginhangingindentsgtdlgtddmarginleft0paddingleft32emtextindent32emliststylenonemwparseroutput refbegin100fontsize100 Aster Richard Borchers Brian and Thurber Clifford 2012 Parameter Estimation and Inverse Problems Second Edition Elsevier ISBN 0123850487 ISBN 9780123850485 Bickel Peter J Doksum Kjell A 2001 Mathematical Statistics Volume 1 Basic and Selected Topics Second updated printing 2007 ed Pearson PrenticeHall ISBN 9780138503635 Box GEP and Tiao GC 1973 Bayesian Inference in Statistical Analysis Wiley ISBN 0471574287 Edwards Ward 1968 Conservatism in Human Information Processing In Kleinmuntz B ed Formal Representation of Human Judgment Wiley Edwards Ward 1982 Daniel Kahneman Paul Slovic and Amos Tversky eds Judgment under uncertainty Heuristics and biases Science 185 4157 11241131 Bibcode 1974Sci1851124T doi 101126science18541571124 PMID 17835457 chapter ignored help CS1 maint uses editors parameter link Jaynes ET 2003 Probability Theory The Logic of Science CUP ISBN 9780521592710 Link to Fragmentary Edition of March 1996 Howson C Urbach P 2005 Scientific Reasoning the Bayesian Approach 3rd ed Open Court Publishing Company ISBN 9780812695786 Phillips L D Edwards Ward October 2008 Chapter 6 Conservatism in a Simple Probability Inference Task Journal of Experimental Psychology 1966 72 346354 In Jie W Weiss David J Weiss eds A Science of Decision MakingThe Legacy of Ward Edwards Oxford University Press p536 ISBN 9780195322989 Further reading edit For a full report on the history of Bayesian statistics and the debates with frequentists approaches read Vallverdu Jordi 2016 Bayesians Versus Frequentists A Philosophical Debate on Statistical Reasoning New York Springer ISBN 9783662486382 Elementary edit The following books are listed in ascending order of probabilistic sophistication Stone JV 2013 Bayes Rule A Tutorial Introduction to Bayesian Analysis Download first chapter here Sebtel Press England Dennis V Lindley 2013 Understanding Uncertainty Revised Edition 2nd ed John Wiley ISBN 9781118650127 Colin Howson Peter Urbach 2005 Scientific Reasoning The Bayesian Approach 3rd ed Open Court Publishing Company ISBN 9780812695786 Berry Donald A 1996 Statistics A Bayesian Perspective Duxbury ISBN 9780534234768 Morris H DeGroot Mark J Schervish 2002 Probability and Statistics third ed AddisonWesley ISBN 9780201524888 Bolstad William M 2007 Introduction to Bayesian Statistics Second Edition John Wiley ISBN 0471270202 Winkler Robert L 2003 Introduction to Bayesian Inference and Decision 2nd ed Probabilistic ISBN 9780964793842 Updated classic textbook Bayesian theory clearly presented Lee Peter M Bayesian Statistics An Introduction Fourth Edition 2012 John Wiley ISBN 9781118332573 Carlin Bradley P Louis Thomas A 2008 Bayesian Methods for Data Analysis Third Edition Boca Raton FL Chapman and HallCRC ISBN 9781584886976 Gelman Andrew Carlin John B Stern Hal S Dunson David B Vehtari Aki Rubin Donald B 2013 Bayesian Data Analysis Third Edition Chapman and HallCRC ISBN 9781439840955 Intermediate or advanced edit Berger James O 1985 Statistical Decision Theory and Bayesian Analysis Springer Series in Statistics Second ed SpringerVerlag Bibcode 1985sdtbbookB ISBN 9780387960982 Bernardo JoséM Smith AdrianFM 1994 Bayesian Theory Wiley DeGroot Morris H Optimal Statistical Decisions Wiley Classics Library 2004 Originally published 1970 by McGrawHill ISBN 047168029X Schervish Mark J 1995 Theory of statistics SpringerVerlag ISBN 9780387945460 Jaynes E T 1998 Probability Theory The Logic of Science OHagan A and Forster J 2003 Kendalls Advanced Theory of Statistics Volume 2B Bayesian Inference Arnold New York ISBN 0340529229 Robert Christian P 2001 The Bayesian Choice A DecisionTheoretic Motivation second ed Springer ISBN 9780387942964 Glenn Shafer and Pearl Judea eds 1988 Probabilistic Reasoning in Intelligent Systems San Mateo CA Morgan Kaufmann Pierre Bessière et al 2013 Bayesian Programming CRC Press ISBN 9781439880326 Francisco J Samaniego 2010 A Comparison of the Bayesian and Frequentist Approaches to Estimation Springer New York ISBN 9781441959409 External links edit Hazewinkel Michiel ed 2001 1994 Bayesian approach to statistical problems Encyclopedia of Mathematics Springer ScienceBusiness Media BV Kluwer Academic Publishers ISBN 9781556080104 Bayesian Statistics from Scholarpedia Introduction to Bayesian probability from Queen Mary University of London Mathematical Notes on Bayesian Statistics and Markov Chain Monte Carlo Bayesian reading list categorized and annotated by Tom Griffiths A Hajek and S Hartmann Bayesian Epistemology in J Dancy et al eds A Companion to Epistemology Oxford Blackwell 2010 93106 S Hartmann and J Sprenger Bayesian Epistemology in S Bernecker and D Pritchard eds Routledge Companion to Epistemology London Routledge 2010 609620 Stanford Encyclopedia of Philosophy Inductive Logic Bayesian Confirmation Theory What Is Bayesian Learning Data Uncertainty and Inference An introduction to Bayesian inference and MCMC with a lot of examples fully explained free ebook v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean arithmetic geometric harmonic Median Mode Dispersion Variance Standard deviation Coefficient of variation Percentile Range Interquartile range Shape Central limit theorem Moments Skewness Kurtosis Lmoments Count data Index of dispersion Summary tables Grouped data Frequency distribution Contingency table Dependence Pearson productmoment correlation Rank correlation Spearmans ρ Kendalls τ Partial correlation Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart QQ plot Run chart Scatter plot Stemandleaf display Radar chart Violin plot Data collection Study design Population Statistic Effect size Statistical power Optimal design Sample size determination Replication Missing data Survey methodology Sampling stratified cluster Standard error Opinion poll Questionnaire Controlled experiments Scientific control Randomized experiment Randomized controlled trial Random assignment Blocking Interaction Factorial experiment Adaptive Designs Adaptive clinical trial UpandDown Designs Stochastic approximation Observational Studies Crosssectional study Cohort study Natural experiment Quasiexperiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood monotone Locationscale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments Mestimator Minimum distance Unbiased estimators Meanunbiased minimumvariance RaoBlackwellization LehmannScheffé theorem Median unbiased Plugin Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1 2tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihoodratio ScoreLagrange multiplier Wald Specific tests Z test normal Students t test F test Goodness of fit Chisquared G test KolmogorovSmirnov AndersonDarling Lilliefors JarqueBera Normality ShapiroWilk Likelihoodratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank Wilcoxon HodgesLehmann estimator Rank sum MannWhitney Nonparametric anova 1way KruskalWallis 2way Friedman Ordered alternative JonckheereTerpstra Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson productmoment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines MARS Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Nonstandard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Heteroscedasticity Homoscedasticity Generalized linear model Exponential families Logistic Bernoulli Binomial Poisson regressions Partition of variance Analysis of variance ANOVA anova Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical Multivariate Timeseries Survival analysis Categorical Cohens kappa Contingency table Graphical model Loglinear model McNemars test Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Timeseries General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests DickeyFuller Johansen Qstatistic LjungBox DurbinWatson BreuschGodfrey Time domain Autocorrelation ACF partial PACF Crosscorrelation XCF ARMA model ARIMA model BoxJenkins Autoregressive conditional heteroskedasticity ARCH Vector autoregression VAR Frequency domain Spectral density estimation Fourier analysis Wavelet Whittle likelihood Survival Survival function KaplanMeier estimator product limit Proportional hazards models Accelerated failure time AFT model First hitting time Hazard function NelsonAalen estimator Test Logrank test Applications Biostatistics Bioinformatics Clinical trials studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Authority control BNE XX550382 BNF cb121309043 data GND 41442209 LCCN sh85012506 SUDOC 029753090 