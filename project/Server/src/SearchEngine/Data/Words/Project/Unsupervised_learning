Unsupervised learning Machine learning and data mining Problems Classification Clustering Regression Anomaly detection AutoML Association rules Reinforcement learning Structured prediction Feature engineering Feature learning Online learning Semisupervised learning Unsupervised learning Learning to rank Grammar induction Supervised learning mwparseroutput noboldfontweightnormal classification regression Decision trees Ensembles Bagging Boosting Random forest k NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine RVM Support vector machine SVM Clustering BIRCH CURE Hierarchical k means Expectationmaximization EM DBSCAN OPTICS Meanshift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA tSNE Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection k NN Local outlier factor Artificial neural network Autoencoder Deep learning DeepDream Multilayer perceptron RNN LSTM GRU Restricted Boltzmann machine GAN SOM Convolutional neural network UNet Reinforcement learning Qlearning SARSA Temporal difference TD Theory Biasvariance dilemma Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Machinelearning venues NeurIPS ICML ML JMLR ArXivcsLG Glossary of artificial intelligence Glossary of artificial intelligence Related articles List of datasets for machinelearning research Outline of machine learning v t e Unsupervised learning is a type of selforganized Hebbian learning that helps find previously unknown patterns in data set without preexisting labels It is also known as selforganization and allows modeling probability densities of given inputs 1 It is one of the main three categories of machine learning along with supervised and reinforcement learning Semisupervised learning has also been described and is a hybridization of supervised and unsupervised techniques Two of the main methods used in unsupervised learning are principal component and cluster analysis Cluster analysis is used in unsupervised learning to group or segment datasets with shared attributes in order to extrapolate algorithmic relationships 2 Cluster analysis is a branch of machine learning that groups the data that has not been labelled classified or categorized Instead of responding to feedback cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data This approach helps detect anomalous data points that do not fit into either group A central application of unsupervised learning is in the field of density estimation in statistics 3 though unsupervised learning encompasses many other domains involving summarizing and explaining data features It could be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution p X x y textstyle pXxy conditioned on the label y textstyle y of input data unsupervised learning intends to infer an a priori probability distribution p X x textstyle pXx Generative adversarial networks can also be used with unsupervised learning though they can also be applied to supervised and reinforcement techniques Contents 1 Approaches 2 Neural networks 3 Method of moments 4 See also 5 Notes 6 Further reading Approaches edit This article is in list format but may read better as prose You can help by converting this article if appropriate Editing help is available September 2018 Some of the most common algorithms used in unsupervised learning include Clustering hierarchical clustering 4 kmeans 5 mixture models DBSCAN OPTICS algorithm Anomaly detection Local Outlier Factor Neural Networks Autoencoders Deep Belief Nets Hebbian Learning Generative adversarial networks Selforganizing map Approaches for learning latent variable models such as Expectationmaximization algorithm EM Method of moments Blind signal separation techniques Principal component analysis Independent component analysis Nonnegative matrix factorization Singular value decomposition Neural networks edit The classical example of unsupervised learning in the study of neural networks is Donald Hebb s principle that is neurons that fire together wire together 6 In Hebbian learning the connection is reinforced irrespective of an error but is exclusively a function of the coincidence between action potentials between the two neurons 7 A similar version that modifies synaptic weights takes into account the time between the action potentials spiketimingdependent plasticity or STDP Hebbian Learning has been hypothesized to underlie a range of cognitive functions such as pattern recognition and experiential learning Among neural network models the selforganizing map SOM and adaptive resonance theory ART are commonly used in unsupervised learning algorithms The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a userdefined constant called the vigilance parameter ART networks are used for many pattern recognition tasks such as automatic target recognition and seismic signal processing 8 Method of moments edit One of the statistical approaches for unsupervised learning is the method of moments In the method of moments the unknown parameters of interest in the model are related to the moments of one or more random variables and thus these unknown parameters can be estimated given the moments The moments are usually estimated from samples empirically The basic moments are first and second order moments For a random vector the first order moment is the mean vector and the second order moment is the covariance matrix when the mean is zero Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multidimensional arrays In particular the method of moments is shown to be effective in learning the parameters of latent variable models 9 Latent variable models are statistical models where in addition to the observed variables a set of latent variables also exists which is not observed A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words observed variables in the document based on the topic latent variable of the document In the topic modeling the words in the document are generated according to different statistical parameters when the topic of the document is changed It is shown that method of moments tensor decomposition techniques consistently recover the parameters of a large class of latent variable models under some assumptions 9 The Expectationmaximization algorithm EM is also one of the most practical methods for learning latent variable models However it can get stuck in local optima and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model In contrast for the method of moments the global convergence is guaranteed under some conditions 9 See also edit Automated machine learning Cluster analysis Anomaly detection Expectationmaximization algorithm Generative topographic map Metalearning computer science Multivariate analysis Radial basis function network Weak supervision Notes edit Hinton Geoffrey Sejnowski Terrence 1999 Unsupervised Learning Foundations of Neural Computation MIT Press ISBN 9780262581684 mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput idlockfree amwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocklimited amwparseroutput idlockregistration amwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocksubscription amwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em Roman Victor 20190421 Unsupervised Machine Learning Clustering Analysis Medium Retrieved 20191001 Jordan Michael I Bishop Christopher M 2004 Neural Networks In Allen B Tucker ed Computer Science Handbook Second Edition Section VII Intelligent Systems Boca Raton Florida Chapman HallCRC Press LLC ISBN 158488360X Hastie Trevor Robert Tibshirani Friedman Jerome 2009 The Elements of Statistical Learning Data mining Inference and Prediction New York Springer pp485586 ISBN 9780387848570 CS1 maint multiple names authors list link Garbade Dr Michael J 20180912 Understanding Kmeans Clustering in Machine Learning Medium Retrieved 20191031 Buhmann J Kuhnel H 1992 Unsupervised and supervised data clustering with competitive neural networks Proceedings 1992 IJCNN International Joint Conference on Neural Networks 4 IEEE pp796801 doi 101109ijcnn1992227220 ISBN 0780305590 ComesañaCampos Alberto BouzaRodríguez José Benito June 2016 An application of Hebbian learning in the design process decisionmaking Journal of Intelligent Manufacturing 27 3 487506 doi 101007s108450140881z ISSN 09565515 Carpenter GA Grossberg S 1988 The ART of adaptive pattern recognition by a selforganizing neural network PDF Computer 21 3 7788 doi 101109233 a b c Anandkumar Animashree Ge Rong Hsu Daniel Kakade Sham Telgarsky Matus 2014 Tensor Decompositions for Learning Latent Variable Models PDF Journal of Machine Learning Research JMLR 15 27732832 arXiv 12107559 Bibcode 2012arXiv12107559A Further reading edit Bousquet O von Luxburg U Raetsch G eds 2004 Advanced Lectures on Machine Learning SpringerVerlag ISBN 9783540231226 Duda Richard O Hart Peter E Stork David G 2001 Unsupervised Learning and Clustering Pattern classification 2nd ed Wiley ISBN 0471056693 Hastie Trevor Tibshirani Robert 2009 The Elements of Statistical Learning Data mining Inference and Prediction New York Springer pp485586 doi 101007978038784858714 ISBN 9780387848570 Hinton Geoffrey Sejnowski Terrence J eds 1999 Unsupervised Learning Foundations of Neural Computation MIT Press ISBN 026258168X This book focuses on unsupervised learning in neural networks 