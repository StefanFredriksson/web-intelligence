Question answering For other uses see Question and Answer Question answering QA is a computer science discipline within the fields of information retrieval and natural language processing NLP which is concerned with building systems that automatically answer questions posed by humans in a natural language 1 Contents 1 Overview 2 History 3 Architecture 4 Question answering methods 41 Open domain question answering 42 Mathematical question answering 5 Progress 6 References 7 Further reading 8 External links Overview edit A question answering implementation usually a computer program may construct its answers by querying a structured database of knowledge or information usually a knowledge base More commonly question answering systems can pull answers from an unstructured collection of natural language documents Some examples of natural language document collections used for question answering systems include a local collection of reference texts internal organization documents and web pages compiled newswire reports a set of Wikipedia pages a subset of World Wide Web pages Question answering research attempts to deal with a wide range of question types including fact list definition How Why hypothetical semantically constrained and crosslingual questions Closeddomain question answering deals with questions under a specific domain for example medicine or automotive maintenance and can exploit domainspecific knowledge frequently formalized in ontologies Alternatively closeddomain might refer to a situation where only a limited type of questions are accepted such as questions asking for descriptive rather than procedural information Question answering systems in the context of machine reading applications have also been constructed in the medical domain for instance related to Alzheimers disease 2 Opendomain question answering deals with questions about nearly anything and can only rely on general ontologies and world knowledge On the other hand these systems usually have much more data available from which to extract the answer History edit Two early question answering systems were BASEBALL 3 and LUNAR 4 BASEBALL answered questions about the US baseball league over a period of one year LUNAR in turn answered questions about the geological analysis of rocks returned by the Apollo moon missions Both question answering systems were very effective in their chosen domains In fact LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90 of the questions in its domain posed by people untrained on the system Further restricteddomain question answering systems were developed in the following years The common feature of all these systems is that they had a core database or knowledge system that was handwritten by experts of the chosen domain The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR the first chatterbot programs SHRDLU was a highly successful questionanswering program developed by Terry Winograd in the late 1960s and early 1970s It simulated the operation of a robot in a toy world the blocks world and it offered the possibility of asking the robot questions about the state of the world Again the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program In the 1970s knowledge bases were developed that targeted narrower domains of knowledge The question answering systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge These expert systems closely resembled modern question answering systems except in their internal architecture Expert systems rely heavily on expertconstructed and organized knowledge bases whereas many modern question answering systems rely on statistical processing of a large unstructured natural language text corpus The 1970s and 1980s saw the development of comprehensive theories in computational linguistics which led to the development of ambitious projects in text comprehension and question answering One example of such a system was the Unix Consultant UC developed by Robert Wilensky at UC Berkeley in the late 1980s The system answered questions pertaining to the Unix operating system It had a comprehensive handcrafted knowledge base of its domain and it aimed at phrasing the answer to accommodate various types of users Another project was LILOG a textunderstanding system that operated on the domain of tourism information in a German city The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations but they helped the development of theories on computational linguistics and reasoning Specialized natural language question answering systems have been developed such as EAGLi for health and life scientists and Wolfram Alpha an online computational knowledge engine that answers factual queries directly by computing the answer from externally sourced curated data citation needed Architecture edit As of 2001 question answering systems typically included a question classifier module that determines the type of question and the type of answer 5 A multiagent questionanswering architecture has been proposed where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge a metaagent controls the cooperation between question answering agents and chooses the most relevant answers 6 Question answering methods edit Question answering is very dependent on a good search corpus for without documents containing the answer there is little any question answering system can do It thus makes sense that larger collection sizes generally lend well to better question answering performance unless the question domain is orthogonal to the collection The notion of data redundancy in massive collections such as the web means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents 7 leading to two benefits By having the right information appear in many forms the burden on the question answering system to perform complex NLP techniques to understand the text is lessened Correct answers can be filtered from false positives by relying on the correct answer to appear more times in the documents than instances of incorrect ones Some question answering systems rely heavily on automated reasoning 8 9 There are a number of question answering systems designed in Prolog 10 a logic programming language associated with artificial intelligence Open domain question answering edit This section needs additional citations for verification Please help improve this article by adding citations to reliable sources Unsourced material may be challenged and removed Find sources Question answering news newspapers books scholar JSTOR January 2016 Learn how and when to remove this template message In information retrieval an open domain question answering system aims at returning an answer in response to the users question The returned answer is in the form of short texts rather than a list of relevant documents 11 The system uses a combination of techniques from computational linguistics information retrieval and knowledge representation for finding answers The system takes a natural language question as an input rather than a set of keywords for example When is the national day of China The sentence is then transformed into a query through its logical form Having the input in the form of a natural language question makes the system more userfriendly but harder to implement as there are various question types and the system will have to identify the correct one in order to give a sensible answer Assigning a question type to the question is a crucial task the entire answer extraction process relies on finding the correct question type and hence the correct answer type Keyword extraction is the first step for identifying the input question type 12 In some cases there are clear words that indicate the question type directly ie Who Where or How many these words tell the system that the answers should be of type Person Location Number respectively In the example above the word When indicates that the answer should be of type Date POS partofspeech tagging and syntactic parsing techniques can also be used to determine the answer type In this case the subject is Chinese National Day the predicate is is and the adverbial modifier is when therefore the answer type is Date Unfortunately some interrogative words like Which What or How do not give clear answer types Each of these words can represent more than one type In situations like this other words in the question need to be considered First thing to do is to find the words that can indicate the meaning of the question A lexical dictionary such as WordNet can then be used for understanding the context Once the question type has been identified an information retrieval system is used to find a set of documents containing the correct key words A tagger and NPVerb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents For questions such as Who or Where a namedentity recogniser is used to find relevant Person and Location names from the retrieved documents Only the relevant paragraphs are selected for ranking A vector space model can be used as a strategy for classifying the candidate answers Check if the answer is of the correct type as determined in the question type analysis stage An inference technique can also be used to validate the candidate answers A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate the more and the closer the better The answer is then translated into a compact and meaningful representation by parsing In the previous example the expected output answer is 1st Oct Mathematical question answering edit An open source mathaware question answering system based on Ask Platypus and Wikidata was published in 2018 13 The system takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as succinct answer The resulting formula is translated into a computable form allowing the user to insert values for the variables Names and values of variables and common constants are retrieved from Wikidata if available It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set Progress edit Question answering systems have been extended in recent years to encompass additional domains of knowledge 14 For example systems have been developed to automatically answer temporal and geospatial questions questions of definition and terminology biographical questions multilingual questions and questions about the content of audio images and video Current question answering research topics include interactivityclarification of questions or answers 15 answer reuse or caching citation needed semantic parsing 16 answer presentation 17 knowledge representation and reasoning social media analysis with question answering systems sentiment analysis 18 utilization of thematic roles 19 semantic resolution to bridge the gap between syntactically different questions and answerbearing texts 20 utilization of linguistic resources 21 such as WordNet FrameNet and the similar Image captioning for visual question answering 22 IBMs question answering system Watson defeated the two greatest Jeopardy champions Brad Rutter and Ken Jennings by a significant margin 23 Facebook Research has made their DrQA system 24 available under an open source license This system has been used for open domain question answering using Wikipedia as knowledge source 25 References edit Philipp Cimiano Christina Unger John McCrae 1 March 2014 OntologyBased Interpretation of Natural Language Morgan Claypool Publishers ISBN 9781608459902 mwparseroutput citecitationfontstyleinheritmwparseroutput citation qquotesmwparseroutput idlockfree amwparseroutput citation cs1lockfree abackgroundurluploadwikimediaorgwikipediacommonsthumb665Lockgreensvg9pxLockgreensvgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocklimited amwparseroutput idlockregistration amwparseroutput citation cs1locklimited amwparseroutput citation cs1lockregistration abackgroundurluploadwikimediaorgwikipediacommonsthumbdd6Lockgrayalt2svg9pxLockgrayalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput idlocksubscription amwparseroutput citation cs1locksubscription abackgroundurluploadwikimediaorgwikipediacommonsthumbaaaLockredalt2svg9pxLockredalt2svgpngnorepeatbackgroundpositionright 1em centermwparseroutput cs1subscriptionmwparseroutput cs1registrationcolor555mwparseroutput cs1subscription spanmwparseroutput cs1registration spanborderbottom1px dottedcursorhelpmwparseroutput cs1wsicon abackgroundurluploadwikimediaorgwikipediacommonsthumb44cWikisourcelogosvg12pxWikisourcelogosvgpngnorepeatbackgroundpositionright 1em centermwparseroutput codecs1codecolorinheritbackgroundinheritborderinheritpaddinginheritmwparseroutput cs1hiddenerrordisplaynonefontsize100mwparseroutput cs1visibleerrorfontsize100mwparseroutput cs1maintdisplaynonecolor33aa33marginleft03emmwparseroutput cs1subscriptionmwparseroutput cs1registrationmwparseroutput cs1formatfontsize95mwparseroutput cs1kernleftmwparseroutput cs1kernwlleftpaddingleft02emmwparseroutput cs1kernrightmwparseroutput cs1kernwlrightpaddingright02em Roser Morante Martin Krallinger Alfonso Valencia and Walter Daelemans Machine Reading of Biomedical Texts about Alzheimers Disease CLEF 2012 Evaluation Labs and Workshop September 17 2012 GREEN JR Bert F et al 1961 Baseball an automatic questionanswerer PDF Western Joint IREAIEEACM Computer Conference 219224 Woods William A Kaplan R 1977 Lunar rocks in natural English Explorations in natural language question answering Linguistic Structures Processing 5 5 521569 Hirschman L Gaizauskas R 2001 Natural Language Question Answering The View from Here Natural Language Engineering 2001 74275300 Cambridge University Press Galitsky B Pampapathi R Can many agents answer questions better than one First Monday 200510 doi 105210fmv10i11204 Lin J 2002 The Web as a Resource for Question Answering Perspectives and Challenges In Proceedings of the Third International Conference on Language Resources and Evaluation LREC 2002 Moldovan Dan et al Cogex A logic prover for question answering Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1 Association for Computational Linguistics 2003 Furbach Ulrich Ingo Glöckner and Björn Pelzer An application of automated reasoning in natural language question answering Ai Communications 2323 2010 241265 Galitsky Boris 2003 Natural Language Question Answering System Technique of Semantic Headers International Series on Advanced Intelligence Volume 2 Australia Advanced Knowledge International ISBN 9780868039794 Sun Haitian Dhingra Bhuwan Zaheer Manzil Mazaitis Kathryn Salakhutdinov Ruslan Cohen William 2018 Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text Association for Computational Linguistics Brussels Belgium 42314242 Harabagiu Sanda Hickl Andrew 2006 Methods for using textual entailment in opendomain question answering Association for Computational Linguistics Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics 905912 doi 10311512201751220289 Moritz Schubotz Philipp Scharpf et al 12 September 2018 Introducing MathQA a MathAware question answering system Information Discovery and Delivery Emerald Publishing Limited 46 4 214224 doi 101108IDD0620180022 Paşca Marius 2005 Book Review New Directions in Question Answering Mark T Maybury editor MITRE Corporation Menlo Park CA AAAI Press and Cambridge MA The MIT Press 2004 xi336 pp paperbound ISBN 0262633043 4000 2595 Computational Linguistics 31 3 413417 doi 101162089120105774321055 Quarteroni Silvia and Suresh Manandhar Designing an interactive opendomain question answering system Natural Language Engineering 151 2009 7395 Yih Wentau Xiaodong He and Christopher Meek Semantic parsing for singlerelation question answering Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics Volume 2 Short Papers 2014 Perera R Nand P and Naeem A 2017 Utilizing typed dependency subtree patterns for answer sentence generation in question answering systems BitCrawl by Hobson Lane Archived from the original on October 27 2012 Retrieved 20120529 CS1 maint BOT originalurl status unknown link Perera R and Perera U 2012 Towards a thematic role based target identification model for question answering Bahadorreza Ofoghi John Yearwood Liping Ma 2008 The impact of semantic class identification and semantic role labeling on natural language answer extraction The 30th European Conference on Information Retrieval ECIR08 Springer Berlin Heidelberg pp430437 doi 101007978354078646740 Bahadorreza Ofoghi John Yearwood Liping Ma 2009 The impact of frame semantic annotation levels framealignment techniques and fusion methods on factoid answer processing Journal of the American Society for Information Science and Technology 60 2 247263 doi 101002asi20989 Anderson Peter et al Bottomup and topdown attention for image captioning and visual question answering Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2018 Markoff John 20110216 On Jeopardy Watson Win is All but Trivial The New York Times DrQA Chen Danqi Fisch Adam Weston Jason Bordes Antoine 2017 Reading Wikipedia to Answer OpenDomain Questions arXiv 170400051 csCL Further reading edit Dragomir R Radev John Prager and Valerie Samn Ranking suspected answers to natural language questions using predictive annotation In Proceedings of the 6th Conference on Applied Natural Language Processing Seattle WA May 2000 John Prager Eric Brown Anni Coden and Dragomir Radev Questionanswering by predictive annotation In Proceedings 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval Athens Greece July 2000 Hutchins W John Harold L Somers 1992 An Introduction to Machine Translation London Academic Press ISBN 9780123628305 L Fortnow Steve Homer 20022003 A Short History of Computational Complexity In D van Dalen J Dawson and A Kanamori editors The History of Mathematical Logic NorthHolland Amsterdam External links edit Question Answering Evaluation at NTCIR Question Answering Evaluation at TREC Question Answering Evaluation at CLEF Quiz Question Answers Online Question Answering System v t e Computable knowledge Topics and concepts Alphabet of human thought Authority control Automated reasoning Commonsense knowledge Commonsense reasoning Computability Formal system Inference engine Knowledge base Knowledgebased systems Knowledge engineering Knowledge extraction Knowledge representation Knowledge retrieval Library classification Logic programming Ontology Personal knowledge base Question answering Semantic reasoner Proposals and implementations Zairja Ars Magna 1300 An Essay towards a Real Character and a Philosophical Language 1688 Calculus ratiocinator and characteristica universalis 1700 Dewey Decimal Classification 1876 Begriffsschrift 1879 Mundaneum 1910 Logical atomism 1918 Tractatus LogicoPhilosophicus 1921 Hilberts program 1920s Incompleteness theorem 1931 World Brain 1938 Memex 1945 General Problem Solver 1959 Prolog 1972 Cyc 1984 Semantic Web 2001 Evi 2007 Wolfram Alpha 2009 Watson 2011 Siri 2011 Knowledge Graph 2012 Wikidata 2012 Cortana 2014 Viv 2016 In fiction The Engine Gullivers Travels 1726 Joe A Logic Named Joe 1946 The Librarian Snow Crash 1992 Dr Know AI Artificial Intelligence 2001 Waterhouse The Baroque Cycle 2003 See also Logic machines in fiction and List of fictional computers v t e Natural language processing General terms Natural language understanding Text corpus Speech corpus Stopwords Bagofwords AIcomplete ngram Bigram Trigram Text analysis Text segmentation Partofspeech tagging Text chunking Compound term processing Collocation extraction Stemming Lemmatisation Namedentity recognition Coreference resolution Sentiment analysis Concept mining Parsing Wordsense disambiguation Ontology learning Terminology extraction Textual entailment Truecasing Automatic summarization Multidocument summarization Sentence extraction Text simplification Machine translation Computerassisted Examplebased Rulebased Neural Automatic identification and data capture Speech recognition Speech synthesis Optical character recognition Natural language generation Topic model Pachinko allocation Latent Dirichlet allocation Latent semantic analysis Computerassisted reviewing Automated essay scoring Concordancer Grammar checker Predictive text Spell checker Syntax guessing Natural language user interface Automated online assistant Chatbot Interactive fiction Question answering Voice user interface 